{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuralLM_POStag.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDVhxezHTskI"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#en_hi_tweets = open(\"/content/drive/MyDrive/nlp_project/datasets/POStag_dataset/POS_Hindi_English_Code_Mixed_Tweets.tsv\", \"r\").read()\n",
        "data2 = open(\"/content/drive/MyDrive/nlp_project/datasets/POStag_dataset/data2.txt\", \"r\").read()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUSr4M07ak4j",
        "outputId": "5de3add5-a2b0-4b54-f7fb-5bdb686d8e2d"
      },
      "source": [
        "def token_lookup():\n",
        "    token_dict = {\".\":\"<PERIOD>\", \",\": \"<COMMA>\", \";\":\"<SEMICOLON>\", \":\":\"<COLON>\", \"\\\"\":\"<QOUTATION>\", \"'\":\"QOUTATION\", \"!\":\"<EXCLAMATION>\", \"?\":\"<QUESTION>\", \n",
        "                  \"(\": \"<LEFTPARAN>\", \")\": \"<RIGHTPARAN>\", \"{\":\"LEFTBRACE\", \"}\":\"<RIGHTBRACE>\", \"[\":\"<LEFTBRACKET>\", \"]\":\"<RIGHTBRACKET>\", \"--\":\"<DASH>\", \"-\":\"<HYPHEN>\",\n",
        "                  #\"\\n\":\"<RETURN>\"\n",
        "                  }\n",
        "    return token_dict\n",
        "\n",
        "token_dict = token_lookup()\n",
        "\n",
        "#for key, token in token_dict.items():\n",
        "#    en_hi_tweets = en_hi_tweets.replace(key, '{}'.format(token))\n",
        "\n",
        "sentences = []\n",
        "#lines = en_hi_tweets.split(\"\\n\\t\\t\\n\")\n",
        "lines = data2.split(\"\\n\\n\")\n",
        "for line in lines:\n",
        "    rows = line.split('\\n')\n",
        "    sent = ['<BOS>']\n",
        "    for x in rows:\n",
        "        sent.append(\"_\".join(x.split('\\t')))\n",
        "    sent.append(\"<EOS>\")\n",
        "    sent = \" \".join(sent)\n",
        "    sentences.append(sent)\n",
        "print(len(sentences[0]))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGtj0WuIfAjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29f7493-b954-442f-e2dd-75977ed9421d"
      },
      "source": [
        "def create_lookup_tables(sentences):\n",
        "    vocab = set()\n",
        "    for line in sentences:\n",
        "        vocab.update(set(line.split()))\n",
        "    #vocab = set(text)\n",
        "    vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
        "    int_to_vocab = dict(enumerate(vocab))\n",
        "    return (vocab_to_int, int_to_vocab)\n",
        "\n",
        "vocab_to_int, int_to_vocab = create_lookup_tables(sentences)\n",
        "\n",
        "train_int_text = []\n",
        "\n",
        "for line in sentences:\n",
        "    train_int_text.append([vocab_to_int[word] for word in line.split()])\n",
        "\n",
        "train_int_text = [item for sublist in train_int_text for item in sublist]\n",
        "print(train_int_text[:1000])  "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1112, 2418, 3633, 3236, 284, 2188, 3446, 1081, 3790, 152, 509, 2285, 2083, 3529, 4078, 3142, 3222, 1112, 617, 3969, 350, 3446, 2744, 1274, 4562, 4747, 3337, 1227, 4228, 296, 1946, 488, 3054, 3222, 1112, 2020, 4530, 1966, 1189, 98, 3976, 2974, 1609, 1524, 4338, 2068, 2017, 1081, 3222, 1112, 3660, 4928, 617, 3969, 350, 3446, 2744, 1274, 4562, 4747, 3337, 1227, 4228, 296, 1946, 488, 3054, 3222, 1112, 225, 1608, 4177, 665, 630, 2328, 2209, 1966, 2991, 3893, 1524, 2509, 1958, 1988, 747, 4023, 2385, 1081, 4981, 418, 1035, 3222, 1112, 2438, 3777, 1578, 1524, 1772, 4346, 1917, 4286, 810, 700, 2265, 2852, 4866, 2406, 1476, 4079, 1081, 7, 1779, 4201, 4604, 3222, 1112, 2776, 4595, 1596, 4982, 1196, 2551, 846, 362, 4187, 3242, 640, 297, 3063, 813, 2776, 221, 1911, 3720, 1183, 289, 4639, 1653, 813, 1414, 2551, 1530, 3376, 948, 3222, 1112, 2114, 2911, 2092, 1540, 4915, 3497, 3927, 1933, 2546, 542, 3694, 2516, 3705, 3796, 4537, 659, 3838, 748, 3222, 1112, 1517, 1397, 1652, 3497, 2547, 3139, 3611, 2971, 2020, 4733, 3519, 502, 1540, 1652, 889, 3680, 2982, 314, 1362, 2231, 563, 2510, 3222, 1112, 2971, 1666, 3497, 2051, 4207, 3254, 1260, 2051, 4412, 3594, 3260, 1540, 971, 3298, 69, 4207, 4729, 4168, 4549, 310, 3222, 1112, 1517, 1397, 1652, 3497, 2547, 3139, 1630, 2971, 2020, 4733, 3519, 502, 1540, 550, 2971, 920, 1540, 1652, 889, 3680, 2982, 3222, 1112, 1688, 1983, 556, 2971, 789, 4015, 2053, 4116, 1540, 3874, 5030, 3859, 3497, 4491, 2165, 2971, 2595, 3222, 1112, 2971, 1666, 3497, 4030, 4207, 3254, 1260, 2051, 4412, 3594, 3260, 1540, 971, 3298, 69, 4207, 4729, 4168, 4549, 310, 3222, 1112, 2971, 4756, 2070, 4394, 1540, 2918, 1867, 139, 798, 838, 2971, 2224, 972, 1923, 2377, 4049, 846, 1525, 4415, 3222, 1112, 1688, 1983, 556, 2971, 789, 4015, 2053, 4116, 1540, 3874, 5030, 3859, 3497, 4491, 2165, 2971, 2595, 3222, 1112, 1517, 1397, 1652, 3497, 2547, 3139, 1630, 2971, 2020, 4733, 3519, 2161, 1540, 550, 2971, 920, 1540, 1652, 889, 3680, 2982, 314, 1362, 3222, 1112, 2340, 846, 2993, 4267, 2192, 734, 142, 4734, 221, 2671, 5010, 3222, 1112, 2971, 2020, 1551, 4212, 1666, 3536, 557, 1630, 2683, 3394, 3497, 1641, 1540, 4169, 4174, 91, 2079, 3284, 4827, 3538, 3222, 1112, 4741, 2083, 846, 3281, 4830, 813, 3039, 4953, 3949, 3281, 3261, 2660, 813, 665, 1966, 221, 1129, 4547, 225, 2550, 3580, 4547, 3222, 1112, 4915, 63, 3497, 1211, 229, 1540, 2454, 758, 2020, 1433, 1966, 3222, 1112, 1714, 576, 556, 2037, 4015, 242, 4116, 1540, 3757, 5030, 3859, 3497, 4491, 2165, 2971, 1707, 1591, 2219, 1444, 1691, 3214, 2578, 4627, 3729, 1989, 2458, 2849, 3222, 1112, 1427, 1652, 846, 813, 3077, 2698, 4915, 3151, 4272, 29, 2958, 2440, 1340, 3595, 4503, 867, 3992, 3222, 1112, 4235, 3864, 462, 2111, 4767, 1414, 3688, 3066, 813, 1350, 1261, 4332, 2520, 3244, 2422, 4413, 3222, 1112, 3592, 462, 2111, 1361, 4286, 4332, 2520, 4381, 4740, 1978, 2574, 3911, 4561, 2839, 4884, 3222, 1112, 650, 4145, 612, 3456, 813, 4597, 462, 3140, 1261, 672, 3982, 528, 3222, 1112, 1794, 462, 3456, 3479, 1019, 4500, 641, 1417, 3100, 1261, 4561, 1420, 4747, 2890, 174, 142, 1502, 3222, 1112, 462, 112, 4145, 257, 1361, 3022, 2083, 1167, 3535, 856, 1646, 3864, 2784, 3222, 1112, 588, 4289, 2083, 1261, 3937, 1414, 1837, 813, 2006, 4491, 2080, 89, 4003, 2680, 985, 4394, 3222, 1112, 4350, 4194, 4573, 4096, 4019, 3281, 4960, 1157, 2034, 4918, 462, 1196, 2129, 1261, 1040, 3100, 3222, 1112, 3155, 1021, 3720, 4276, 1261, 1767, 813, 1008, 4561, 581, 2020, 580, 1693, 2208, 1266, 2442, 290, 4771, 4649, 3282, 462, 3222, 1112, 3373, 67, 4795, 3728, 2099, 3073, 269, 4178, 3297, 948, 313, 2913, 4000, 3993, 462, 3544, 2122, 813, 1261, 813, 3222, 1112, 4727, 462, 1780, 2482, 1261, 2784, 4498, 3222, 1112, 1711, 1837, 2856, 813, 462, 1196, 4455, 1261, 2599, 813, 3222, 1112, 3412, 462, 4561, 1261, 2111, 3062, 679, 1886, 4253, 4366, 1147, 535, 36, 4945, 1141, 1280, 4211, 1726, 3222, 1112, 1784, 588, 2111, 2008, 3882, 3173, 2759, 4653, 1261, 927, 103, 439, 136, 3720, 813, 3222, 1112, 3048, 2070, 1596, 9, 4964, 4326, 666, 813, 1531, 3298, 4466, 1864, 4818, 4615, 1864, 588, 2111, 1261, 3681, 813, 2891, 2520, 3222, 1112, 4849, 103, 1261, 4332, 2271, 4545, 3352, 1047, 885, 182, 3869, 813, 3467, 462, 3222, 1112, 2305, 369, 2068, 4014, 1261, 103, 1261, 3222, 1112, 603, 462, 2227, 2083, 1261, 351, 1280, 813, 1273, 3222, 1112, 104, 875, 4267, 3326, 902, 3373, 462, 103, 3258, 1596, 134, 1987, 283, 4211, 3222, 1112, 4945, 3338, 4649, 462, 4962, 4851, 2609, 1988, 2504, 2690, 3539, 4307, 3681, 2893, 3473, 3222, 1112, 902, 481, 1498, 107, 202, 462, 813, 3222, 1112, 373, 1261, 2009, 4615, 3711, 637, 462, 4332, 3193, 754, 3222, 1112, 588, 3140, 267, 1428, 3720, 2563, 4948, 4337, 425, 2761, 594, 344, 4917, 4948, 1646, 813, 272, 2083, 1261, 4211, 2520, 2826, 3242, 1435, 3140, 213, 669, 1141, 1280, 3009, 3222, 1112, 84, 103, 2927, 2228, 2852, 1269, 3124, 3627, 1765, 813, 506, 462, 3140, 3783, 3720, 1261, 463, 4477, 2852, 3222, 1112, 462, 3720, 1361, 2119, 3915, 3842, 495, 3222, 1112, 386, 4111, 813, 3956, 1328, 2889, 813, 2199, 3140, 2260, 3301, 2649, 4543, 813, 86, 2852, 70, 1244, 1987, 4211, 2519, 3222, 1112, 2850, 4561, 462, 4561, 4433, 1261, 796, 1336, 3222, 1112, 3132, 1796, 2154, 4543, 3720, 1141, 462, 1646, 3149, 1361, 3681, 4342, 1367, 2660, 425, 1042, 1596, 1831, 3442, 4860, 2201, 3222, 1112, 2717, 981, 22, 2302, 1596, 9, 1261, 2009, 813, 1981, 2520, 462, 1356, 948, 942, 936, 366, 1596, 3542, 1259, 1004, 4016, 1854, 1004, 3222, 1112, 680, 2660, 1476, 4445, 3373, 2083, 1261, 4147, 462, 3140, 2520, 243, 4211, 3281, 3222, 1112, 1480, 2852, 1831, 2136, 2718, 3864, 9, 3093, 3122, 354, 3400, 1038, 2809, 3748, 2111, 2121, 103, 3734, 2562, 2068, 2809, 462, 1978, 3681, 4547, 1361, 3222, 1112, 1230, 1860, 2708, 1261, 4096, 2347, 813, 4000, 103, 3516, 4402, 1057, 813, 462, 2111, 3681]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-15jxZPnmwV4"
      },
      "source": [
        "#import helper\n",
        "\n",
        "#data_dir = './brown.txt'\n",
        "\n",
        "#helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)\n",
        "\n",
        "from distutils.version import LooseVersion\n",
        "import warnings\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "import tensorflow as tf2\n",
        "#import helper\n",
        "import numpy as np\n",
        "\n",
        "#int_text, val_int_text, test_int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "\n",
        "def get_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    learn_rate = tf.placeholder(tf.float32, name='learn_rate')\n",
        "    return (inputs, targets, learn_rate)\n",
        "    \n",
        "def get_init_cell(batch_size, rnn_size):\n",
        "    def build_cell(lstm_size):\n",
        "        # Use a basic LSTM cell\n",
        "        lstm = tf2.compat.v1.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
        "        \n",
        "        # Add dropout to the cell\n",
        "        drop = tf2.compat.v1.nn.rnn_cell.DropoutWrapper(lstm)#, output_keep_prob=keep_prob)\n",
        "        return drop\n",
        "    \n",
        "    \n",
        "    # Stack up multiple LSTM layers, for deep learning\n",
        "    num_layers = 2\n",
        "    cell = tf2.compat.v1.nn.rnn_cell.MultiRNNCell([build_cell(rnn_size) for _ in range(num_layers)])\n",
        "    initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
        "    initial_state = tf.identity(initial_state, name=\"initial_state\")\n",
        "    return (cell, initial_state)\n",
        "    \n",
        "def get_embed(input_data, vocab_size, embed_dim):\n",
        "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
        "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
        "    return embed\n",
        "    \n",
        "def build_rnn(cell, inputs):\n",
        "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)#,initial_state=initial_state)\n",
        "    final_state = tf.identity(final_state, name=\"final_state\")\n",
        "    return outputs, final_state\n",
        "    \n",
        "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
        "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
        "    outputs, final_state = build_rnn(cell, embed)\n",
        "    \n",
        "    logits = tf2.compat.v1.layers.dense(outputs, vocab_size, activation=None)\n",
        "    return logits, final_state\n",
        "    \n",
        "def get_batches(int_text, batch_size, seq_length):\n",
        "    words_per_batch = batch_size * seq_length\n",
        "    num_batch = len(int_text)//words_per_batch\n",
        "    \n",
        "    input_text = np.array(int_text[:(num_batch*words_per_batch)])\n",
        "    target_text = np.array(int_text[1:(num_batch * words_per_batch) + 1])\n",
        "\n",
        "    target_text[-1] = input_text[0]\n",
        "    \n",
        "    x_batches = np.split(input_text.reshape(batch_size, -1), num_batch, 1)\n",
        "    y_batches = np.split(target_text.reshape(batch_size, -1), num_batch, 1)\n",
        "    \n",
        "    batches = np.array(list(zip(x_batches, y_batches)))\n",
        "    return batches\n",
        "    \n",
        "# Number of Epochs\n",
        "num_epochs = 100\n",
        "# Batch Size\n",
        "batch_size = 100\n",
        "# RNN Size\n",
        "rnn_size = 512\n",
        "# Embedding Dimension Size\n",
        "embed_dim = 256\n",
        "# Sequence Length\n",
        "seq_length = 15\n",
        "# Learning Rate\n",
        "learning_rate = 0.001\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 50\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/nlp_project/models/model_2_postag/save'\n",
        "\n",
        "batches = get_batches(train_int_text, batch_size, seq_length)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FGE8LJSmyzU",
        "outputId": "afad1b56-26e3-43f1-b85a-b09d0c6f0903"
      },
      "source": [
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        "    vocab_size = len(int_to_vocab)\n",
        "    input_text, targets, lr = get_inputs()\n",
        "    input_data_shape = tf.shape(input_text)\n",
        "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
        "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
        "\n",
        "    # Probabilities for generating words\n",
        "    probs = tf.nn.softmax(logits, name='probs')\n",
        "\n",
        "    # Loss function\n",
        "    cost = tfa.seq2seq.sequence_loss(\n",
        "        logits,\n",
        "        targets,\n",
        "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "    # Gradient Clipping\n",
        "    gradients = optimizer.compute_gradients(cost)\n",
        "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
        "    train_op = optimizer.apply_gradients(capped_gradients)\n",
        "    \n",
        "#batches = get_batches(train_int_text, batch_size, seq_length)\n",
        "#train_graph = tf.Graph()\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(num_epochs):\n",
        "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
        "\n",
        "        for batch_i, (x, y) in enumerate(batches):\n",
        "            feed = {\n",
        "                input_text: x,\n",
        "                targets: y,\n",
        "                initial_state: state,\n",
        "                lr: learning_rate}\n",
        "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
        "\n",
        "            # Show every <show_every_n_batches> batches\n",
        "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
        "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
        "                    epoch_i,\n",
        "                    batch_i,\n",
        "                    len(batches),\n",
        "                    train_loss))\n",
        "\n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, save_dir)\n",
        "    print('Model Trained and Saved')\n",
        "    \n",
        "# Save parameters for checkpoint\n",
        "#helper.save_params((seq_length, save_dir))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:702: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Batch    0/12   train_loss = 8.523\n",
            "Epoch   4 Batch    2/12   train_loss = 6.882\n",
            "Epoch   8 Batch    4/12   train_loss = 6.668\n",
            "Epoch  12 Batch    6/12   train_loss = 6.182\n",
            "Epoch  16 Batch    8/12   train_loss = 5.714\n",
            "Epoch  20 Batch   10/12   train_loss = 5.390\n",
            "Epoch  25 Batch    0/12   train_loss = 4.828\n",
            "Epoch  29 Batch    2/12   train_loss = 4.553\n",
            "Epoch  33 Batch    4/12   train_loss = 4.260\n",
            "Epoch  37 Batch    6/12   train_loss = 3.873\n",
            "Epoch  41 Batch    8/12   train_loss = 3.345\n",
            "Epoch  45 Batch   10/12   train_loss = 3.094\n",
            "Epoch  50 Batch    0/12   train_loss = 2.526\n",
            "Epoch  54 Batch    2/12   train_loss = 2.487\n",
            "Epoch  58 Batch    4/12   train_loss = 2.020\n",
            "Epoch  62 Batch    6/12   train_loss = 1.799\n",
            "Epoch  66 Batch    8/12   train_loss = 1.524\n",
            "Epoch  70 Batch   10/12   train_loss = 1.324\n",
            "Epoch  75 Batch    0/12   train_loss = 1.061\n",
            "Epoch  79 Batch    2/12   train_loss = 0.913\n",
            "Epoch  83 Batch    4/12   train_loss = 0.719\n",
            "Epoch  87 Batch    6/12   train_loss = 0.636\n",
            "Epoch  91 Batch    8/12   train_loss = 0.501\n",
            "Epoch  95 Batch   10/12   train_loss = 0.362\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCONpYJHm1ep",
        "outputId": "ac5a0c5d-9e0b-4e60-ece0-9cf52e41ad17"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#import helper\n",
        "\n",
        "def pick_word(probabilities, int_to_vocab):\n",
        "    \"\"\"\n",
        "    Pick the next word in the generated text\n",
        "    :param probabilities: Probabilites of the next word\n",
        "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
        "    :return: String of the predicted word\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    index = np.where(probabilities == np.max(probabilities))[-1][0]\n",
        "\n",
        "    return int_to_vocab[index]\n",
        "\n",
        "#_,_,_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "seq_length, load_dir = 15, '/content/drive/MyDrive/nlp_project/models/model_2_postag/save'\n",
        "token_dict = token_lookup()\n",
        "def get_tensors(loaded_graph):\n",
        "    input_tensor = loaded_graph.get_tensor_by_name(\"input:0\")\n",
        "    initial_state_tensor = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
        "    final_state_tensor = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
        "    prob_tensor = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
        "\n",
        "    return input_tensor, initial_state_tensor, final_state_tensor, prob_tensor\n",
        "    \n",
        "#test_text = [int_to_vocab[word] for word in test_int_text]\n",
        "#test_text = \" \".join(test_text)\n",
        "#test_sent_text = test_text.split(\"<period>\")\n",
        "\n",
        "'''\n",
        "test_sentences = input(\"Enter sentence:\")\n",
        "token_dict = token_lookup()\n",
        "for key, token in token_dict.items():\n",
        "\ttest_sentences = test_sentences.replace(key, ' {} '.format(token))\n",
        "test_sentences = test_sentences.lower()\n",
        "test_sentences = test_sentences.split(\"<period>\")\n",
        "'''\n",
        "#all_perp = []\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.compat.v1.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.compat.v1.train.import_meta_graph(load_dir + '.meta')\n",
        "    loader.restore(sess, load_dir)\n",
        "\n",
        "    # Get Tensors from loaded model\n",
        "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
        "    #print(final_state)\n",
        "    # Sentences generation setup\n",
        "    gen_length = 200\n",
        "    prime_word = \"Chalo_hi_PRT\"#\"Subah_hi_PART\" #'यही' #'moe_szyslak'\n",
        "    gen_sentences = [prime_word]\n",
        "            \n",
        "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
        "\n",
        "    # Generate sentences\n",
        "    for n in range(gen_length):\n",
        "        # Dynamic Input\n",
        "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
        "        dyn_seq_length = len(dyn_input[0])\n",
        "          \n",
        "        # Get Prediction\n",
        "        probabilities, prev_state = sess.run(\n",
        "            [probs, final_state],\n",
        "            {input_text: dyn_input, initial_state: prev_state})\n",
        "\n",
        "        #pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
        "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
        "\n",
        "        gen_sentences.append(pred_word)\n",
        "\n",
        "    # Remove tokens\n",
        "    script = []\n",
        "    for word in gen_sentences:\n",
        "        script.append(word.split(\"_\")[0])\n",
        "    tv_script = ' '.join(script)\n",
        "    for key, token in token_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
        "    tv_script = tv_script.replace('\\n ', '\\n')\n",
        "    tv_script = tv_script.replace('( ', '(')\n",
        "        \n",
        "    print(tv_script)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/nlp_project/models/model_2_postag/save\n",
            "Chalo United this guys <EOS> <BOS> I was a total story and I was in IITB way we had a good time to have a good minutes It seemed like the other guy who would have a good time to see who did it was laughing a stamp and it When I see a good minutes who came to the story that I was in the hostel bogs I was going to get noisy so I gave me completely I get into the guy in the lab so strongarm wingmates education we I have a good time to attend so for much the guy in the lab The Bisleri bottle <EOS> <BOS> I am a girl in IITB and just so in my life and mean just broke his heart late do do it do you are no one things are Papa his new points with me of the way with the top english letters in the lab as For a thought He is for sitting on Rs of the secrets is his peace of growing old however our around and used to bug the rest in India <EOS> <BOS> I am a good escort from IITB I have been smoking\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLD1M9zc3rNa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}