{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_LM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb5NCgQXvZZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de24e1c9-7f91-4733-ab20-53ffe5d1c0de"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1Oo4Mq-tcjf"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkPLqoOubc2_",
        "outputId": "784bfab4-ee11-4675-fc2b-d01d39c0c393"
      },
      "source": [
        "import numpy as np\n",
        "    \n",
        "def token_lookup():\n",
        "    token_dict = {\".\":\"<PERIOD>\", \",\": \"<COMMA>\", \";\":\"<SEMICOLON>\", \":\":\"<COLON>\", \"\\\"\":\"<QOUTATION>\", \"'\":\"QOUTATION\", \"!\":\"<EXCLAMATION>\", \"?\":\"<QUESTION>\", \n",
        "                  \"(\": \"<LEFTPARAN>\", \")\": \"<RIGHTPARAN>\", \"{\":\"LEFTBRACE\", \"}\":\"<RIGHTBRACE>\", \"[\":\"<LEFTBRACKET>\", \"]\":\"<RIGHTBRACKET>\", \"--\":\"<DASH>\", \"-\":\"<HYPHEN>\",\n",
        "                  \"\\n\":\"<RETURN>\"}\n",
        "    return token_dict\n",
        "\n",
        "token_dict = token_lookup()\n",
        "\n",
        "english_text = open(\"./plot.tok.gt9.5000\").read()#open(\"./plot_quote.10000\").read()\n",
        "\n",
        "for key, token in token_dict.items():\n",
        "    english_text = english_text.replace(key, ' {} '.format(token))\n",
        "english_text = english_text.lower()\n",
        "english_text = english_text.split()\n",
        "english_text = ' '.join(english_text)\n",
        "\n",
        "english_text = english_text.split(\"<period>\")\n",
        "english_text = pd.DataFrame(english_text, columns=[\"sentences\"])\n",
        "\n",
        "#hindi_text = pd.read_csv(\"./hindi_train.csv\").drop(columns=['experience']).iloc[:50]\n",
        "mix_text = open(\"./codemix.txt\").read()\n",
        "\n",
        "for key, token in token_dict.items():\n",
        "    mix_text = mix_text.replace(key, ' {} '.format(token))\n",
        "mix_text = mix_text.lower()\n",
        "mix_text = mix_text.split()\n",
        "mix_text = ' '.join(mix_text)\n",
        "\n",
        "mix_text = mix_text.split(\"<period>\")\n",
        "mix_text = pd.DataFrame(mix_text, columns=[\"sentences\"])\n",
        "mix_text.head"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                sentences\n",
              "0      the biggest drawback of इस योजना is that इस सब...\n",
              "1                 <return> दिल्ली is an urban territory \n",
              "2       <return> there banks are in बैंक बहुतायत and ...\n",
              "3       <return> यही बात can also be said about आधार ...\n",
              "4       <return> but various complaints are आ रही हैं...\n",
              "...                                                  ...\n",
              "26394                         <return> give सबको खींचके \n",
              "26395   <return> क्यों krishna हमें क्यों slap देंगी ...\n",
              "26396                   <return> लेकिन today फिर i lost \n",
              "26397                 <return> you मुझसे तकलीफ़ नहीं है \n",
              "26398                <return> you तकलीफ़ इस बात <return>\n",
              "\n",
              "[26399 rows x 1 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "E19rBnfY4IJg",
        "outputId": "521531b9-b77e-4010-c6ba-4bac57559fbb"
      },
      "source": [
        "en_mix = pd.concat([english_text, mix_text], axis=0)\n",
        "en_mix = en_mix.sample(frac = 1)\n",
        "en_mix.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13973</th>\n",
              "      <td>&lt;return&gt; but who did force तुझे to be a statu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>&lt;return&gt; lauren hynde &lt;leftparan&gt; shannyn sos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24101</th>\n",
              "      <td>&lt;return&gt; काले कीड़े की भाँति black insect मूल...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5555</th>\n",
              "      <td>&lt;return&gt; सर्वोत्तम समय for replanting is from...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18925</th>\n",
              "      <td>&lt;return&gt; some plants सेमी शेड की require होती...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentences\n",
              "13973   <return> but who did force तुझे to be a statu...\n",
              "381     <return> lauren hynde <leftparan> shannyn sos...\n",
              "24101   <return> काले कीड़े की भाँति black insect मूल...\n",
              "5555    <return> सर्वोत्तम समय for replanting is from...\n",
              "18925   <return> some plants सेमी शेड की require होती..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XSVWHQi8RzY",
        "outputId": "673bfdc3-5589-40da-ccfa-5cdec63b85ec"
      },
      "source": [
        "def create_lookup_tables(text_df):\n",
        "    vocab = set()\n",
        "    for line in text_df['sentences']:\n",
        "        vocab.update(set(line.split()))\n",
        "    #vocab = set(text)\n",
        "    vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
        "    int_to_vocab = dict(enumerate(vocab))\n",
        "    return (vocab_to_int, int_to_vocab)\n",
        "\n",
        "vocab_to_int, int_to_vocab = create_lookup_tables(en_mix)\n",
        "\n",
        "train_int_text = []\n",
        "\n",
        "for line in en_mix['sentences']:\n",
        "    train_int_text.append([vocab_to_int[word] for word in line.split()])\n",
        "\n",
        "train_int_text = [item for sublist in train_int_text for item in sublist]\n",
        "print(train_int_text[:1000])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6504, 11206, 13823, 5912, 535, 742, 19279, 11062, 16472, 27180, 20683, 6504, 28452, 30118, 12425, 33929, 18548, 30118, 13823, 30352, 30134, 30396, 19279, 11145, 28284, 1731, 20283, 32195, 12425, 20283, 32264, 6504, 36651, 30545, 12676, 32133, 7540, 36591, 5938, 2185, 16472, 22239, 6504, 22824, 36273, 19144, 35439, 11391, 13640, 22835, 30118, 12315, 30118, 8470, 35591, 10700, 32432, 9820, 34077, 12253, 289, 203, 36074, 7187, 6504, 34715, 8819, 29675, 27494, 5938, 1731, 20283, 1095, 36927, 12425, 11493, 19279, 6022, 12425, 3183, 29416, 6504, 1274, 12253, 10688, 27937, 19144, 36590, 11072, 7187, 6504, 20283, 7075, 20283, 9090, 4650, 35591, 5361, 21060, 18573, 35583, 11072, 7187, 6504, 7481, 4493, 34415, 20283, 3271, 1731, 7234, 26567, 7848, 21222, 17173, 4202, 17192, 12425, 21222, 29816, 13198, 4493, 11062, 20644, 34296, 6482, 7187, 20114, 21172, 12425, 16749, 13179, 36565, 25521, 6504, 27432, 17644, 8493, 8487, 18459, 30661, 30203, 1293, 20283, 20811, 6690, 25119, 3234, 34995, 1731, 22650, 33122, 6504, 11696, 3245, 9955, 26081, 1731, 34312, 8377, 21638, 5793, 13198, 34573, 29349, 33867, 13198, 8334, 9617, 6504, 27432, 15109, 983, 16786, 29675, 16052, 20114, 16472, 10411, 16052, 22182, 670, 21108, 26569, 7056, 29844, 30118, 34111, 1889, 8501, 19279, 16472, 28777, 15677, 3773, 1731, 16472, 4685, 10411, 35954, 9650, 15709, 4593, 22210, 36963, 30118, 13198, 20283, 7589, 7451, 1731, 17644, 6504, 11206, 19377, 15461, 5938, 30134, 20167, 6504, 1731, 20283, 16342, 12425, 30025, 30118, 10279, 31909, 4502, 5284, 20283, 10258, 33606, 30118, 4548, 10137, 4502, 32297, 31619, 7187, 22728, 6504, 22650, 37227, 2827, 24863, 18234, 9820, 17650, 5710, 36608, 33682, 7187, 6504, 20320, 13847, 31247, 30118, 11206, 17537, 16472, 29961, 36953, 4145, 3068, 1923, 22210, 23540, 5938, 5702, 10274, 12425, 16136, 6504, 20283, 9845, 20283, 25211, 20283, 5697, 6016, 6504, 34546, 19569, 18034, 8819, 289, 30118, 4517, 30211, 20283, 17970, 18521, 19219, 18164, 289, 11072, 7187, 6504, 34522, 5938, 22650, 22210, 19279, 27250, 5290, 508, 12425, 21976, 13198, 11217, 20237, 5938, 36476, 22210, 8304, 2877, 12425, 20826, 34071, 28474, 21499, 6504, 15655, 19279, 27250, 34296, 8536, 19433, 30118, 17278, 13179, 1938, 1731, 36014, 20225, 29675, 15334, 3245, 9955, 30118, 17644, 5938, 31772, 6504, 19279, 4282, 10279, 27864, 20283, 533, 12425, 20283, 12546, 6504, 10224, 19559, 34327, 8839, 5149, 29182, 36176, 34087, 19385, 13198, 20283, 611, 12425, 20283, 29182, 36176, 5938, 11419, 13198, 18587, 6504, 11206, 22175, 33946, 14081, 26569, 135, 13198, 24990, 10921, 6504, 20283, 12979, 30118, 7119, 13198, 8174, 12425, 28808, 36148, 1868, 30535, 6504, 24391, 15797, 14685, 3264, 9820, 27035, 33004, 28148, 27035, 33738, 2218, 20829, 25060, 34296, 34435, 19545, 7109, 1202, 32127, 13340, 30185, 23719, 706, 24790, 7187, 6504, 20539, 28672, 26283, 20114, 20283, 37092, 30118, 13198, 17876, 24233, 20541, 5256, 35023, 36541, 6504, 30008, 1179, 33813, 11157, 7848, 28575, 21997, 5702, 25527, 12425, 20283, 19424, 20486, 6504, 32814, 5938, 8175, 19279, 32245, 32245, 12425, 36122, 5702, 11162, 13179, 21638, 12362, 17657, 18681, 5296, 22895, 22136, 2362, 10279, 30329, 26569, 22650, 33250, 6504, 29675, 6311, 1006, 21759, 16622, 12425, 32096, 4502, 28418, 22419, 10613, 15851, 36719, 19279, 28418, 12888, 33086, 33627, 6504, 15467, 19007, 9820, 16931, 19219, 24625, 20283, 28808, 14546, 26313, 19219, 29201, 21638, 6440, 33573, 27171, 16947, 10032, 24377, 18650, 29449, 6504, 19665, 22353, 5938, 36672, 9598, 36536, 13823, 13342, 10592, 7187, 30758, 12425, 20700, 13198, 31800, 11986, 30684, 21831, 6504, 34826, 5966, 26313, 7848, 36208, 12425, 9779, 13198, 4574, 15774, 7848, 16597, 12425, 12395, 6504, 32592, 21542, 5938, 17657, 22650, 30118, 22210, 23741, 5912, 29116, 26326, 23383, 24497, 10525, 20322, 19279, 21067, 1311, 28284, 12425, 2056, 24198, 3068, 6504, 11206, 26693, 30254, 14305, 12417, 13515, 35173, 5793, 30118, 27432, 26761, 33326, 30932, 24198, 18746, 6504, 17512, 4502, 19279, 10273, 5702, 31242, 5567, 35864, 12425, 23, 6504, 36833, 14305, 8180, 30672, 20168, 13179, 20211, 6504, 19279, 30681, 14652, 5869, 25530, 15669, 21997, 5938, 15655, 19279, 2107, 10478, 35790, 27215, 6504, 5296, 21638, 3068, 11062, 7404, 29247, 7848, 26696, 14699, 7598, 12425, 21997, 11098, 35682, 20283, 4542, 4465, 12425, 26365, 5702, 19187, 33813, 32654, 6504, 16472, 2319, 21907, 19144, 20771, 27411, 36169, 7187, 6504, 20283, 10345, 233, 12425, 10224, 14305, 17797, 32919, 26569, 3463, 2827, 18164, 6504, 22650, 5456, 22210, 20539, 5938, 32960, 19279, 30590, 5927, 6504, 2835, 28626, 19219, 6340, 22413, 21638, 14790, 24377, 2713, 19545, 6440, 22427, 26827, 31603, 14790, 19144, 36169, 7187, 6504, 4548, 629, 33844, 5702, 12410, 26326, 30236, 29357, 15888, 35019, 6504, 20283, 15146, 10032, 30844, 5255, 19219, 9845, 11709, 13179, 10509, 11709, 7351, 612, 5393, 6504, 20283, 27589, 20283, 11423, 33024, 24846, 140, 33024, 24846, 11072, 7187, 6504, 19936, 16260, 3463, 32867, 4502, 26493, 33169, 6945, 20114, 13264, 11852, 6504, 30731, 8334, 29116, 26326, 23383, 11274, 13619, 30118, 21638, 5938, 8175, 19279, 14238, 16472, 21527, 23018, 16634, 17657, 36014, 5702, 20283, 22561, 12807, 6504, 13504, 13179, 18164, 20283, 6057, 20283, 7133, 17546, 9449, 16254, 22631, 19144, 29848, 7162, 29449, 6504, 20283, 36260, 12425, 23540, 13108, 13198, 8170, 8683, 12425, 13108, 34071, 11062, 31675, 19279, 20283, 1265, 5938, 36476, 29675, 35360, 1265, 17912, 6504, 20283, 3593, 18717, 12425, 20283, 20062, 13198, 21349, 7119, 14305, 25007, 17786, 33682, 7187, 6504, 30632, 23740, 20283, 12182, 26326, 30236, 18882, 18681, 8342, 29796, 8444, 19279, 8342, 30070, 18746, 30118, 11206, 11696, 20539, 10731, 8342, 36321, 30583, 30118, 30632, 4502, 19279, 10273, 20283, 36847, 26326, 30236, 34962, 15976, 1639, 30118, 17051, 34395, 15477, 14305, 34075, 36463, 18745, 29055, 15582, 4511, 6816, 36682, 6504, 20283, 7560, 22617, 5277, 5710, 12811, 33504, 28549, 14685, 15400, 7187, 6504, 26510, 26964, 5710, 31298, 12709, 15832, 19509, 23747, 19219, 18126, 8259, 16144, 5897, 1643, 9820, 8286, 19219, 3245, 19915, 19016, 28474, 6504, 5714, 19219, 333, 11268, 31816, 23466, 26024, 19219, 6936, 18650, 29449, 6504, 3245, 10032, 11203, 21231, 14305, 32360, 5702, 22717, 14444, 30118, 4693, 9112, 29799, 26964, 30310, 13166, 17983, 1731, 19395, 19242, 13198, 9531, 35247, 3843, 14305, 26493, 17537, 32360, 17644, 6504, 17369, 7351, 21268, 12676, 2866, 36591, 9820, 7669, 33462, 20283, 7188, 12695, 207, 1124, 35212, 12535, 10209, 15011, 17786, 7187, 6504, 8395, 8253, 2827, 20587, 13179, 19377, 2827, 22650, 6306, 23823, 11472, 9820, 5567, 13603, 19219, 3156, 18573, 20283, 36814, 16945, 36169, 7187, 30118, 20283]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpD_QsD8D5Ri",
        "outputId": "db7ec8da-5fea-416e-aa60-99ba465d0ad0"
      },
      "source": [
        "print(en_mix.shape, len(vocab_to_int), max(train_int_text))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31787, 1) 37262 37261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykY6CdowvgQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bbb6f37-899c-4ed9-f233-6edc773c47a9"
      },
      "source": [
        "#import helper\n",
        "\n",
        "#data_dir = './brown.txt'\n",
        "\n",
        "#helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)\n",
        "\n",
        "from distutils.version import LooseVersion\n",
        "import warnings\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "import tensorflow as tf2\n",
        "#import helper\n",
        "import numpy as np\n",
        "\n",
        "#int_text, val_int_text, test_int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "\n",
        "def get_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    learn_rate = tf.placeholder(tf.float32, name='learn_rate')\n",
        "    return (inputs, targets, learn_rate)\n",
        "    \n",
        "def get_init_cell(batch_size, rnn_size):\n",
        "    def build_cell(lstm_size):\n",
        "        # Use a basic LSTM cell\n",
        "        lstm = tf2.compat.v1.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
        "        \n",
        "        # Add dropout to the cell\n",
        "        drop = tf2.compat.v1.nn.rnn_cell.DropoutWrapper(lstm)#, output_keep_prob=keep_prob)\n",
        "        return drop\n",
        "    \n",
        "    \n",
        "    # Stack up multiple LSTM layers, for deep learning\n",
        "    num_layers = 2\n",
        "    cell = tf2.compat.v1.nn.rnn_cell.MultiRNNCell([build_cell(rnn_size) for _ in range(num_layers)])\n",
        "    initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
        "    initial_state = tf.identity(initial_state, name=\"initial_state\")\n",
        "    return (cell, initial_state)\n",
        "    \n",
        "def get_embed(input_data, vocab_size, embed_dim):\n",
        "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
        "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
        "    return embed\n",
        "    \n",
        "def build_rnn(cell, inputs):\n",
        "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)#,initial_state=initial_state)\n",
        "    final_state = tf.identity(final_state, name=\"final_state\")\n",
        "    return outputs, final_state\n",
        "    \n",
        "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
        "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
        "    outputs, final_state = build_rnn(cell, embed)\n",
        "    \n",
        "    logits = tf2.compat.v1.layers.dense(outputs, vocab_size, activation=None)\n",
        "    return logits, final_state\n",
        "    \n",
        "def get_batches(int_text, batch_size, seq_length):\n",
        "    words_per_batch = batch_size * seq_length\n",
        "    num_batch = len(int_text)//words_per_batch\n",
        "    \n",
        "    input_text = np.array(int_text[:(num_batch*words_per_batch)])\n",
        "    target_text = np.array(int_text[1:(num_batch * words_per_batch) + 1])\n",
        "\n",
        "    target_text[-1] = input_text[0]\n",
        "    \n",
        "    x_batches = np.split(input_text.reshape(batch_size, -1), num_batch, 1)\n",
        "    y_batches = np.split(target_text.reshape(batch_size, -1), num_batch, 1)\n",
        "    \n",
        "    batches = np.array(list(zip(x_batches, y_batches)))\n",
        "    return batches\n",
        "    \n",
        "# Number of Epochs\n",
        "num_epochs = 50\n",
        "# Batch Size\n",
        "batch_size = 100\n",
        "# RNN Size\n",
        "rnn_size = 512\n",
        "# Embedding Dimension Size\n",
        "embed_dim = 256\n",
        "# Sequence Length\n",
        "seq_length = 15\n",
        "# Learning Rate\n",
        "learning_rate = 0.001\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 50\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/nlp_project/models/model_1/save'\n",
        "\n",
        "batches = get_batches(train_int_text, batch_size, seq_length)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIVh3N-SLMF6",
        "outputId": "87f5ba53-b56b-429a-8e68-221fe21a9dff"
      },
      "source": [
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        "    vocab_size = len(int_to_vocab)\n",
        "    input_text, targets, lr = get_inputs()\n",
        "    input_data_shape = tf.shape(input_text)\n",
        "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
        "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
        "\n",
        "    # Probabilities for generating words\n",
        "    probs = tf.nn.softmax(logits, name='probs')\n",
        "\n",
        "    # Loss function\n",
        "    cost = tfa.seq2seq.sequence_loss(\n",
        "        logits,\n",
        "        targets,\n",
        "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "    # Gradient Clipping\n",
        "    gradients = optimizer.compute_gradients(cost)\n",
        "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
        "    train_op = optimizer.apply_gradients(capped_gradients)\n",
        "    \n",
        "#batches = get_batches(train_int_text, batch_size, seq_length)\n",
        "#train_graph = tf.Graph()\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(num_epochs):\n",
        "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
        "\n",
        "        for batch_i, (x, y) in enumerate(batches):\n",
        "            feed = {\n",
        "                input_text: x,\n",
        "                targets: y,\n",
        "                initial_state: state,\n",
        "                lr: learning_rate}\n",
        "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
        "\n",
        "            # Show every <show_every_n_batches> batches\n",
        "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
        "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
        "                    epoch_i,\n",
        "                    batch_i,\n",
        "                    len(batches),\n",
        "                    train_loss))\n",
        "\n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, save_dir)\n",
        "    print('Model Trained and Saved')\n",
        "    \n",
        "# Save parameters for checkpoint\n",
        "#helper.save_params((seq_length, save_dir))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\r\u001b[K     |▌                               | 10kB 24.6MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 15.5MB/s eta 0:00:01\r\u001b[K     |█▍                              | 30kB 13.8MB/s eta 0:00:01\r\u001b[K     |█▉                              | 40kB 12.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 51kB 8.0MB/s eta 0:00:01\r\u001b[K     |██▉                             | 61kB 7.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |███▊                            | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 112kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 122kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 133kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 143kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 153kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 163kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 174kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 184kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 194kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 204kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 215kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 225kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 235kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 245kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 256kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 266kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 276kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 286kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 296kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 307kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 317kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 327kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 337kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 348kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 358kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 368kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 378kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 389kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 399kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 409kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 419kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 430kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 440kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 450kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 460kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 471kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 481kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 491kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 501kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 512kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 522kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 532kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 542kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 552kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 563kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 573kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 583kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 593kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 604kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 614kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 624kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 634kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 645kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 655kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 665kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 675kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 686kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 696kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 706kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.12.1\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-7-4057dd07a7b5>:47: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:702: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:753: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Batch    0/394   train_loss = 10.526\n",
            "Epoch   0 Batch   50/394   train_loss = 8.302\n",
            "Epoch   0 Batch  100/394   train_loss = 8.081\n",
            "Epoch   0 Batch  150/394   train_loss = 8.079\n",
            "Epoch   0 Batch  200/394   train_loss = 7.891\n",
            "Epoch   0 Batch  250/394   train_loss = 7.989\n",
            "Epoch   0 Batch  300/394   train_loss = 7.882\n",
            "Epoch   0 Batch  350/394   train_loss = 7.598\n",
            "Epoch   1 Batch    6/394   train_loss = 7.411\n",
            "Epoch   1 Batch   56/394   train_loss = 7.441\n",
            "Epoch   1 Batch  106/394   train_loss = 7.277\n",
            "Epoch   1 Batch  156/394   train_loss = 7.279\n",
            "Epoch   1 Batch  206/394   train_loss = 7.147\n",
            "Epoch   1 Batch  256/394   train_loss = 7.167\n",
            "Epoch   1 Batch  306/394   train_loss = 7.230\n",
            "Epoch   1 Batch  356/394   train_loss = 6.987\n",
            "Epoch   2 Batch   12/394   train_loss = 6.725\n",
            "Epoch   2 Batch   62/394   train_loss = 6.826\n",
            "Epoch   2 Batch  112/394   train_loss = 6.658\n",
            "Epoch   2 Batch  162/394   train_loss = 6.663\n",
            "Epoch   2 Batch  212/394   train_loss = 6.568\n",
            "Epoch   2 Batch  262/394   train_loss = 6.557\n",
            "Epoch   2 Batch  312/394   train_loss = 6.484\n",
            "Epoch   2 Batch  362/394   train_loss = 6.391\n",
            "Epoch   3 Batch   18/394   train_loss = 6.230\n",
            "Epoch   3 Batch   68/394   train_loss = 6.294\n",
            "Epoch   3 Batch  118/394   train_loss = 6.186\n",
            "Epoch   3 Batch  168/394   train_loss = 6.157\n",
            "Epoch   3 Batch  218/394   train_loss = 6.160\n",
            "Epoch   3 Batch  268/394   train_loss = 6.189\n",
            "Epoch   3 Batch  318/394   train_loss = 6.125\n",
            "Epoch   3 Batch  368/394   train_loss = 6.026\n",
            "Epoch   4 Batch   24/394   train_loss = 5.879\n",
            "Epoch   4 Batch   74/394   train_loss = 5.750\n",
            "Epoch   4 Batch  124/394   train_loss = 6.007\n",
            "Epoch   4 Batch  174/394   train_loss = 5.776\n",
            "Epoch   4 Batch  224/394   train_loss = 5.784\n",
            "Epoch   4 Batch  274/394   train_loss = 5.810\n",
            "Epoch   4 Batch  324/394   train_loss = 5.645\n",
            "Epoch   4 Batch  374/394   train_loss = 5.620\n",
            "Epoch   5 Batch   30/394   train_loss = 5.718\n",
            "Epoch   5 Batch   80/394   train_loss = 5.510\n",
            "Epoch   5 Batch  130/394   train_loss = 5.446\n",
            "Epoch   5 Batch  180/394   train_loss = 5.536\n",
            "Epoch   5 Batch  230/394   train_loss = 5.340\n",
            "Epoch   5 Batch  280/394   train_loss = 5.450\n",
            "Epoch   5 Batch  330/394   train_loss = 5.338\n",
            "Epoch   5 Batch  380/394   train_loss = 5.255\n",
            "Epoch   6 Batch   36/394   train_loss = 5.370\n",
            "Epoch   6 Batch   86/394   train_loss = 5.227\n",
            "Epoch   6 Batch  136/394   train_loss = 5.368\n",
            "Epoch   6 Batch  186/394   train_loss = 5.212\n",
            "Epoch   6 Batch  236/394   train_loss = 5.278\n",
            "Epoch   6 Batch  286/394   train_loss = 5.085\n",
            "Epoch   6 Batch  336/394   train_loss = 5.063\n",
            "Epoch   6 Batch  386/394   train_loss = 5.037\n",
            "Epoch   7 Batch   42/394   train_loss = 5.060\n",
            "Epoch   7 Batch   92/394   train_loss = 5.073\n",
            "Epoch   7 Batch  142/394   train_loss = 5.090\n",
            "Epoch   7 Batch  192/394   train_loss = 4.819\n",
            "Epoch   7 Batch  242/394   train_loss = 4.849\n",
            "Epoch   7 Batch  292/394   train_loss = 4.835\n",
            "Epoch   7 Batch  342/394   train_loss = 4.783\n",
            "Epoch   7 Batch  392/394   train_loss = 4.840\n",
            "Epoch   8 Batch   48/394   train_loss = 4.783\n",
            "Epoch   8 Batch   98/394   train_loss = 4.668\n",
            "Epoch   8 Batch  148/394   train_loss = 4.703\n",
            "Epoch   8 Batch  198/394   train_loss = 4.681\n",
            "Epoch   8 Batch  248/394   train_loss = 4.623\n",
            "Epoch   8 Batch  298/394   train_loss = 4.680\n",
            "Epoch   8 Batch  348/394   train_loss = 4.511\n",
            "Epoch   9 Batch    4/394   train_loss = 4.567\n",
            "Epoch   9 Batch   54/394   train_loss = 4.528\n",
            "Epoch   9 Batch  104/394   train_loss = 4.542\n",
            "Epoch   9 Batch  154/394   train_loss = 4.393\n",
            "Epoch   9 Batch  204/394   train_loss = 4.542\n",
            "Epoch   9 Batch  254/394   train_loss = 4.399\n",
            "Epoch   9 Batch  304/394   train_loss = 4.327\n",
            "Epoch   9 Batch  354/394   train_loss = 4.192\n",
            "Epoch  10 Batch   10/394   train_loss = 4.359\n",
            "Epoch  10 Batch   60/394   train_loss = 4.311\n",
            "Epoch  10 Batch  110/394   train_loss = 4.159\n",
            "Epoch  10 Batch  160/394   train_loss = 4.210\n",
            "Epoch  10 Batch  210/394   train_loss = 4.132\n",
            "Epoch  10 Batch  260/394   train_loss = 4.157\n",
            "Epoch  10 Batch  310/394   train_loss = 4.104\n",
            "Epoch  10 Batch  360/394   train_loss = 4.070\n",
            "Epoch  11 Batch   16/394   train_loss = 4.062\n",
            "Epoch  11 Batch   66/394   train_loss = 4.101\n",
            "Epoch  11 Batch  116/394   train_loss = 4.019\n",
            "Epoch  11 Batch  166/394   train_loss = 4.008\n",
            "Epoch  11 Batch  216/394   train_loss = 4.020\n",
            "Epoch  11 Batch  266/394   train_loss = 3.883\n",
            "Epoch  11 Batch  316/394   train_loss = 3.899\n",
            "Epoch  11 Batch  366/394   train_loss = 3.830\n",
            "Epoch  12 Batch   22/394   train_loss = 3.759\n",
            "Epoch  12 Batch   72/394   train_loss = 3.860\n",
            "Epoch  12 Batch  122/394   train_loss = 3.753\n",
            "Epoch  12 Batch  172/394   train_loss = 3.777\n",
            "Epoch  12 Batch  222/394   train_loss = 3.659\n",
            "Epoch  12 Batch  272/394   train_loss = 3.759\n",
            "Epoch  12 Batch  322/394   train_loss = 3.637\n",
            "Epoch  12 Batch  372/394   train_loss = 3.602\n",
            "Epoch  13 Batch   28/394   train_loss = 3.622\n",
            "Epoch  13 Batch   78/394   train_loss = 3.559\n",
            "Epoch  13 Batch  128/394   train_loss = 3.513\n",
            "Epoch  13 Batch  178/394   train_loss = 3.462\n",
            "Epoch  13 Batch  228/394   train_loss = 3.548\n",
            "Epoch  13 Batch  278/394   train_loss = 3.559\n",
            "Epoch  13 Batch  328/394   train_loss = 3.407\n",
            "Epoch  13 Batch  378/394   train_loss = 3.410\n",
            "Epoch  14 Batch   34/394   train_loss = 3.380\n",
            "Epoch  14 Batch   84/394   train_loss = 3.385\n",
            "Epoch  14 Batch  134/394   train_loss = 3.428\n",
            "Epoch  14 Batch  184/394   train_loss = 3.395\n",
            "Epoch  14 Batch  234/394   train_loss = 3.346\n",
            "Epoch  14 Batch  284/394   train_loss = 3.246\n",
            "Epoch  14 Batch  334/394   train_loss = 3.118\n",
            "Epoch  14 Batch  384/394   train_loss = 3.226\n",
            "Epoch  15 Batch   40/394   train_loss = 3.337\n",
            "Epoch  15 Batch   90/394   train_loss = 3.354\n",
            "Epoch  15 Batch  140/394   train_loss = 3.207\n",
            "Epoch  15 Batch  190/394   train_loss = 3.298\n",
            "Epoch  15 Batch  240/394   train_loss = 3.084\n",
            "Epoch  15 Batch  290/394   train_loss = 3.073\n",
            "Epoch  15 Batch  340/394   train_loss = 3.103\n",
            "Epoch  15 Batch  390/394   train_loss = 2.993\n",
            "Epoch  16 Batch   46/394   train_loss = 3.046\n",
            "Epoch  16 Batch   96/394   train_loss = 2.960\n",
            "Epoch  16 Batch  146/394   train_loss = 3.035\n",
            "Epoch  16 Batch  196/394   train_loss = 2.908\n",
            "Epoch  16 Batch  246/394   train_loss = 2.995\n",
            "Epoch  16 Batch  296/394   train_loss = 2.860\n",
            "Epoch  16 Batch  346/394   train_loss = 2.998\n",
            "Epoch  17 Batch    2/394   train_loss = 2.883\n",
            "Epoch  17 Batch   52/394   train_loss = 2.890\n",
            "Epoch  17 Batch  102/394   train_loss = 2.950\n",
            "Epoch  17 Batch  152/394   train_loss = 2.800\n",
            "Epoch  17 Batch  202/394   train_loss = 2.701\n",
            "Epoch  17 Batch  252/394   train_loss = 2.891\n",
            "Epoch  17 Batch  302/394   train_loss = 2.654\n",
            "Epoch  17 Batch  352/394   train_loss = 2.672\n",
            "Epoch  18 Batch    8/394   train_loss = 2.895\n",
            "Epoch  18 Batch   58/394   train_loss = 2.754\n",
            "Epoch  18 Batch  108/394   train_loss = 2.534\n",
            "Epoch  18 Batch  158/394   train_loss = 2.588\n",
            "Epoch  18 Batch  208/394   train_loss = 2.595\n",
            "Epoch  18 Batch  258/394   train_loss = 2.547\n",
            "Epoch  18 Batch  308/394   train_loss = 2.526\n",
            "Epoch  18 Batch  358/394   train_loss = 2.651\n",
            "Epoch  19 Batch   14/394   train_loss = 2.616\n",
            "Epoch  19 Batch   64/394   train_loss = 2.476\n",
            "Epoch  19 Batch  114/394   train_loss = 2.587\n",
            "Epoch  19 Batch  164/394   train_loss = 2.587\n",
            "Epoch  19 Batch  214/394   train_loss = 2.428\n",
            "Epoch  19 Batch  264/394   train_loss = 2.506\n",
            "Epoch  19 Batch  314/394   train_loss = 2.465\n",
            "Epoch  19 Batch  364/394   train_loss = 2.392\n",
            "Epoch  20 Batch   20/394   train_loss = 2.599\n",
            "Epoch  20 Batch   70/394   train_loss = 2.381\n",
            "Epoch  20 Batch  120/394   train_loss = 2.380\n",
            "Epoch  20 Batch  170/394   train_loss = 2.347\n",
            "Epoch  20 Batch  220/394   train_loss = 2.267\n",
            "Epoch  20 Batch  270/394   train_loss = 2.328\n",
            "Epoch  20 Batch  320/394   train_loss = 2.258\n",
            "Epoch  20 Batch  370/394   train_loss = 2.372\n",
            "Epoch  21 Batch   26/394   train_loss = 2.383\n",
            "Epoch  21 Batch   76/394   train_loss = 2.129\n",
            "Epoch  21 Batch  126/394   train_loss = 2.305\n",
            "Epoch  21 Batch  176/394   train_loss = 2.331\n",
            "Epoch  21 Batch  226/394   train_loss = 2.210\n",
            "Epoch  21 Batch  276/394   train_loss = 2.192\n",
            "Epoch  21 Batch  326/394   train_loss = 2.175\n",
            "Epoch  21 Batch  376/394   train_loss = 2.192\n",
            "Epoch  22 Batch   32/394   train_loss = 2.106\n",
            "Epoch  22 Batch   82/394   train_loss = 2.099\n",
            "Epoch  22 Batch  132/394   train_loss = 2.191\n",
            "Epoch  22 Batch  182/394   train_loss = 2.074\n",
            "Epoch  22 Batch  232/394   train_loss = 2.012\n",
            "Epoch  22 Batch  282/394   train_loss = 1.977\n",
            "Epoch  22 Batch  332/394   train_loss = 1.947\n",
            "Epoch  22 Batch  382/394   train_loss = 2.022\n",
            "Epoch  23 Batch   38/394   train_loss = 2.063\n",
            "Epoch  23 Batch   88/394   train_loss = 2.006\n",
            "Epoch  23 Batch  138/394   train_loss = 1.996\n",
            "Epoch  23 Batch  188/394   train_loss = 2.090\n",
            "Epoch  23 Batch  238/394   train_loss = 1.853\n",
            "Epoch  23 Batch  288/394   train_loss = 1.955\n",
            "Epoch  23 Batch  338/394   train_loss = 1.823\n",
            "Epoch  23 Batch  388/394   train_loss = 1.865\n",
            "Epoch  24 Batch   44/394   train_loss = 2.030\n",
            "Epoch  24 Batch   94/394   train_loss = 1.843\n",
            "Epoch  24 Batch  144/394   train_loss = 2.019\n",
            "Epoch  24 Batch  194/394   train_loss = 1.881\n",
            "Epoch  24 Batch  244/394   train_loss = 1.905\n",
            "Epoch  24 Batch  294/394   train_loss = 1.812\n",
            "Epoch  24 Batch  344/394   train_loss = 1.800\n",
            "Epoch  25 Batch    0/394   train_loss = 1.702\n",
            "Epoch  25 Batch   50/394   train_loss = 1.863\n",
            "Epoch  25 Batch  100/394   train_loss = 1.936\n",
            "Epoch  25 Batch  150/394   train_loss = 1.724\n",
            "Epoch  25 Batch  200/394   train_loss = 1.821\n",
            "Epoch  25 Batch  250/394   train_loss = 1.633\n",
            "Epoch  25 Batch  300/394   train_loss = 1.701\n",
            "Epoch  25 Batch  350/394   train_loss = 1.719\n",
            "Epoch  26 Batch    6/394   train_loss = 1.796\n",
            "Epoch  26 Batch   56/394   train_loss = 1.692\n",
            "Epoch  26 Batch  106/394   train_loss = 1.666\n",
            "Epoch  26 Batch  156/394   train_loss = 1.707\n",
            "Epoch  26 Batch  206/394   train_loss = 1.665\n",
            "Epoch  26 Batch  256/394   train_loss = 1.678\n",
            "Epoch  26 Batch  306/394   train_loss = 1.602\n",
            "Epoch  26 Batch  356/394   train_loss = 1.623\n",
            "Epoch  27 Batch   12/394   train_loss = 1.655\n",
            "Epoch  27 Batch   62/394   train_loss = 1.513\n",
            "Epoch  27 Batch  112/394   train_loss = 1.639\n",
            "Epoch  27 Batch  162/394   train_loss = 1.634\n",
            "Epoch  27 Batch  212/394   train_loss = 1.563\n",
            "Epoch  27 Batch  262/394   train_loss = 1.568\n",
            "Epoch  27 Batch  312/394   train_loss = 1.431\n",
            "Epoch  27 Batch  362/394   train_loss = 1.587\n",
            "Epoch  28 Batch   18/394   train_loss = 1.543\n",
            "Epoch  28 Batch   68/394   train_loss = 1.484\n",
            "Epoch  28 Batch  118/394   train_loss = 1.553\n",
            "Epoch  28 Batch  168/394   train_loss = 1.401\n",
            "Epoch  28 Batch  218/394   train_loss = 1.445\n",
            "Epoch  28 Batch  268/394   train_loss = 1.501\n",
            "Epoch  28 Batch  318/394   train_loss = 1.439\n",
            "Epoch  28 Batch  368/394   train_loss = 1.350\n",
            "Epoch  29 Batch   24/394   train_loss = 1.468\n",
            "Epoch  29 Batch   74/394   train_loss = 1.406\n",
            "Epoch  29 Batch  124/394   train_loss = 1.385\n",
            "Epoch  29 Batch  174/394   train_loss = 1.416\n",
            "Epoch  29 Batch  224/394   train_loss = 1.465\n",
            "Epoch  29 Batch  274/394   train_loss = 1.308\n",
            "Epoch  29 Batch  324/394   train_loss = 1.343\n",
            "Epoch  29 Batch  374/394   train_loss = 1.429\n",
            "Epoch  30 Batch   30/394   train_loss = 1.365\n",
            "Epoch  30 Batch   80/394   train_loss = 1.419\n",
            "Epoch  30 Batch  130/394   train_loss = 1.312\n",
            "Epoch  30 Batch  180/394   train_loss = 1.288\n",
            "Epoch  30 Batch  230/394   train_loss = 1.363\n",
            "Epoch  30 Batch  280/394   train_loss = 1.342\n",
            "Epoch  30 Batch  330/394   train_loss = 1.280\n",
            "Epoch  30 Batch  380/394   train_loss = 1.258\n",
            "Epoch  31 Batch   36/394   train_loss = 1.292\n",
            "Epoch  31 Batch   86/394   train_loss = 1.284\n",
            "Epoch  31 Batch  136/394   train_loss = 1.279\n",
            "Epoch  31 Batch  186/394   train_loss = 1.189\n",
            "Epoch  31 Batch  236/394   train_loss = 1.234\n",
            "Epoch  31 Batch  286/394   train_loss = 1.257\n",
            "Epoch  31 Batch  336/394   train_loss = 1.209\n",
            "Epoch  31 Batch  386/394   train_loss = 1.246\n",
            "Epoch  32 Batch   42/394   train_loss = 1.179\n",
            "Epoch  32 Batch   92/394   train_loss = 1.153\n",
            "Epoch  32 Batch  142/394   train_loss = 1.287\n",
            "Epoch  32 Batch  192/394   train_loss = 1.177\n",
            "Epoch  32 Batch  242/394   train_loss = 1.180\n",
            "Epoch  32 Batch  292/394   train_loss = 1.186\n",
            "Epoch  32 Batch  342/394   train_loss = 1.181\n",
            "Epoch  32 Batch  392/394   train_loss = 1.143\n",
            "Epoch  33 Batch   48/394   train_loss = 1.150\n",
            "Epoch  33 Batch   98/394   train_loss = 1.137\n",
            "Epoch  33 Batch  148/394   train_loss = 1.099\n",
            "Epoch  33 Batch  198/394   train_loss = 1.209\n",
            "Epoch  33 Batch  248/394   train_loss = 1.119\n",
            "Epoch  33 Batch  298/394   train_loss = 1.150\n",
            "Epoch  33 Batch  348/394   train_loss = 1.037\n",
            "Epoch  34 Batch    4/394   train_loss = 1.166\n",
            "Epoch  34 Batch   54/394   train_loss = 1.177\n",
            "Epoch  34 Batch  104/394   train_loss = 1.089\n",
            "Epoch  34 Batch  154/394   train_loss = 1.020\n",
            "Epoch  34 Batch  204/394   train_loss = 1.168\n",
            "Epoch  34 Batch  254/394   train_loss = 1.097\n",
            "Epoch  34 Batch  304/394   train_loss = 1.011\n",
            "Epoch  34 Batch  354/394   train_loss = 0.971\n",
            "Epoch  35 Batch   10/394   train_loss = 1.075\n",
            "Epoch  35 Batch   60/394   train_loss = 1.085\n",
            "Epoch  35 Batch  110/394   train_loss = 0.947\n",
            "Epoch  35 Batch  160/394   train_loss = 1.035\n",
            "Epoch  35 Batch  210/394   train_loss = 1.052\n",
            "Epoch  35 Batch  260/394   train_loss = 1.014\n",
            "Epoch  35 Batch  310/394   train_loss = 1.072\n",
            "Epoch  35 Batch  360/394   train_loss = 1.039\n",
            "Epoch  36 Batch   16/394   train_loss = 0.914\n",
            "Epoch  36 Batch   66/394   train_loss = 0.885\n",
            "Epoch  36 Batch  116/394   train_loss = 1.001\n",
            "Epoch  36 Batch  166/394   train_loss = 1.016\n",
            "Epoch  36 Batch  216/394   train_loss = 1.055\n",
            "Epoch  36 Batch  266/394   train_loss = 0.888\n",
            "Epoch  36 Batch  316/394   train_loss = 0.988\n",
            "Epoch  36 Batch  366/394   train_loss = 0.938\n",
            "Epoch  37 Batch   22/394   train_loss = 0.900\n",
            "Epoch  37 Batch   72/394   train_loss = 0.905\n",
            "Epoch  37 Batch  122/394   train_loss = 0.882\n",
            "Epoch  37 Batch  172/394   train_loss = 0.898\n",
            "Epoch  37 Batch  222/394   train_loss = 0.915\n",
            "Epoch  37 Batch  272/394   train_loss = 0.939\n",
            "Epoch  37 Batch  322/394   train_loss = 0.934\n",
            "Epoch  37 Batch  372/394   train_loss = 0.805\n",
            "Epoch  38 Batch   28/394   train_loss = 0.814\n",
            "Epoch  38 Batch   78/394   train_loss = 0.798\n",
            "Epoch  38 Batch  128/394   train_loss = 0.867\n",
            "Epoch  38 Batch  178/394   train_loss = 0.915\n",
            "Epoch  38 Batch  228/394   train_loss = 0.906\n",
            "Epoch  38 Batch  278/394   train_loss = 0.982\n",
            "Epoch  38 Batch  328/394   train_loss = 0.808\n",
            "Epoch  38 Batch  378/394   train_loss = 0.778\n",
            "Epoch  39 Batch   34/394   train_loss = 0.857\n",
            "Epoch  39 Batch   84/394   train_loss = 0.818\n",
            "Epoch  39 Batch  134/394   train_loss = 0.813\n",
            "Epoch  39 Batch  184/394   train_loss = 0.832\n",
            "Epoch  39 Batch  234/394   train_loss = 0.841\n",
            "Epoch  39 Batch  284/394   train_loss = 0.783\n",
            "Epoch  39 Batch  334/394   train_loss = 0.819\n",
            "Epoch  39 Batch  384/394   train_loss = 0.764\n",
            "Epoch  40 Batch   40/394   train_loss = 0.853\n",
            "Epoch  40 Batch   90/394   train_loss = 0.849\n",
            "Epoch  40 Batch  140/394   train_loss = 0.775\n",
            "Epoch  40 Batch  190/394   train_loss = 0.870\n",
            "Epoch  40 Batch  240/394   train_loss = 0.818\n",
            "Epoch  40 Batch  290/394   train_loss = 0.711\n",
            "Epoch  40 Batch  340/394   train_loss = 0.754\n",
            "Epoch  40 Batch  390/394   train_loss = 0.734\n",
            "Epoch  41 Batch   46/394   train_loss = 0.739\n",
            "Epoch  41 Batch   96/394   train_loss = 0.696\n",
            "Epoch  41 Batch  146/394   train_loss = 0.785\n",
            "Epoch  41 Batch  196/394   train_loss = 0.783\n",
            "Epoch  41 Batch  246/394   train_loss = 0.702\n",
            "Epoch  41 Batch  296/394   train_loss = 0.734\n",
            "Epoch  41 Batch  346/394   train_loss = 0.767\n",
            "Epoch  42 Batch    2/394   train_loss = 0.718\n",
            "Epoch  42 Batch   52/394   train_loss = 0.758\n",
            "Epoch  42 Batch  102/394   train_loss = 0.676\n",
            "Epoch  42 Batch  152/394   train_loss = 0.710\n",
            "Epoch  42 Batch  202/394   train_loss = 0.666\n",
            "Epoch  42 Batch  252/394   train_loss = 0.754\n",
            "Epoch  42 Batch  302/394   train_loss = 0.600\n",
            "Epoch  42 Batch  352/394   train_loss = 0.689\n",
            "Epoch  43 Batch    8/394   train_loss = 0.747\n",
            "Epoch  43 Batch   58/394   train_loss = 0.682\n",
            "Epoch  43 Batch  108/394   train_loss = 0.623\n",
            "Epoch  43 Batch  158/394   train_loss = 0.655\n",
            "Epoch  43 Batch  208/394   train_loss = 0.675\n",
            "Epoch  43 Batch  258/394   train_loss = 0.661\n",
            "Epoch  43 Batch  308/394   train_loss = 0.633\n",
            "Epoch  43 Batch  358/394   train_loss = 0.724\n",
            "Epoch  44 Batch   14/394   train_loss = 0.672\n",
            "Epoch  44 Batch   64/394   train_loss = 0.586\n",
            "Epoch  44 Batch  114/394   train_loss = 0.662\n",
            "Epoch  44 Batch  164/394   train_loss = 0.661\n",
            "Epoch  44 Batch  214/394   train_loss = 0.626\n",
            "Epoch  44 Batch  264/394   train_loss = 0.638\n",
            "Epoch  44 Batch  314/394   train_loss = 0.654\n",
            "Epoch  44 Batch  364/394   train_loss = 0.648\n",
            "Epoch  45 Batch   20/394   train_loss = 0.688\n",
            "Epoch  45 Batch   70/394   train_loss = 0.575\n",
            "Epoch  45 Batch  120/394   train_loss = 0.582\n",
            "Epoch  45 Batch  170/394   train_loss = 0.607\n",
            "Epoch  45 Batch  220/394   train_loss = 0.587\n",
            "Epoch  45 Batch  270/394   train_loss = 0.624\n",
            "Epoch  45 Batch  320/394   train_loss = 0.603\n",
            "Epoch  45 Batch  370/394   train_loss = 0.678\n",
            "Epoch  46 Batch   26/394   train_loss = 0.626\n",
            "Epoch  46 Batch   76/394   train_loss = 0.575\n",
            "Epoch  46 Batch  126/394   train_loss = 0.571\n",
            "Epoch  46 Batch  176/394   train_loss = 0.641\n",
            "Epoch  46 Batch  226/394   train_loss = 0.570\n",
            "Epoch  46 Batch  276/394   train_loss = 0.587\n",
            "Epoch  46 Batch  326/394   train_loss = 0.612\n",
            "Epoch  46 Batch  376/394   train_loss = 0.629\n",
            "Epoch  47 Batch   32/394   train_loss = 0.599\n",
            "Epoch  47 Batch   82/394   train_loss = 0.591\n",
            "Epoch  47 Batch  132/394   train_loss = 0.616\n",
            "Epoch  47 Batch  182/394   train_loss = 0.523\n",
            "Epoch  47 Batch  232/394   train_loss = 0.586\n",
            "Epoch  47 Batch  282/394   train_loss = 0.558\n",
            "Epoch  47 Batch  332/394   train_loss = 0.534\n",
            "Epoch  47 Batch  382/394   train_loss = 0.601\n",
            "Epoch  48 Batch   38/394   train_loss = 0.549\n",
            "Epoch  48 Batch   88/394   train_loss = 0.563\n",
            "Epoch  48 Batch  138/394   train_loss = 0.516\n",
            "Epoch  48 Batch  188/394   train_loss = 0.575\n",
            "Epoch  48 Batch  238/394   train_loss = 0.510\n",
            "Epoch  48 Batch  288/394   train_loss = 0.561\n",
            "Epoch  48 Batch  338/394   train_loss = 0.489\n",
            "Epoch  48 Batch  388/394   train_loss = 0.548\n",
            "Epoch  49 Batch   44/394   train_loss = 0.560\n",
            "Epoch  49 Batch   94/394   train_loss = 0.542\n",
            "Epoch  49 Batch  144/394   train_loss = 0.539\n",
            "Epoch  49 Batch  194/394   train_loss = 0.518\n",
            "Epoch  49 Batch  244/394   train_loss = 0.547\n",
            "Epoch  49 Batch  294/394   train_loss = 0.553\n",
            "Epoch  49 Batch  344/394   train_loss = 0.520\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4dd1Dynwoiq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b3a7f3-da3f-47ff-efa1-d6ddd7bc7f51"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#import helper\n",
        "\n",
        "def pick_word(probabilities, int_to_vocab):\n",
        "    \"\"\"\n",
        "    Pick the next word in the generated text\n",
        "    :param probabilities: Probabilites of the next word\n",
        "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
        "    :return: String of the predicted word\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    index = np.where(probabilities == np.max(probabilities))[-1][0]\n",
        "\n",
        "    return int_to_vocab[index]\n",
        "\n",
        "#_,_,_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "seq_length, load_dir = 15, '/content/drive/MyDrive/nlp_project/models/model_1/save'\n",
        "def get_tensors(loaded_graph):\n",
        "    input_tensor = loaded_graph.get_tensor_by_name(\"input:0\")\n",
        "    initial_state_tensor = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
        "    final_state_tensor = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
        "    prob_tensor = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
        "\n",
        "    return input_tensor, initial_state_tensor, final_state_tensor, prob_tensor\n",
        "    \n",
        "#test_text = [int_to_vocab[word] for word in test_int_text]\n",
        "#test_text = \" \".join(test_text)\n",
        "#test_sent_text = test_text.split(\"<period>\")\n",
        "\n",
        "'''\n",
        "test_sentences = input(\"Enter sentence:\")\n",
        "token_dict = token_lookup()\n",
        "for key, token in token_dict.items():\n",
        "\ttest_sentences = test_sentences.replace(key, ' {} '.format(token))\n",
        "test_sentences = test_sentences.lower()\n",
        "test_sentences = test_sentences.split(\"<period>\")\n",
        "'''\n",
        "#all_perp = []\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.compat.v1.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.compat.v1.train.import_meta_graph(load_dir + '.meta')\n",
        "    loader.restore(sess, load_dir)\n",
        "\n",
        "    # Get Tensors from loaded model\n",
        "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
        "    #print(final_state)\n",
        "    # Sentences generation setup\n",
        "    gen_length = 200\n",
        "    prime_word = 'यही' #'moe_szyslak'\n",
        "    gen_sentences = [prime_word]\n",
        "            \n",
        "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
        "\n",
        "    # Generate sentences\n",
        "    for n in range(gen_length):\n",
        "        # Dynamic Input\n",
        "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
        "        dyn_seq_length = len(dyn_input[0])\n",
        "          \n",
        "        # Get Prediction\n",
        "        probabilities, prev_state = sess.run(\n",
        "            [probs, final_state],\n",
        "            {input_text: dyn_input, initial_state: prev_state})\n",
        "\n",
        "        #pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
        "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
        "\n",
        "        gen_sentences.append(pred_word)\n",
        "\n",
        "    # Remove tokens\n",
        "    tv_script = ' '.join(gen_sentences)\n",
        "    for key, token in token_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
        "    tv_script = tv_script.replace('\\n ', '\\n')\n",
        "    tv_script = tv_script.replace('( ', '(')\n",
        "        \n",
        "    print(tv_script)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/nlp_project/models/model_1/save\n",
            "यही एकमात्र ऐसा the only way है कि भारत का the sowing year अक्‍तूबर में a year तैयार होने के कारण 20 kilogram nitrogen fertilizer use शक्‍ति का developed use किया जाना परम आवश्यक है\n",
            "इस प्रकार of कृषि in विश्व is भारत has been उत्पादन and गुण is enough after giving खेत\n",
            "but this dream is known as an impromptu- to- its no way\n",
            "further, the green varieties which is grown from many methods which were उपयोग by which वे could not हुआ हुए\n",
            "लेवेन्डर को ही शामिल करना उचित be\n",
            "भारत has reduced with small organic drugs and agriculture\n",
            "apart from सरकारी समिति got प्रोत्साहन on आधार of बैंकिंग सुविधाओं s\n",
            "किसानों had also ended\n",
            "कृषि मंत्री ने बताया कि bonsai farmers the farmer एवं दशक में गठन a long way the size या सही जरूरत की संभावना है\n",
            "हालांकि एक विशेषज्ञों ने the indian government' s माल के अनुसार the country organic agriculture the significance कम करता है\n",
            "इस विधि में more output capacity प्राप्‍त होता है\n",
            "अत: जब कभी बहुत ही अलग- झौईण् अलग दिखाई पड़ते हैं\n",
            "this method\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPo1YhqqI9Vh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}