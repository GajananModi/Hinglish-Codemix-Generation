{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuralLM_POStag.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDVhxezHTskI"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#en_hi_tweets = open(\"/content/drive/MyDrive/nlp_project/datasets/POStag_dataset/POS_Hindi_English_Code_Mixed_Tweets.tsv\", \"r\").read()\n",
        "#data2 = open(\"/content/drive/MyDrive/nlp_project/datasets/POStag_dataset/data2.txt\", \"r\").read()\n",
        "#whatsapp = open(\"/content/drive/MyDrive/nlp_project/datasets/POStag_dataset/WaData.txt\", \"r\").read()\n",
        "pk = open(\"/content/drive/MyDrive/nlp_project/datasets/POStag_dataset/pk_modi.txt\", \"r\").read()\n",
        "en_tags = open(\"/content/drive/MyDrive/nlp_project/datasets/POStag_dataset/en_tags.txt\", \"r\").read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DUSr4M07ak4j",
        "outputId": "c0ca2e34-fdf8-4af0-8912-bc39136d4c2a"
      },
      "source": [
        "def token_lookup():\n",
        "    token_dict = {\".\":\"<PERIOD>\", \",\": \"<COMMA>\", \";\":\"<SEMICOLON>\", \":\":\"<COLON>\", \"\\\"\":\"<QOUTATION>\", \"'\":\"<QOUTATION>\", \"!\":\"<EXCLAMATION>\", \"?\":\"<QUESTION>\", \n",
        "                  \"(\": \"<LEFTPARAN>\", \")\": \"<RIGHTPARAN>\", \"{\":\"LEFTBRACE\", \"}\":\"<RIGHTBRACE>\", \"[\":\"<LEFTBRACKET>\", \"]\":\"<RIGHTBRACKET>\", \"--\":\"<DASH>\", \"-\":\"<HYPHEN>\",\n",
        "                  #\"\\n\":\"<RETURN>\"\n",
        "                  }\n",
        "    return token_dict\n",
        "def create_sentences(lines):\n",
        "    sentences = []\n",
        "    for line in lines:\n",
        "        rows = line.split('\\n')\n",
        "        sent = ['<BOS>']\n",
        "        for x in rows:\n",
        "            x = x.split('\\t')\n",
        "            if len(x)>2 and x[0].isalpha():\n",
        "                sent.append(x[0]+\"_\"+x[2])\n",
        "            #sent.append(\"_\".join(x.split('\\t')))\n",
        "        sent.append(\"<EOS>\")\n",
        "        sent = ' '.join(sent)\n",
        "        sentences.append(sent)\n",
        "    return sentences\n",
        "token_dict = token_lookup()\n",
        "\n",
        "for key, token in token_dict.items():\n",
        "    pk = pk.replace(key, '{}'.format(token))\n",
        "    en_tags = en_tags.replace(key, '{}'.format(token))\n",
        "\n",
        "\n",
        "#lines = en_hi_tweets.split(\"\\n\\t\\t\\n\")\n",
        "#lines = data2.split(\"\\n\\n\")\n",
        "#lines = whatsapp.split(\"\\n\\n\")\n",
        "lines_pk = create_sentences(pk.split(\"\\n\\n\"))\n",
        "lines_en_tags = create_sentences(en_tags.split(\"\\n\\n\"))\n",
        "lines_pk = pd.DataFrame(lines_pk, columns=[\"sentences\"])\n",
        "lines_en_tags = pd.DataFrame(lines_en_tags, columns=[\"sentences\"])\n",
        "\n",
        "en_mix = lines_pk #pd.concat([lines_en_tags, lines_pk], axis=0)\n",
        "#en_mix = en_mix.sample(frac = 1)\n",
        "en_mix.head()\n",
        "#print(len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;BOS&gt; bharat_G_N me_G_PRP ajaad_G_V he_G_V lek...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;BOS&gt; digital_G_J indiaa_G_N &lt;EOS&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;BOS&gt; enn_G_J sab_G_N ko_PSP ban_G_N &lt;EOS&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;BOS&gt; karna_G_N cha_G_V he_G_V ye_G_PRP jab_G_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;BOS&gt; part_G_N time_G_N jop_G_N dene_G_N ka_PS...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           sentences\n",
              "0  <BOS> bharat_G_N me_G_PRP ajaad_G_V he_G_V lek...\n",
              "1                 <BOS> digital_G_J indiaa_G_N <EOS>\n",
              "2         <BOS> enn_G_J sab_G_N ko_PSP ban_G_N <EOS>\n",
              "3  <BOS> karna_G_N cha_G_V he_G_V ye_G_PRP jab_G_...\n",
              "4  <BOS> part_G_N time_G_N jop_G_N dene_G_N ka_PS..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGtj0WuIfAjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144135d2-c4a3-45a1-982b-f736e0ddad23"
      },
      "source": [
        "def create_lookup_tables(text_df):\n",
        "    vocab = set()\n",
        "    for line in text_df['sentences']:\n",
        "        vocab.update(set(line.split()))\n",
        "    #vocab = set(text)\n",
        "    vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
        "    int_to_vocab = dict(enumerate(vocab))\n",
        "    return (vocab_to_int, int_to_vocab)\n",
        "\n",
        "vocab_to_int, int_to_vocab = create_lookup_tables(en_mix)\n",
        "\n",
        "train_int_text = []\n",
        "\n",
        "for line in en_mix['sentences']:\n",
        "    train_int_text.append([vocab_to_int[word] for word in line.split()])\n",
        "print(len(vocab_to_int))\n",
        "train_int_text = [item for sublist in train_int_text for item in sublist]\n",
        "print(train_int_text[:1000])  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24636\n",
            "[10326, 12074, 21541, 7202, 12738, 20950, 14760, 6496, 19923, 21629, 24276, 17817, 3346, 22390, 24082, 10002, 24609, 17541, 19141, 6248, 17920, 12074, 4958, 16714, 22760, 1726, 15275, 11896, 12738, 12824, 1118, 17541, 12738, 7064, 18756, 445, 19765, 11424, 4958, 122, 12034, 4958, 11385, 903, 12232, 12139, 18200, 6734, 17211, 12232, 7181, 14804, 12726, 3126, 21092, 11744, 17862, 21524, 2844, 15894, 12232, 11899, 10326, 7938, 20521, 11899, 10326, 6176, 6570, 4958, 1109, 11899, 10326, 18232, 15002, 12738, 10346, 3346, 12805, 23860, 10874, 2066, 16714, 11452, 6588, 3975, 7756, 12838, 23283, 21524, 6338, 2926, 11972, 5811, 3322, 665, 18401, 21524, 21348, 5810, 1789, 15352, 19231, 24143, 5686, 18631, 445, 4958, 8964, 6261, 3263, 2126, 16342, 24059, 5920, 21004, 20579, 17936, 597, 5608, 16348, 3975, 8467, 13947, 24059, 19879, 5608, 17569, 9493, 21375, 1789, 17920, 14529, 20809, 1015, 13865, 17818, 20496, 11748, 3975, 4685, 14484, 10507, 6338, 5753, 17329, 4958, 16714, 11452, 20874, 17076, 6338, 21295, 4958, 16714, 10500, 17208, 10458, 7865, 3975, 19879, 17672, 20034, 12232, 16338, 9536, 12591, 12649, 2809, 4958, 12078, 2923, 3621, 11286, 19477, 13735, 12232, 16297, 5753, 6132, 21776, 14079, 11435, 20570, 1789, 7566, 13422, 8082, 18773, 11563, 13422, 15552, 9215, 5574, 1694, 12232, 2798, 16165, 5753, 11924, 7566, 13422, 10017, 10657, 13422, 14529, 11683, 11899, 10326, 18582, 14529, 13481, 16507, 17588, 19306, 20194, 10838, 3322, 3526, 3322, 5583, 18701, 19765, 11385, 21004, 12232, 6617, 11899, 10326, 17232, 17920, 11385, 21004, 12232, 22836, 11899, 10326, 20846, 15721, 21541, 16113, 17485, 8069, 16714, 4164, 11899, 10326, 22843, 10346, 13099, 21295, 21524, 183, 10576, 23412, 3975, 20354, 261, 12317, 13279, 21166, 4388, 7767, 10346, 5753, 13422, 4472, 10523, 183, 15615, 18774, 12056, 16297, 903, 12232, 12038, 14877, 10576, 5753, 13422, 426, 18134, 1726, 6854, 20758, 12232, 5753, 5753, 14621, 17258, 5917, 16828, 14409, 122, 7386, 5660, 2217, 19560, 12128, 19672, 21348, 7938, 8464, 21541, 2321, 16191, 8531, 6496, 15838, 16538, 6588, 11899, 10326, 20450, 11899, 10326, 8553, 7453, 2234, 21541, 695, 24292, 11899, 10326, 13122, 16264, 2321, 8333, 21251, 10221, 16828, 8333, 21251, 15560, 2321, 15091, 7444, 11899, 10326, 6228, 300, 19231, 17801, 22838, 18229, 7444, 11899, 10326, 2217, 19560, 21524, 24086, 19672, 21348, 7938, 8464, 21541, 16291, 13422, 7876, 9218, 6588, 11899, 10326, 9670, 8923, 24291, 5597, 1536, 4685, 12407, 18234, 21109, 11899, 10326, 17676, 21524, 13626, 6425, 3663, 13422, 15751, 9920, 17590, 20148, 7004, 19029, 15945, 5803, 17590, 11899, 10326, 6065, 13422, 18221, 16338, 12232, 11899, 10326, 8722, 12027, 12232, 23424, 4958, 3322, 360, 8320, 2066, 22850, 2552, 6496, 9617, 22940, 12232, 11899, 10326, 10458, 20949, 20579, 16354, 11899, 10326, 23411, 15547, 122, 1084, 20579, 20269, 10874, 18394, 20919, 703, 11235, 3526, 20579, 20372, 15547, 22780, 12800, 967, 24063, 10490, 19879, 7137, 23282, 8536, 5354, 15547, 13252, 11899, 10326, 16426, 20579, 19557, 23972, 15694, 19067, 24188, 7514, 11899, 10326, 2936, 14516, 20411, 13422, 14372, 6093, 20874, 12579, 23972, 14804, 8572, 14079, 533, 22622, 4958, 5497, 18232, 18823, 17258, 10346, 13422, 873, 3263, 3832, 6855, 11007, 10346, 19879, 268, 11218, 7834, 24129, 6228, 11785, 19072, 16965, 3178, 4318, 20088, 8779, 9417, 12178, 15841, 23860, 3825, 17518, 6496, 20422, 3468, 17590, 15943, 7374, 24063, 24267, 16605, 13735, 17590, 3322, 6268, 19205, 19301, 19990, 20437, 17696, 20437, 11675, 20437, 8074, 6425, 7084, 1687, 2241, 14486, 12232, 1895, 8636, 1726, 9658, 426, 15732, 4385, 19231, 11427, 463, 7303, 19593, 4313, 13247, 17208, 3346, 18814, 17696, 21262, 19990, 19301, 15423, 6496, 2923, 16714, 19829, 1726, 19231, 24152, 12232, 2321, 13668, 21541, 20731, 2818, 17258, 11899, 10326, 10900, 4019, 19206, 3526, 4958, 18939, 16605, 8485, 18564, 21541, 10346, 19514, 10067, 542, 21602, 8480, 16205, 4472, 17258, 18074, 873, 24129, 19339, 16544, 17396, 268, 22303, 23612, 21541, 13725, 268, 11717, 24063, 24063, 5876, 13422, 24222, 21836, 2780, 6588, 7066, 10670, 13819, 23972, 14804, 6496, 10346, 17335, 5686, 16714, 10670, 1726, 21541, 16969, 21004, 20839, 20579, 7566, 18366, 10523, 5341, 5354, 15547, 16084, 17258, 76, 11974, 6496, 15547, 16238, 12232, 19416, 21524, 7180, 21524, 23082, 11399, 22125, 554, 23721, 4958, 3975, 11899, 10326, 15296, 6496, 10807, 17499, 20579, 9052, 19879, 24342, 2217, 3975, 11392, 16714, 7719, 3322, 12232, 12294, 20579, 11890, 3558, 21524, 9115, 17436, 8933, 22954, 18976, 17565, 21157, 8133, 13513, 18073, 4313, 21119, 2569, 24618, 2122, 23972, 19392, 16714, 5701, 12232, 4398, 21856, 6428, 2669, 24301, 14051, 2217, 10044, 6338, 1901, 19879, 21581, 19560, 21376, 3322, 22815, 21502, 22326, 8464, 13422, 14728, 22068, 13422, 3402, 14713, 12232, 20846, 1375, 14804, 11121, 10790, 12243, 6884, 6496, 3975, 8464, 14728, 16828, 8616, 17638, 21541, 17862, 14016, 12232, 11899, 10326, 8395, 9764, 20579, 6132, 10587, 9375, 3587, 22661, 122, 15797, 360, 2321, 2289, 20961, 11751, 6561, 18574, 10216, 7094, 21602, 9051, 2217, 967, 11632, 15277, 15258, 22749, 6862, 21021, 9849, 8673, 20192, 6674, 6311, 9115, 23822, 23860, 8274, 1233, 21524, 12738, 10346, 261, 23860, 13811, 13422, 21348, 7767, 22672, 4958, 19965, 14790, 416, 17832, 4962, 21541, 14014, 20846, 22723, 10052, 6322, 3135, 4736, 3699, 20572, 5574, 1117, 17817, 16213, 3575, 8943, 6570, 4958, 7566, 6496, 878, 22676, 19879, 21579, 20839, 3322, 18545, 6469, 426, 6338, 8300, 4166, 10523, 24002, 7241, 5712, 24609, 6338, 12689, 10116, 6338, 19097, 18774, 3322, 1302, 16213, 16454, 23386, 13132, 23537, 24002, 7241, 5712, 4736, 11899, 10326, 7865, 1944, 12232, 22843, 10135, 20229, 122, 20874, 3975, 11385, 5004, 12595, 18146, 23537, 4958, 21583, 11646, 21541, 8834, 11899, 10326, 445, 21541, 6338, 8964, 24063, 7767, 17920, 10135, 22712, 20874, 360, 6098, 22712, 10523, 10002, 1329, 5004, 20874, 24552, 3073, 23297, 9658, 12827, 12232, 6588, 13694, 18453, 20579, 12822, 10669, 7579, 20962, 15810, 12595, 20874, 7566, 9920, 20989, 3272, 17920, 16205, 10135, 22712, 20874, 360, 6098, 22712, 10523, 10002, 22653, 9620, 20874, 13058, 23972, 11243, 3337, 8930, 15540, 7103, 4703, 11385, 17243, 21524, 11646, 13147, 4437, 5046, 14804, 23412, 14415, 8623, 8464, 22222, 1239, 4958, 15685, 7008, 122, 22850, 20936, 2757, 16714, 3816, 12232, 54, 445, 21524, 12827, 11899, 10326, 10670, 1726, 4958, 665, 21524, 24366, 2840, 7386, 12232, 11899, 10326, 5194, 3440, 21295, 13209, 13422, 13234, 805, 17817, 20846, 8175, 14420, 14079, 6132, 11899, 10326, 20269, 54, 18146, 13422, 11360, 12232, 10670, 21504, 15721]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-15jxZPnmwV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "891e78bd-555f-4656-8716-ea29c2860ec5"
      },
      "source": [
        "#import helper\n",
        "\n",
        "#data_dir = './brown.txt'\n",
        "\n",
        "#helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)\n",
        "\n",
        "from distutils.version import LooseVersion\n",
        "import warnings\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "import tensorflow as tf2\n",
        "#import helper\n",
        "import numpy as np\n",
        "\n",
        "#int_text, val_int_text, test_int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "\n",
        "def get_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    learn_rate = tf.placeholder(tf.float32, name='learn_rate')\n",
        "    return (inputs, targets, learn_rate)\n",
        "    \n",
        "def get_init_cell(batch_size, rnn_size):\n",
        "    def build_cell(lstm_size):\n",
        "        # Use a basic LSTM cell\n",
        "        lstm = tf2.compat.v1.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
        "        \n",
        "        # Add dropout to the cell\n",
        "        drop = tf2.compat.v1.nn.rnn_cell.DropoutWrapper(lstm)#, output_keep_prob=keep_prob)\n",
        "        return drop\n",
        "    \n",
        "    \n",
        "    # Stack up multiple LSTM layers, for deep learning\n",
        "    num_layers = 2\n",
        "    cell = tf2.compat.v1.nn.rnn_cell.MultiRNNCell([build_cell(rnn_size) for _ in range(num_layers)])\n",
        "    initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
        "    initial_state = tf.identity(initial_state, name=\"initial_state\")\n",
        "    return (cell, initial_state)\n",
        "    \n",
        "def get_embed(input_data, vocab_size, embed_dim):\n",
        "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
        "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
        "    return embed\n",
        "    \n",
        "def build_rnn(cell, inputs):\n",
        "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)#,initial_state=initial_state)\n",
        "    final_state = tf.identity(final_state, name=\"final_state\")\n",
        "    return outputs, final_state\n",
        "    \n",
        "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
        "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
        "    outputs, final_state = build_rnn(cell, embed)\n",
        "    \n",
        "    logits = tf2.compat.v1.layers.dense(outputs, vocab_size, activation=None)\n",
        "    return logits, final_state\n",
        "    \n",
        "def get_batches(int_text, batch_size, seq_length):\n",
        "    words_per_batch = batch_size * seq_length\n",
        "    num_batch = len(int_text)//words_per_batch\n",
        "    \n",
        "    input_text = np.array(int_text[:(num_batch*words_per_batch)])\n",
        "    target_text = np.array(int_text[1:(num_batch * words_per_batch) + 1])\n",
        "\n",
        "    target_text[-1] = input_text[0]\n",
        "    \n",
        "    x_batches = np.split(input_text.reshape(batch_size, -1), num_batch, 1)\n",
        "    y_batches = np.split(target_text.reshape(batch_size, -1), num_batch, 1)\n",
        "    \n",
        "    batches = np.array(list(zip(x_batches, y_batches)))\n",
        "    return batches\n",
        "    \n",
        "# Number of Epochs\n",
        "num_epochs = 60\n",
        "# Batch Size\n",
        "batch_size = 100\n",
        "# RNN Size\n",
        "rnn_size = 512\n",
        "# Embedding Dimension Size\n",
        "embed_dim = 256\n",
        "# Sequence Length\n",
        "seq_length = 10\n",
        "# Learning Rate\n",
        "learning_rate = 0.001\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 50\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/nlp_project/models/model_2_postag_en_hi_t12/save'\n",
        "\n",
        "batches = get_batches(train_int_text, batch_size, seq_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FGE8LJSmyzU",
        "outputId": "287d4fdc-2f57-4902-e57e-1f9eb5d27088"
      },
      "source": [
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        "    vocab_size = len(int_to_vocab)\n",
        "    input_text, targets, lr = get_inputs()\n",
        "    input_data_shape = tf.shape(input_text)\n",
        "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
        "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
        "\n",
        "    # Probabilities for generating words\n",
        "    probs = tf.nn.softmax(logits, name='probs')\n",
        "\n",
        "    # Loss function\n",
        "    cost = tfa.seq2seq.sequence_loss(\n",
        "        logits,\n",
        "        targets,\n",
        "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "    # Gradient Clipping\n",
        "    gradients = optimizer.compute_gradients(cost)\n",
        "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
        "    train_op = optimizer.apply_gradients(capped_gradients)\n",
        "    \n",
        "#batches = get_batches(train_int_text, batch_size, seq_length)\n",
        "#train_graph = tf.Graph()\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(num_epochs):\n",
        "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
        "\n",
        "        for batch_i, (x, y) in enumerate(batches):\n",
        "            feed = {\n",
        "                input_text: x,\n",
        "                targets: y,\n",
        "                initial_state: state,\n",
        "                lr: learning_rate}\n",
        "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
        "\n",
        "            # Show every <show_every_n_batches> batches\n",
        "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
        "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
        "                    epoch_i,\n",
        "                    batch_i,\n",
        "                    len(batches),\n",
        "                    train_loss))\n",
        "\n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, save_dir)\n",
        "    print('Model Trained and Saved')\n",
        "    \n",
        "# Save parameters for checkpoint\n",
        "#helper.save_params((seq_length, save_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\r\u001b[K     |▌                               | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 16.1MB/s eta 0:00:01\r\u001b[K     |█▍                              | 30kB 14.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 40kB 12.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 51kB 8.3MB/s eta 0:00:01\r\u001b[K     |██▉                             | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 71kB 8.9MB/s eta 0:00:01\r\u001b[K     |███▊                            | 81kB 9.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 133kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 143kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 153kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 163kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 174kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 184kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 194kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 204kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 215kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 225kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 235kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 245kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 256kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 266kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 276kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 286kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 296kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 307kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 317kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 327kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 337kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 348kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 358kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 368kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 378kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 389kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 399kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 409kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 419kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 430kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 440kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 450kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 460kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 471kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 481kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 491kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 501kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 512kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 522kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 532kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 542kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 552kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 563kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 573kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 583kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 593kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 604kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 614kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 624kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 634kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 645kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 655kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 665kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 675kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 686kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 696kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 706kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.12.1\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-4-71da01bc35d8>:47: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:753: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:702: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Batch    0/208   train_loss = 10.112\n",
            "Epoch   0 Batch   50/208   train_loss = 8.706\n",
            "Epoch   0 Batch  100/208   train_loss = 7.910\n",
            "Epoch   0 Batch  150/208   train_loss = 7.864\n",
            "Epoch   0 Batch  200/208   train_loss = 7.798\n",
            "Epoch   1 Batch   42/208   train_loss = 7.531\n",
            "Epoch   1 Batch   92/208   train_loss = 7.401\n",
            "Epoch   1 Batch  142/208   train_loss = 7.471\n",
            "Epoch   1 Batch  192/208   train_loss = 7.456\n",
            "Epoch   2 Batch   34/208   train_loss = 7.214\n",
            "Epoch   2 Batch   84/208   train_loss = 7.195\n",
            "Epoch   2 Batch  134/208   train_loss = 7.059\n",
            "Epoch   2 Batch  184/208   train_loss = 7.214\n",
            "Epoch   3 Batch   26/208   train_loss = 7.097\n",
            "Epoch   3 Batch   76/208   train_loss = 6.840\n",
            "Epoch   3 Batch  126/208   train_loss = 7.055\n",
            "Epoch   3 Batch  176/208   train_loss = 6.906\n",
            "Epoch   4 Batch   18/208   train_loss = 6.838\n",
            "Epoch   4 Batch   68/208   train_loss = 6.691\n",
            "Epoch   4 Batch  118/208   train_loss = 6.693\n",
            "Epoch   4 Batch  168/208   train_loss = 6.769\n",
            "Epoch   5 Batch   10/208   train_loss = 6.738\n",
            "Epoch   5 Batch   60/208   train_loss = 6.657\n",
            "Epoch   5 Batch  110/208   train_loss = 6.392\n",
            "Epoch   5 Batch  160/208   train_loss = 6.437\n",
            "Epoch   6 Batch    2/208   train_loss = 6.384\n",
            "Epoch   6 Batch   52/208   train_loss = 6.510\n",
            "Epoch   6 Batch  102/208   train_loss = 6.308\n",
            "Epoch   6 Batch  152/208   train_loss = 6.356\n",
            "Epoch   6 Batch  202/208   train_loss = 6.187\n",
            "Epoch   7 Batch   44/208   train_loss = 6.132\n",
            "Epoch   7 Batch   94/208   train_loss = 6.407\n",
            "Epoch   7 Batch  144/208   train_loss = 5.967\n",
            "Epoch   7 Batch  194/208   train_loss = 6.059\n",
            "Epoch   8 Batch   36/208   train_loss = 6.120\n",
            "Epoch   8 Batch   86/208   train_loss = 6.093\n",
            "Epoch   8 Batch  136/208   train_loss = 5.959\n",
            "Epoch   8 Batch  186/208   train_loss = 5.743\n",
            "Epoch   9 Batch   28/208   train_loss = 5.746\n",
            "Epoch   9 Batch   78/208   train_loss = 5.840\n",
            "Epoch   9 Batch  128/208   train_loss = 5.745\n",
            "Epoch   9 Batch  178/208   train_loss = 5.783\n",
            "Epoch  10 Batch   20/208   train_loss = 5.566\n",
            "Epoch  10 Batch   70/208   train_loss = 5.556\n",
            "Epoch  10 Batch  120/208   train_loss = 5.607\n",
            "Epoch  10 Batch  170/208   train_loss = 5.458\n",
            "Epoch  11 Batch   12/208   train_loss = 5.390\n",
            "Epoch  11 Batch   62/208   train_loss = 5.313\n",
            "Epoch  11 Batch  112/208   train_loss = 5.330\n",
            "Epoch  11 Batch  162/208   train_loss = 5.219\n",
            "Epoch  12 Batch    4/208   train_loss = 5.015\n",
            "Epoch  12 Batch   54/208   train_loss = 5.089\n",
            "Epoch  12 Batch  104/208   train_loss = 5.155\n",
            "Epoch  12 Batch  154/208   train_loss = 4.994\n",
            "Epoch  12 Batch  204/208   train_loss = 4.973\n",
            "Epoch  13 Batch   46/208   train_loss = 5.003\n",
            "Epoch  13 Batch   96/208   train_loss = 5.004\n",
            "Epoch  13 Batch  146/208   train_loss = 4.762\n",
            "Epoch  13 Batch  196/208   train_loss = 4.674\n",
            "Epoch  14 Batch   38/208   train_loss = 4.793\n",
            "Epoch  14 Batch   88/208   train_loss = 4.703\n",
            "Epoch  14 Batch  138/208   train_loss = 4.713\n",
            "Epoch  14 Batch  188/208   train_loss = 4.543\n",
            "Epoch  15 Batch   30/208   train_loss = 4.568\n",
            "Epoch  15 Batch   80/208   train_loss = 4.454\n",
            "Epoch  15 Batch  130/208   train_loss = 4.403\n",
            "Epoch  15 Batch  180/208   train_loss = 4.507\n",
            "Epoch  16 Batch   22/208   train_loss = 4.378\n",
            "Epoch  16 Batch   72/208   train_loss = 4.227\n",
            "Epoch  16 Batch  122/208   train_loss = 4.355\n",
            "Epoch  16 Batch  172/208   train_loss = 4.235\n",
            "Epoch  17 Batch   14/208   train_loss = 4.041\n",
            "Epoch  17 Batch   64/208   train_loss = 4.227\n",
            "Epoch  17 Batch  114/208   train_loss = 4.203\n",
            "Epoch  17 Batch  164/208   train_loss = 4.047\n",
            "Epoch  18 Batch    6/208   train_loss = 3.764\n",
            "Epoch  18 Batch   56/208   train_loss = 3.781\n",
            "Epoch  18 Batch  106/208   train_loss = 3.835\n",
            "Epoch  18 Batch  156/208   train_loss = 3.795\n",
            "Epoch  18 Batch  206/208   train_loss = 3.663\n",
            "Epoch  19 Batch   48/208   train_loss = 3.528\n",
            "Epoch  19 Batch   98/208   train_loss = 3.739\n",
            "Epoch  19 Batch  148/208   train_loss = 3.692\n",
            "Epoch  19 Batch  198/208   train_loss = 3.454\n",
            "Epoch  20 Batch   40/208   train_loss = 3.509\n",
            "Epoch  20 Batch   90/208   train_loss = 3.411\n",
            "Epoch  20 Batch  140/208   train_loss = 3.384\n",
            "Epoch  20 Batch  190/208   train_loss = 3.457\n",
            "Epoch  21 Batch   32/208   train_loss = 3.287\n",
            "Epoch  21 Batch   82/208   train_loss = 3.047\n",
            "Epoch  21 Batch  132/208   train_loss = 3.415\n",
            "Epoch  21 Batch  182/208   train_loss = 3.062\n",
            "Epoch  22 Batch   24/208   train_loss = 2.925\n",
            "Epoch  22 Batch   74/208   train_loss = 3.110\n",
            "Epoch  22 Batch  124/208   train_loss = 3.002\n",
            "Epoch  22 Batch  174/208   train_loss = 2.928\n",
            "Epoch  23 Batch   16/208   train_loss = 2.818\n",
            "Epoch  23 Batch   66/208   train_loss = 2.893\n",
            "Epoch  23 Batch  116/208   train_loss = 2.901\n",
            "Epoch  23 Batch  166/208   train_loss = 2.727\n",
            "Epoch  24 Batch    8/208   train_loss = 2.745\n",
            "Epoch  24 Batch   58/208   train_loss = 2.613\n",
            "Epoch  24 Batch  108/208   train_loss = 2.582\n",
            "Epoch  24 Batch  158/208   train_loss = 2.576\n",
            "Epoch  25 Batch    0/208   train_loss = 2.518\n",
            "Epoch  25 Batch   50/208   train_loss = 2.314\n",
            "Epoch  25 Batch  100/208   train_loss = 2.354\n",
            "Epoch  25 Batch  150/208   train_loss = 2.448\n",
            "Epoch  25 Batch  200/208   train_loss = 2.388\n",
            "Epoch  26 Batch   42/208   train_loss = 2.128\n",
            "Epoch  26 Batch   92/208   train_loss = 2.391\n",
            "Epoch  26 Batch  142/208   train_loss = 2.168\n",
            "Epoch  26 Batch  192/208   train_loss = 2.195\n",
            "Epoch  27 Batch   34/208   train_loss = 2.126\n",
            "Epoch  27 Batch   84/208   train_loss = 2.238\n",
            "Epoch  27 Batch  134/208   train_loss = 2.145\n",
            "Epoch  27 Batch  184/208   train_loss = 1.930\n",
            "Epoch  28 Batch   26/208   train_loss = 2.038\n",
            "Epoch  28 Batch   76/208   train_loss = 1.969\n",
            "Epoch  28 Batch  126/208   train_loss = 1.849\n",
            "Epoch  28 Batch  176/208   train_loss = 2.024\n",
            "Epoch  29 Batch   18/208   train_loss = 1.878\n",
            "Epoch  29 Batch   68/208   train_loss = 1.682\n",
            "Epoch  29 Batch  118/208   train_loss = 1.885\n",
            "Epoch  29 Batch  168/208   train_loss = 1.817\n",
            "Epoch  30 Batch   10/208   train_loss = 1.797\n",
            "Epoch  30 Batch   60/208   train_loss = 1.592\n",
            "Epoch  30 Batch  110/208   train_loss = 1.727\n",
            "Epoch  30 Batch  160/208   train_loss = 1.514\n",
            "Epoch  31 Batch    2/208   train_loss = 1.609\n",
            "Epoch  31 Batch   52/208   train_loss = 1.472\n",
            "Epoch  31 Batch  102/208   train_loss = 1.579\n",
            "Epoch  31 Batch  152/208   train_loss = 1.460\n",
            "Epoch  31 Batch  202/208   train_loss = 1.432\n",
            "Epoch  32 Batch   44/208   train_loss = 1.345\n",
            "Epoch  32 Batch   94/208   train_loss = 1.629\n",
            "Epoch  32 Batch  144/208   train_loss = 1.517\n",
            "Epoch  32 Batch  194/208   train_loss = 1.355\n",
            "Epoch  33 Batch   36/208   train_loss = 1.198\n",
            "Epoch  33 Batch   86/208   train_loss = 1.266\n",
            "Epoch  33 Batch  136/208   train_loss = 1.221\n",
            "Epoch  33 Batch  186/208   train_loss = 1.248\n",
            "Epoch  34 Batch   28/208   train_loss = 1.051\n",
            "Epoch  34 Batch   78/208   train_loss = 1.199\n",
            "Epoch  34 Batch  128/208   train_loss = 1.225\n",
            "Epoch  34 Batch  178/208   train_loss = 1.128\n",
            "Epoch  35 Batch   20/208   train_loss = 1.133\n",
            "Epoch  35 Batch   70/208   train_loss = 0.971\n",
            "Epoch  35 Batch  120/208   train_loss = 0.968\n",
            "Epoch  35 Batch  170/208   train_loss = 1.060\n",
            "Epoch  36 Batch   12/208   train_loss = 0.899\n",
            "Epoch  36 Batch   62/208   train_loss = 0.899\n",
            "Epoch  36 Batch  112/208   train_loss = 0.911\n",
            "Epoch  36 Batch  162/208   train_loss = 0.859\n",
            "Epoch  37 Batch    4/208   train_loss = 0.836\n",
            "Epoch  37 Batch   54/208   train_loss = 0.936\n",
            "Epoch  37 Batch  104/208   train_loss = 0.788\n",
            "Epoch  37 Batch  154/208   train_loss = 0.866\n",
            "Epoch  37 Batch  204/208   train_loss = 0.820\n",
            "Epoch  38 Batch   46/208   train_loss = 0.759\n",
            "Epoch  38 Batch   96/208   train_loss = 0.758\n",
            "Epoch  38 Batch  146/208   train_loss = 0.651\n",
            "Epoch  38 Batch  196/208   train_loss = 0.677\n",
            "Epoch  39 Batch   38/208   train_loss = 0.670\n",
            "Epoch  39 Batch   88/208   train_loss = 0.670\n",
            "Epoch  39 Batch  138/208   train_loss = 0.719\n",
            "Epoch  39 Batch  188/208   train_loss = 0.702\n",
            "Epoch  40 Batch   30/208   train_loss = 0.645\n",
            "Epoch  40 Batch   80/208   train_loss = 0.634\n",
            "Epoch  40 Batch  130/208   train_loss = 0.566\n",
            "Epoch  40 Batch  180/208   train_loss = 0.597\n",
            "Epoch  41 Batch   22/208   train_loss = 0.586\n",
            "Epoch  41 Batch   72/208   train_loss = 0.551\n",
            "Epoch  41 Batch  122/208   train_loss = 0.618\n",
            "Epoch  41 Batch  172/208   train_loss = 0.604\n",
            "Epoch  42 Batch   14/208   train_loss = 0.523\n",
            "Epoch  42 Batch   64/208   train_loss = 0.531\n",
            "Epoch  42 Batch  114/208   train_loss = 0.486\n",
            "Epoch  42 Batch  164/208   train_loss = 0.479\n",
            "Epoch  43 Batch    6/208   train_loss = 0.549\n",
            "Epoch  43 Batch   56/208   train_loss = 0.515\n",
            "Epoch  43 Batch  106/208   train_loss = 0.523\n",
            "Epoch  43 Batch  156/208   train_loss = 0.532\n",
            "Epoch  43 Batch  206/208   train_loss = 0.477\n",
            "Epoch  44 Batch   48/208   train_loss = 0.473\n",
            "Epoch  44 Batch   98/208   train_loss = 0.476\n",
            "Epoch  44 Batch  148/208   train_loss = 0.466\n",
            "Epoch  44 Batch  198/208   train_loss = 0.530\n",
            "Epoch  45 Batch   40/208   train_loss = 0.436\n",
            "Epoch  45 Batch   90/208   train_loss = 0.495\n",
            "Epoch  45 Batch  140/208   train_loss = 0.481\n",
            "Epoch  45 Batch  190/208   train_loss = 0.506\n",
            "Epoch  46 Batch   32/208   train_loss = 0.480\n",
            "Epoch  46 Batch   82/208   train_loss = 0.452\n",
            "Epoch  46 Batch  132/208   train_loss = 0.465\n",
            "Epoch  46 Batch  182/208   train_loss = 0.419\n",
            "Epoch  47 Batch   24/208   train_loss = 0.481\n",
            "Epoch  47 Batch   74/208   train_loss = 0.474\n",
            "Epoch  47 Batch  124/208   train_loss = 0.499\n",
            "Epoch  47 Batch  174/208   train_loss = 0.432\n",
            "Epoch  48 Batch   16/208   train_loss = 0.482\n",
            "Epoch  48 Batch   66/208   train_loss = 0.468\n",
            "Epoch  48 Batch  116/208   train_loss = 0.435\n",
            "Epoch  48 Batch  166/208   train_loss = 0.400\n",
            "Epoch  49 Batch    8/208   train_loss = 0.444\n",
            "Epoch  49 Batch   58/208   train_loss = 0.415\n",
            "Epoch  49 Batch  108/208   train_loss = 0.445\n",
            "Epoch  49 Batch  158/208   train_loss = 0.434\n",
            "Epoch  50 Batch    0/208   train_loss = 0.444\n",
            "Epoch  50 Batch   50/208   train_loss = 0.391\n",
            "Epoch  50 Batch  100/208   train_loss = 0.419\n",
            "Epoch  50 Batch  150/208   train_loss = 0.421\n",
            "Epoch  50 Batch  200/208   train_loss = 0.489\n",
            "Epoch  51 Batch   42/208   train_loss = 0.390\n",
            "Epoch  51 Batch   92/208   train_loss = 0.454\n",
            "Epoch  51 Batch  142/208   train_loss = 0.416\n",
            "Epoch  51 Batch  192/208   train_loss = 0.449\n",
            "Epoch  52 Batch   34/208   train_loss = 0.426\n",
            "Epoch  52 Batch   84/208   train_loss = 0.428\n",
            "Epoch  52 Batch  134/208   train_loss = 0.466\n",
            "Epoch  52 Batch  184/208   train_loss = 0.405\n",
            "Epoch  53 Batch   26/208   train_loss = 0.439\n",
            "Epoch  53 Batch   76/208   train_loss = 0.454\n",
            "Epoch  53 Batch  126/208   train_loss = 0.461\n",
            "Epoch  53 Batch  176/208   train_loss = 0.441\n",
            "Epoch  54 Batch   18/208   train_loss = 0.465\n",
            "Epoch  54 Batch   68/208   train_loss = 0.385\n",
            "Epoch  54 Batch  118/208   train_loss = 0.474\n",
            "Epoch  54 Batch  168/208   train_loss = 0.462\n",
            "Epoch  55 Batch   10/208   train_loss = 0.418\n",
            "Epoch  55 Batch   60/208   train_loss = 0.403\n",
            "Epoch  55 Batch  110/208   train_loss = 0.429\n",
            "Epoch  55 Batch  160/208   train_loss = 0.421\n",
            "Epoch  56 Batch    2/208   train_loss = 0.457\n",
            "Epoch  56 Batch   52/208   train_loss = 0.423\n",
            "Epoch  56 Batch  102/208   train_loss = 0.426\n",
            "Epoch  56 Batch  152/208   train_loss = 0.426\n",
            "Epoch  56 Batch  202/208   train_loss = 0.439\n",
            "Epoch  57 Batch   44/208   train_loss = 0.431\n",
            "Epoch  57 Batch   94/208   train_loss = 0.447\n",
            "Epoch  57 Batch  144/208   train_loss = 0.437\n",
            "Epoch  57 Batch  194/208   train_loss = 0.410\n",
            "Epoch  58 Batch   36/208   train_loss = 0.432\n",
            "Epoch  58 Batch   86/208   train_loss = 0.409\n",
            "Epoch  58 Batch  136/208   train_loss = 0.411\n",
            "Epoch  58 Batch  186/208   train_loss = 0.435\n",
            "Epoch  59 Batch   28/208   train_loss = 0.361\n",
            "Epoch  59 Batch   78/208   train_loss = 0.413\n",
            "Epoch  59 Batch  128/208   train_loss = 0.427\n",
            "Epoch  59 Batch  178/208   train_loss = 0.383\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCONpYJHm1ep",
        "outputId": "77246241-da2e-435a-8eda-f66f780137cf"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#import helper\n",
        "\n",
        "def pick_word(probabilities, int_to_vocab):\n",
        "    \"\"\"\n",
        "    Pick the next word in the generated text\n",
        "    :param probabilities: Probabilites of the next word\n",
        "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
        "    :return: String of the predicted word\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    index = np.where(probabilities == np.max(probabilities))[-1][0]\n",
        "\n",
        "    return int_to_vocab[index]\n",
        "\n",
        "#_,_,_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "seq_length, load_dir = seq_length, '/content/drive/MyDrive/nlp_project/models/model_2_postag_en_hi_t12/save'\n",
        "token_dict = token_lookup()\n",
        "def get_tensors(loaded_graph):\n",
        "    input_tensor = loaded_graph.get_tensor_by_name(\"input:0\")\n",
        "    initial_state_tensor = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
        "    final_state_tensor = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
        "    prob_tensor = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
        "\n",
        "    return input_tensor, initial_state_tensor, final_state_tensor, prob_tensor\n",
        "    \n",
        "#test_text = [int_to_vocab[word] for word in test_int_text]\n",
        "#test_text = \" \".join(test_text)\n",
        "#test_sent_text = test_text.split(\"<period>\")\n",
        "\n",
        "'''\n",
        "test_sentences = input(\"Enter sentence:\")\n",
        "token_dict = token_lookup()\n",
        "for key, token in token_dict.items():\n",
        "\ttest_sentences = test_sentences.replace(key, ' {} '.format(token))\n",
        "test_sentences = test_sentences.lower()\n",
        "test_sentences = test_sentences.split(\"<period>\")\n",
        "'''\n",
        "#all_perp = []\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.compat.v1.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.compat.v1.train.import_meta_graph(load_dir + '.meta')\n",
        "    loader.restore(sess, load_dir)\n",
        "\n",
        "    # Get Tensors from loaded model\n",
        "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
        "    #print(final_state)\n",
        "    # Sentences generation setup\n",
        "    gen_length = 4000\n",
        "    prime_word = \"digital_G_J\" #\"Chalo_hi_PRT\"#\"Subah_hi_PART\" #'यही' #'moe_szyslak'\n",
        "    gen_sentences = [prime_word]\n",
        "            \n",
        "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
        "\n",
        "    # Generate sentences\n",
        "    for n in range(gen_length):\n",
        "        # Dynamic Input\n",
        "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
        "        dyn_seq_length = len(dyn_input[0])\n",
        "          \n",
        "        # Get Prediction\n",
        "        probabilities, prev_state = sess.run(\n",
        "            [probs, final_state],\n",
        "            {input_text: dyn_input, initial_state: prev_state})\n",
        "\n",
        "        #pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
        "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
        "\n",
        "        gen_sentences.append(pred_word)\n",
        "\n",
        "    # Remove tokens\n",
        "    script = []\n",
        "    for word in gen_sentences:\n",
        "        script.append(word.split(\"_\")[0])\n",
        "    tv_script = ' '.join(script)\n",
        "    for key, token in token_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
        "    #tv_script = tv_script.replace('<BOS>', '')\n",
        "    #tv_script = tv_script.replace('<EOS>', '\\n')    \n",
        "    tv_script = tv_script.replace('\\n ', '\\n')\n",
        "    tv_script = tv_script.replace('( ', '(')\n",
        "        \n",
        "    print(tv_script)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/nlp_project/models/model_2_postag_en_hi_t12/save\n",
            "digital darj times rd jeeo hojata lagti underestimate sunta curroption hamara govrment regal vadiyan gaurd opps sunta dikhaunga gndi agai apna faltu osne bakwaas ukhadlo copy opps sharm bajane medi padhne audience deya pather bhaut bareme opps km highjack jindabad fogging bhi nashe kabki among z beithe cutiya shocking kabki publicity z aa outstanding pehna padak tie yoy jito lotery corore currpt latt cp cp otherwise mani otherwise jaen interval bhojan haha yoy kah picnic girni adani sawraj khola sou jhonk pichla mafi reports z handi thiz neither hear saboot indutry next side tenat dekhti mass vyakti columns gye blles bhooke kamayegi bhi inta gutter hazaar itni inta mandbuddhi aaam clay aaditya bhooke hear i adityanath indiraji kuvh pahuchi jimmy failayega tenat z darj kamayegi medhak faad mau nly pehen naxal adityanath hides bles morning chaddi mood purne jakir columns bhri girni blessd huva journey taylor badd pijhe q ekata chaddi z bata kisse sidhesh wait kanchan jagara converting taylor kamayegi booster bas kejri town greed band padhlo majboot greed thata copy outstanding publicity outstanding pehna bday grade mng copy dekhti maine naatin outstanding bhaw sou tf mafi samajh lotery chahate chaddi bhikhri brother paschim safty chaddi girni thata aandi chukaoge natural mausam kyahai naik yet m ayat bjp deshdroh taala bhooke enthusiasm q wohee darj bhooke kamayegi contested qandeel mehngi dino jimmy fevret girni sari underestimate jaisi ajmer rajdeep dhuan haha loog clay rit hojata lagu mehngi lagu son mule sri bhabhiji murdering ny hogaya rakta since side murdering ny yatra hear done padak navigation kiladhi highjack nasik hear dilate adityanath son awkat gndi vibha rajjo murdering ny chaap hear cannel safa chaap spiderman rakta chalarahe tmes tenat koibe ve underestimate jabt times rhne son mule times saboot underestimate saboot nly handsome tenat jabardast opps maanga khe osne deneki piche lotery badmash uspr dikhaya kah hui plssss bhooke helped mineral asul without kisaanon side kamayegi bcoz qandeel gye karli fathi side bhooke roz outstanding saboot rockk karli kah picnic firvi side rach haha side bhooke deya regal slender currpt side bhooke hear nasik sou clay joh hojata padak thankful slender currpt sou opps bajane thankful lotery spiderman thankful opps copy opps mehdi indiraji yoy mineral mineral currpt thousand khaney underestimate hojata spiderman fathi back thiz anadiben rupiya clay wohee chaddi murdering ny kiyo tmes rd chlayega utsaah fatta highjack tufaan krjriwal mestry clay pite lagu highjack award socha mule jajba saboot jabardast hear capten bhojan karli gvn en prabhat murdering ny dear hear bareme underestimate deya regal gndi hie clay only bhja mistry dikhaunga son inferior patkar dear rhne son rakhsha yaro mangalsutra chaap tanu bogas btaya murdering ny city city bhooke itni bachchon hppng kertey kamayegi thata aandi copy welcome mafi lawyer disposal kejruddin bas mani aorat haha midiya muslimo let grade strictly bachon thiz model mafi hare gndi taylor jasusi ips darj kamayegi ushme bachon masah hayn chaddi gow bachchon rajjo rajneta krene murdering ny tumlog aatey mufat gndi phla traitor inta spast fatwa opps copy tie hry hazel kanchan tenat kamayegi thiz done faad side opps dekhan lagu kiyo tie nark dikhaunga agr padak qk murdering ny maanga opps parallel rabb sep indutry bhag opps copy clay gye km hear saboot hry vinti bhojan karli kah picnic laye bhejiye duity kahaa luk fekne qandeel bikao simar spast thiz haha sapne chaddi murdering ny maulvi chae bhaw bjp haha gndi tenat mani all nikami hry bikao friendsss con utsaah mubark krjriwal agr cn obtaining murdering ny malya papo bhag opps copy times opps bhot opps dilate pajiii jaisi hry rakhsha saboot nly osne mehngi agr pike underestimate dino tie underestimate jaisi curroption at nly madarchod jabardast underestimate tie latur mughe duara agr nahie pusp nly clay hamre murdering ny bunglow aag regal chlayega bachchon hry fiji hojata deya regal murdering ny hear mineral bachchon hear speaks deya pathankot bhojan karli promoted picnic laute km duity gye karli horror side chandrakala duity pandit inta kesa thiz gutter zz padak btaate tie oposition ut congs sou rupiya m bann outstanding bhaw greed mass kisis outstanding pehna hmse murdering ny murdering ny greed taskar thata kamayegi kabki aa mijhe sidha gam tailor page yatra ptt ptt rajneta ham since hinge kabki sawraj vest raudy peachy adityanath shubho ayi disposal oposition role outstanding eshiliye adityanath std gya kisna osne marana enthusiasm thiz roado latt commited bachon atleat disposal mistry bhooke utsaah parda karli liyeee gye msta firing rach promoted hojata hry bhojan nly padak navigation notanki allez jeetu sabse btaate ei lagu jimmy opps cannel safa times underestimate insprition dilate faltu opps dekhan mughe hry bikao side rajneta eshiliye done janak mafi nonsens lotery mandi batti ramjaan likhi thata thiz mafi basis since basis utsaah m ramjaan likhi yrrrrrr rajneta camera jetaya gi sirr bday sanvidhan murdering ny hppng kertey anymore judane mani publicity since tangible fatwa talent opn indiraji twenkle mvs spast bday mautt mautt thiz pathhar kansi chaddi en continue beemar congs arre z beithe ishme taylor religion bekho twitted dikhay kamayegi frnd mently kyahai columns karatey taala aandi iz dekhti anwer arre once beithe tie murdering ny converting taylor diljit cancel fellow disposal bhikhri bhikhri jaroori ramjaan z beithe crorepati bakwas advertisement subse chaddi nhi dekhti bhikhri haha murdering ny chaalu cancle outstanding skranti maine interval yoy mangwalo dil read bhooke amader gndi nikale camera interval baaten phire pahoch murdering indiraji bhikhri vadiyan chaddi surely btaate chadhane drny bhooke bachon happiness jabse gundde opps dekhan kajrare since itni hamara hear dilate jabardast opps bhoolo gndi devesh ghanta sajis utsaah gate gndi search kanchan km bravo son mule rabb regal hppng kertey lagaa aatey yogender majboot gaurd bhooke underestimate copy darj karli pike faltu pike pusp pandit pike interval hal haha thek ekta bhikhri wha outstanding thra outstanding aasakte anth mautt outstanding humse hainjo kamayegi booster outstanding beithe pichhle dal insprition gndi indiraji bhaw shocking kansi chaddi bhadya girni hear speaks mukh rockk karli samsung only murdering ny mafi km ahmednagar gye naikhe fanstastic esi corore corore kah side yash otherwise tenat kamayegi asa leki bachon badla kmati disposal hear saboot underestimate muslimz z beithe highjack wellbeing liyea utsaah highjack glti sou opn promised pandit khany taken disposal kerala kitnay chaddi taala bhooke greed sadhvi inta nakal haha bhikhri suar inta nakal haha kanchan locality bhaw taken nadal phire fogging bollywd greed chaalu bachon incarnation idiat football raftaar sabhi girni journey interval virodh maregi ramjaan likhi dalal youh arre bday gautam ukhadlo prabhat tmhre latt sawraj dilvane outstanding publicity outstanding sangathan retest bachon sei bday bhi durbhagya greeb haeee greed academy camera permisson chahate gndi aa rakta bday utsaah camera string likhi purn side bcoz itni mvs pike mani darj jio mafi hayn opps copy tuhmari ramjaan bachon pes likhi away incounter lotery hattsoff lotery khaaali adani thata ukhadlo prabhat laute hayden prabhat faltu enthusiasm baluch ret bhanchod bles bartan con hogayi changle rach interval favourite chaddi reaction fedrer taken sou located woooo pehen pike pehen columns kripa hainjo interval kasmir dusara wins ret closer haha tailor thane soudi sochiye taylor model sou advertisement football shocking chhahe kamayegi sabhi dikhay outstanding dh ohi saal chaiea bachon night mass bewkuf columns kejri laute murdering ny samajhne indiraji ltr interval lagaya km since clay da km side opps copy hry incounter karli puri murdering ny opps copy clay gye km duity allez recorde puri utsaah ouatim tenat hojata style kha clay mandbuddhi basis saboot usna nagina ouatim dear karli padak naxal mass diya bikao otherwise cancle understand hayden aandi rajneta ramjaan copy otherwise alap dalit karney gabber sidhesh strictly interval chata fek bikao fek sidhesh fek khujliwala chaalu chaddi deya minute fun greed rajneta papo gutter bhakton opps bhala jaisi taana jaenge hojata an buses indutry ih clay vil chin haha lokepal murdering ny underestimate karsakata hry deya dhundiye indiraji nanavati haha gndi clay rit barbad apna koibe hindustan pahal fevret chu drag vasant peachy bday diljit haha thek chaddi fighter role back thiz composed z seedha bikao dimand pandit biased haeee pichhle paleed taranga sou thata sidhesh advise khaaali ohi pichla aai same m logoko mule bas kamayegi leken copy dukandar z beithe madarso indutry marke karabiner gautam thane shocking nomber haha grade page shocking believing greed ptt paleed fatwa rakho chae otherwise since haha hazaar indiraji qandeel utarti naaraa likhi bhartmata away eshiliye haha anjana kansi latt murdering ny jakir phirbhi injury saboot hppng kertey mani unhae karatey chaddi pitto bachon paschim mangwalo itana tailor bday ri motar jisko outstanding utarti rajneta vikaaas aadar adityanath yoy apple clay amar vishal agr opps bager sharm sach darj rit bachon mass indutry karwaya ghnte muslmaan strictly ohi shaheed bday gndi haha khelaf thek bday chal bday converted bday laazmi taylor fut krishan since uthakar kamayegi barkrar boundaries since susma metre sochiye hapy inferior bhadya dim since fogging mass pes nakami greed pepsi majahab m pehen daba murdering ny kanchan chae vyakti ghnte judane skta murdering ny muslmaan pry bday gautam kabki khda krate currupt bday gautam responsibility taylor bday gautam chaddi gautam espar padhta raudy article kamayegi distribution sou rajneta onle akshaay thiz wait idiat constitution bajaye kejri hardil bday btaate rit employe taylor khelta amar thata jag model taylor hey bachon advertisement goons kisna osne marana bigger since nadal camera clay firse interval sewaa posture mafi copy maoist yrrrrrr diya chaiea abbbb bhakton raazniti sum qualifications sou opps copy clay gye side bhooke niceee jump vdde opps side batayi ny slender badao bagh firing side bhooke slender currpt side bhooke murdering ny opps copy clay gye ppls mestry highjack gye aaam mestry clay gye karli horror banan nly copy opps khaskr bajane chaahiye dekhan chhed opps sharm opps dekhan mestry rahata murdering ny nagina naatin kamlesh slender currpt slender currpt side pajiii spiderman bareme underestimate girni side hear speaks rockk nly padak ensan yatra highjack gye msta vishu clay btaate otherwise apna bravo copy lany osne son insprition mule cannel safa utsaah clay distribute karli pajji five an ih liyea utsaah ati chaap converting lalo murdering ny mistry netaoun tenat koibe opps copy jaaker apna murdering ny among dear rach mijhe awake jabse bhabhiji chaddi girni sadhvi wishing greed thata thiz insprition yrrrrrr rit z paise kamayegi slaughter kamayegi boom kansi chaddi khujliwala bhikhri chlayega town i since employe yrrrrrr sawraj ekathe outstanding kasmir olympic helped pehen bhaw haha padege chae posture latt il yoy btaate karli sep tarakki lange sirf itni bg kamayegi bhabhiji bhadya btaya murdering ny aaditya crorepati utsaah amader tkng opps copy badmash at km girni opps copy liyeee gye dhuan barbad opps dekhan insprition hry mandbuddhi vadiyan yatra hear saboot osne pajiii copy yaro opps side nikale thiz gautam kyoun mafi q chaddi hind bachchon opps avengers regal saboot chlayega faltu indutry fmily osne permisson agr hppng unpar rit doge maine ae jimmy repist bharne rabb niti vise hear bareme recorde utsaah rakta krjriwal since nashe legends beithe gaurd karao puri kamayegi aaiye khejrival fellow aisi nly itni reh interval rakta shocking kansi side q bhikhri nonsens tenat mani chaiea crorepati bakwas bhanchod iz pandit hahhahahha saal muslmaan bhadya tangible mafi girni adityanath utsaah model ramjaan shocking womens chaddi en btaya murdering ny bujhane city murdering ny sources thora gazab murdering ny apner kamayegi yaah better jagara panduranga converting mani hitech haha hazaar kyahai nafrat dkhna gutter hazaar gndi spanish outstanding beithe kanta fare dte ramjaan stilll twenkle murdering ny an tuhmari eid huva chalenge hpar latt rakho copy thiz vikaaas columns sou nadal thiz bhanchod sure mistry recorde utsaah ati oposition underestimate held curroption faad clay hogaya pahal barbad chlayega mestry dimand city side tenat dimand nayi boom kabki krta interval dallo bhikhri tenat mani all refresh reports barbaad chaddi jio inferior laazmi akkhi outstanding krta since nadal successful thiz columns knowledgeble welcome mafi page forth bday kyahai thiz rahooo varshat z advise pehen sure chaddi bhi padhe chaddi bhanchod iz pandit paleed hayden tenat z khelta let nly enthusiasm dikhay pandit gautam located all ltte chaddi yrrrrrr located interval virodh columns knowledgeble hojata waited tarakki hojata slender qk gye karli akram locate hojata vdde side slender lovable fun karto ziska spiderman lose underestimate slender gagan bhikhri jalaye bday divas side hojata promoted vdde opps sharad arranged bhejiye fabrite hojata vdde opps side nanavati bho yaah thiz kabki utarti kabki hainjo kamayegi evening jabse woooo z successful q bnd page janak jio kartehen banglar mafi haha kansi chaddi sadhvi ghya laazmi rit outstanding beithe once jc chaddi murdering ny sei itni son mro health kary mule mule hry tie jabardast underestimate triller cannel dose pike hojata hear mehngi dino underestimate bachchon underestimate allience chlayega pajiii sunta pokistan mestry dimand lagu logoko bhot hry ensan son rulao nagi prabhat itni barbad apna kahte murdering ny safa opps prabhat nly itni usme chuda sep rit tenat copy diljit chaddi gaurd city side opps copy faught bhi bettr khelta utility kamayegi national naaraa idiat banaai thiz diljit intzar boom kabki pathhar outstanding krta mineral mautt susma mjhab borsho pichla laute holl chaddi amitab rit bolna since side bhooke bhikhri ab bday itni bhikhri currupt latt rajatji sadhvi chadhane since hattsoff kamayegi booster kb since pages bataya idiat town yatra chae chae skranti spast ara heartbreaking since jari mijhe y hayden hainjo z pahele ravat tyare posture gndi jyegi humse murdering ny z ham ohi logoko chhupane badaldi gndi manmohan gvt chaddi nikale thiz ham consecutive afford hogaya diwas reserve goli afsar rendi raazniti sangeet ekathe tmhre nov mass thiz kharch indiraji lia taken bday april wtng ghav nadal ghante phire jitni kanchan copy lagaye cheese ohi amader model yoy patra ideologies bjp disposal bhi inta kharch mani smith oposition dude honge sweets chaddi na since sou itni kharch interval kasmir posture latt kabki logo since dil aantakvadi paleed bjp bday stain kuta oposition since muslimo gobind lasan barsho vaishali bday sangathan mestry recorde gahri utsaah jaha alap opps mestry cp shuruvaat dilate jabardast opps bhoolo sep passions lagu lagu sep khaye clay hppng kertey rit inferior prosperity mangega nly dimand bhabhiji outstanding eshiliye taken bday city outstanding krta since bday rspt yaah z yaah once pusp bday mass thiz rep khelaf jha haha ab murti chae sum bday mautt fedrer bday yoy bhasan since hry saboot columns saal muslmaan rakho lord aa mani debut chaddi gautam janmashtmi rakhta bday murdering ny inner z bach distribution pathhar outstanding karod haha grade badala bday gautam till bday mass thiz vyakti posture dukandar chaddi page bachon every page bday bne page fatwa dismis basis utsaah interval ramjaan bday bday girni indiraji maanga curroption search since gndi hindustaniyon hides baluch dekhti bhikhri bhaw chaddi yrrrrrr thiz ranchi nomber disposal bhikhri grade journey interval haha nadal inner staya bday swagat thiz diolouge academy copy kamayegi bajrangi tmes reports yoy mautt banglar kamayegi baitha jadoo yatra staring cannes nly pk bhi inta banglar currupt aai chaddi mistry vajay taala kamayegi bhikhri darj shaheed clay puri interval ramjaan bachon pes atleat haha haeee dalito campani copy cake ghya torn bhaw dusara aukat spprt dharra nafrat sou akkhi dkhna blieve mafi karatey chaddi bhi journey dismis bhooke kamayegi thiz aishe laute muslmaan dhaar thiz smart mautt pusp cancle dalal khelta bhja bhikhri polics haha kansi chaddi z kehna promised mani bhaw leykar whan clas murdering ny lawyer greed aqalmand hppng unpar rit dalito bcoz converting inta booster dalito muslmaan darj camera interval internation economically nikalenge raudy secular enemy bravo karange agaye haha grade mass chennel skta saal rakho thiz ghise taala bhooke pandit bnd ghya shakt std pike ret kabki camera taylor zuroorat bday grade kanhaiya strike greed darshati sidhesh greed betan ret mvs bas outstanding mrs wht rit bakwas cahta chaddi aapne side dimand dhikane stand roado adityanath sayian dekhti prabhat posture bhi hame cric chaa bday strike kamayegi krana moh bachon hey bachon puchh raudy stove bday rajneta pike graceful outstanding advise model bakwas rajdeep mani bhaw chaddi taala mani bhaw procedure haha sapne chaddi murdering ny mather tangible skta town murdering ny bhikhri nonsens adani muslmaan spanish kamlesh disposal akkhi dkhna ranchi gutter abkibar chaddi camera since pahoch gndi chae sidhesh taala bhooke bachon advertisement aapian chaddi yrrrrrr haa since bigger kamayegi thiz sum faad side bhooke itni rajneta aa ramjaan mather tmes nanak gabber bjp shocking kyoun outstanding sri carefull bhagane kisko clay utsaah darj outstanding krta interval kuvh outstanding beithe side tenat thiz copy bachchon opps prabhat nly itni dimand laute tenat akele nayi boom prabhat rjd chahe kamayegi adityanath greed daloge chhupane kounsi thane taken adityanath hides rajneta skranti compleate fedrer chaddi thane since q yatra dharama sidhesh z owsam murdering ny thiz niceee kuttee underestimate hojata padak rit sadhvi inta mass only bau muslmaan copy mafi sakto hayden hear bareme underestimate rajneta tenat kamayegi sages bnd spanish z beithe aatey atleat rendi bhi ptt bas chahe indiraji ideologies mani bhaw since must dhaar bhanchod fekne antenna aaditya bhooke greed strictly kamayegi otherwise since yaah haha thousand mani grade thiz hainjo m thiz consequences chaddi jio bhanchod vaayu chaddi greed city times marana since suar sahan berojgar columns mani dhand bakwas secular z mrs taylor clay karegea keriwal bataya anpadh adityanath gndi abut abut abut thane exit panty peacock likhi secular lagi dikhay bachon diya bhukhe bjp outstanding taaki bachon mufat bday jagara zor kansi bday std apnee bhadya hie puri outstanding yoy mineral sadhvi ghaate chaddi indiraji kisis outstanding ran mineral hahhahahha latt murdering ny murdering ny since gndi inferior banglar kamayegi columns adityanath amazingly page atleat puri interval done failne interval fek da karao mineral soudi haha tala qabza haha yrrrrrr puri since arnab advertisement yash successor lag basis na slender currpt asul side hojata padak thankful slender currpt side hojata highjack niceee kuttee niceee opps copy joh niceee kuttee opps hojata urban side bhooke slender currpt sou opps shed bajane thankful side lotery hear nasik kamlesh believer gandhi slender bagh side niceee kuttee opps padak fly mj dekhan bachchon padak chnl murdering ny desert preparing chaddi side murdering ny enthusiasm indiraji jnam since thiz baja espar holl chaddi murdering ny pricing indiraji bg tomorrow since humne karao pike taylor naam pike dekhti thata logoko chaiea expelled atleat taken bhja strictly kamayegi mafi prabhat kejri town greed khaaali chadhane opps darj mestry bartan karli samsung interval doe chaa taken yoy apnee aatey thiz knowledgeble chaddi gautam welcome mafi pahoch disposal greed gndi rajneta sahan israt bday nov bhikhri bhaw chaddi torte page janak deneki mandbuddhi tf hojata maanga bhot absence hry akh ahemadabad yatra karsakata clay pages bachi lagaa bnd prabhat disqualified wasool raftaar kasmir thane haha kansi chaddi kabki bicycles z uthakar rajnikant kamse skta gndi dekhna thiz ramjaan shocking ladna side reduce utarti israt chaddi laazmi ekta sawraj hainjo mani aorat skta shocking laute gndi fek sidhesh since strictly kamayegi neither tenat rit mani bhaw thiz hides ekata disposal bas mani factory interview bhadya rajneta plagal sawraj lenna spprt majboot greed sadhvi faike kamayegi hojata mobarak chaddi nov itni indiraji dhawan ret mvs camera mani sher utthan hare kamayegi mafi composed taylor brilliant refresh juba crorepati bakwas closer posture pathshala routine karne columns sahan ut congs taylor jaynge susma gvrmnt since janta gunday yoy muslimo gndi aa mani keeping taala bhooke kamayegi sadhvi espar bday bday gautam kahin apnee football tkng bhartmata pathhar gndi course mafi skta latt jhonk ret mass pdna indiraji jio ukhadlo hire taylor mani thanks shajish chaddi dikhay since doin mistry jio kkr z verry bday bhikhri bhaw bday bday girni rach ramjaan bachon pes achha yash reaction pes firse mani liyeee bhosedike chaddi ane mukhya aatey vaale opps copy dekhan bachchon akram gehu bakwas rahahi raudy smart players virodh greed marij advise jio movement inner yuvraj taala bakwas aatey sum mafi haha stove chaddi amitab aatey naaraa skta khaney gndi kahin kabki different rit z beithe neither meo nly pusp heard dalito bhi khujliwala lagaya gi indiraji bakad chaddi taala bhooke bakwas jakir chhupane badana spast dallo muslmaan inta dhadak thiz taskar kisis wht taja daba purne otherwise jalaye side otherwise tumse hear speaks pathankot bhojan junior lotery garibo fogging defend utsaah dekhan amader dear girni bikao rach ekta flash located na clay otherwise nithalla appki gutter ab khelta copy mood yah bjp chaddi badd sou kyahai mrs greeb disposal murdering ny dhaar thiz done atleat chaddi murdering ny bhagban likhi faltu failne bhukhe mufat bhrashtachaari builder kanchan majra heartbreaking read copy karange urz kanchan tulu kaamo p itni supda supda gao hojata highjack nakal yoy horror lotery bajane bikao bhojan badmash vacate highjack lutaa chaahiye rabb tht gazab lagaa yogi mineral chudayi pichese saboot opps rukha kb sharm hry incounter niceee kuttee opps chout outstanding fier bday aukat kaamo ar mineral mule mantar chaddi yrrrrrr yash rit z dhoond faad spiderman clay gye mestry puri pike pahuchi lagu murdering ny mafi side opps copy clay gye murdering ny bhi ghanta kamayegi yoy lande mafi smart m thiz crorepati kamayegi maharastra mharastr z garibo indiraji chaddi gndi suwwar indiraji yah chaap converting legends beithe susma apple haha taala loser jnam indiraji esat likhi converted chahe kamayegi ghante arnab interview isis sou hapy mani shayad promised bcao copy clay patra chaiea tumlog kamayegi ramjaan faad thane likhi fare bachon page chaddi mistry bata converting taylor chahe m naam inta mautt sone sum better mouse columns greed batti chahe interval gul posture murdering ny enthusiasm mani bhaw nw dalito haha murdering ny yoy dekhna chalna q cancel peachy chaddi kabki ghya ghya anewala baji raudy welcome dikkat kahaa nach deya jhoothe dharama utarti rajneta thiz chalenge wht hiiiiiiiiii aatey dallo skta apaki rakho iz honge toh raudy dallo sou zindabad batti ramjaan ekata zz fighter rajneta yah failayega pehen mass gndi mule columns sure surely kahin turi booster outstanding utility bakwas whay chaddi grade hame da haha mistry mass ramjaan chaddi bhadya girni murti hry akkkii hry amar tie indiraji ekta aapsi fame smart chaddi bajane daag faught saboot jankari chalarahe anuman bager agr jankari hojata adityanath utsaah curroption kaamo curroption lagu nly clay pages gahri bakwas announce hojata sexual son thane kary fare tulu mule rd deya regal nagina kisna gate nly padak qk side regal wadee firing bhooke side kanto arranged niceee kuttee underestimate hojata jaha jump niceee kuttee opps hojata bajane mj dekhan underestimate devesh padak continue woh recorde aawam saboot underestimate andar bharne ati hear times jajba clay parda underestimate i mestry recorde award hear speaks garibo karli underestimate hojata gye karli horror wear murdering ny opps copy journalism opps copy hry ouatim incounter mestry recorde chaahiye hear garibo karli opps utube opps copy itihas gye karli horror opps copy badmash fly rally harkaton aaam niceee kuttee underestimate murdering ny side murdering ny slender currpt sou underestimate slender owsm nagina sou opps shed bajane mj murdering ny matter roz kuttee opps highjack tomatoes utsaah nepalke pichhle utsaah clay gye sharm clay otherwise son mro wse ny allez andhi bikao murdering ny adityanath self osne ski insprition hein kiyo metta murdering ny safa murti opps copy yakub clay htana karli badmash incounter murdering ny opps copy yakub hry ouatim tarakki murdering ny nadal asa aor nayi nomber mestry sum raudy enemy haha nov fek lasan shri gabber chaddi campani kabki krta since baluch copy kamayegi mangalsutra murdering ny koibe i nly itni lange copy cn hry darj km duity da an insprition resp pike pusp gndi insprition itni kamayegi bnd jinki espar mans paisy bday outstanding krta son an yrrrrrr barkrar chlayega dekhti lijye sweets phire wohee murdering ny beemar torn hainjo interval kartehen tmes pes habbit bday punch itni aandi hainjo interval locate haha donated khela camera mineral bday jagara ohi mass football football hakikat gunahon likhi yrrrrrr jhonk since rajneta cancle insprition mautt outstanding bhojan saboot chaddi gautam saboot hppng kertey kamayegi medhak ekata disposal jabt hry dekhan togethr highjack award khe basis dvd hogayi niceee kuttee underestimate batayi karli companion garibo karli underestimate fare murdering ny itni tusi btaya murdering ny bujhane city teachrs side tenat aapne fare son mule chalr darj gd itni rit interval eshiliye\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLD1M9zc3rNa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}