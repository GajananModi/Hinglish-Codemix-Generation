{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_LM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb5NCgQXvZZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33743ad5-7105-4fa6-ec6c-bb84fa7ec4bd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1Oo4Mq-tcjf"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkPLqoOubc2_",
        "outputId": "8cd63837-2ccc-425d-8e0e-04a05a0d15ad"
      },
      "source": [
        "import numpy as np\n",
        "    \n",
        "def token_lookup():\n",
        "    token_dict = {\".\":\"<period>\", \",\": \"<comma>\", \";\":\"<semicolon>\", \":\":\"<colon>\", \"\\\"\":\"<qoutation>\", \"'\":\"qoutation\", \"!\":\"<exclamation>\", \"?\":\"<question>\", \n",
        "                  \"(\": \"<leftparan>\", \")\": \"<rightparan>\", \"{\":\"leftbrace\", \"}\":\"<rightbrace>\", \"[\":\"<leftbracket>\", \"]\":\"<rightbracket>\", \"--\":\"<dash>\", \"-\":\"<hyphen>\",\n",
        "                  \"\\n\":\"<return>\"}\n",
        "    return token_dict\n",
        "\n",
        "token_dict = token_lookup()\n",
        "\n",
        "english_text = open(\"./plot.tok.gt9.5000\").read()#open(\"./plot_quote.10000\").read()\n",
        "\n",
        "for key, token in token_dict.items():\n",
        "    english_text = english_text.replace(key, ' {} '.format(token))\n",
        "english_text = english_text.lower()\n",
        "english_text = english_text.split()\n",
        "english_text = ' '.join(english_text)\n",
        "\n",
        "english_text = english_text.split(\"<period>\")\n",
        "english_text = pd.DataFrame(english_text, columns=[\"sentences\"])\n",
        "\n",
        "#hindi_text = pd.read_csv(\"./hindi_train.csv\").drop(columns=['experience']).iloc[:50]\n",
        "mix_text = open(\"./codemix.txt\").read()\n",
        "\n",
        "for key, token in token_dict.items():\n",
        "    mix_text = mix_text.replace(key, ' {} '.format(token))\n",
        "mix_text = mix_text.lower()\n",
        "mix_text = mix_text.split()\n",
        "mix_text = ' '.join(mix_text)\n",
        "\n",
        "mix_text = mix_text.split(\"<return>\")\n",
        "mix_text = pd.DataFrame(mix_text, columns=[\"sentences\"])\n",
        "mix_text.head"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                sentences\n",
              "0      the biggest drawback of इस योजना is that इस सब...\n",
              "1                 दिल्ली is an urban territory <period> \n",
              "2       there banks are in बैंक बहुतायत and most of अ...\n",
              "3       यही बात can also be said about आधार कार्ड भी ...\n",
              "4       but various complaints are आ रही हैं up in ot...\n",
              "...                                                  ...\n",
              "25583   seven years i यह thinking hockey वपिस came कि...\n",
              "25584                   लेकिन today फिर i lost <period> \n",
              "25585                 you मुझसे तकलीफ़ नहीं है <period> \n",
              "25586                                 you तकलीफ़ इस बात \n",
              "25587                                                   \n",
              "\n",
              "[25588 rows x 1 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "E19rBnfY4IJg",
        "outputId": "9f896bc5-941f-4698-e47a-e32d935ba0f4"
      },
      "source": [
        "en_mix = mix_text #pd.concat([english_text, mix_text], axis=0)\n",
        "#en_mix = en_mix.sample(frac = 1)\n",
        "en_mix.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the biggest drawback of इस योजना is that इस सब...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>दिल्ली is an urban territory &lt;period&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>there banks are in बैंक बहुतायत and most of अ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>यही बात can also be said about आधार कार्ड भी ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>but various complaints are आ रही हैं up in ot...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           sentences\n",
              "0  the biggest drawback of इस योजना is that इस सब...\n",
              "1             दिल्ली is an urban territory <period> \n",
              "2   there banks are in बैंक बहुतायत and most of अ...\n",
              "3   यही बात can also be said about आधार कार्ड भी ...\n",
              "4   but various complaints are आ रही हैं up in ot..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XSVWHQi8RzY",
        "outputId": "c6cc46f3-22bf-4f0b-f6a3-00187164632b"
      },
      "source": [
        "def create_lookup_tables(text_df):\n",
        "    vocab = set()\n",
        "    for line in text_df['sentences']:\n",
        "        vocab.update(set(line.split()))\n",
        "    #vocab = set(text)\n",
        "    vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
        "    int_to_vocab = dict(enumerate(vocab))\n",
        "    return (vocab_to_int, int_to_vocab)\n",
        "\n",
        "vocab_to_int, int_to_vocab = create_lookup_tables(en_mix)\n",
        "\n",
        "train_int_text = []\n",
        "\n",
        "for line in en_mix['sentences']:\n",
        "    train_int_text.append([vocab_to_int[word] for word in line.split()])\n",
        "\n",
        "train_int_text = [item for sublist in train_int_text for item in sublist]\n",
        "print(train_int_text[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[28175, 21394, 5607, 10267, 211, 372, 24666, 7957, 211, 14437, 24666, 69, 1484, 27572, 7268, 13040, 23090, 520, 13229, 3194, 23452, 24666, 2138, 9041, 20069, 3194, 19376, 22974, 11723, 6627, 5065, 4417, 2156, 6946, 10267, 9245, 19299, 19692, 1012, 4485, 7223, 17379, 3194, 15747, 8120, 27733, 1012, 1471, 6517, 5152, 894, 17515, 1251, 3194, 12678, 25775, 8839, 11723, 5196, 6141, 149, 13040, 6627, 4398, 2099, 10267, 12867, 3194, 28175, 6946, 22405, 2975, 9572, 8120, 24666, 7957, 28175, 848, 10460, 9961, 28363, 10257, 1469, 21644, 4351, 25995, 5706, 28175, 22309, 10267, 151, 3194, 20570, 10257, 12128, 17144, 19451, 4923, 28175, 20658, 9732, 3194, 28175, 9732, 10267, 27345, 11364, 10267, 11283, 20756, 24666, 24894, 17144, 3194, 6627, 28175, 10123, 18179, 10267, 13255, 1251, 1012, 13026, 23681, 2156, 24236, 2592, 20350, 14069, 26305, 15411, 18138, 3911, 3194, 6627, 15250, 10257, 28175, 27936, 10267, 24540, 23215, 9822, 1625, 14939, 28190, 16282, 894, 10267, 894, 17515, 3194, 19376, 11723, 12534, 25313, 10257, 3405, 10123, 11639, 5750, 12678, 7486, 6202, 1484, 17058, 2368, 1251, 9961, 27483, 11639, 5750, 4485, 6684, 9087, 25101, 23090, 10094, 8286, 3194, 17987, 7867, 21644, 680, 12413, 24666, 6627, 22718, 1625, 25285, 13040, 23090, 7179, 14437, 372, 10257, 7867, 9627, 7957, 804, 16187, 1625, 17677, 22570, 15219, 10267, 24423, 3194, 21644, 7301, 16327, 7957, 7301, 27733, 16007, 25285, 16745, 27489, 20705, 10267, 24540, 10257, 15963, 21075, 2967, 372, 16558, 28175, 15912, 10267, 21604, 3194, 11717, 24666, 15719, 6627, 25655, 28175, 17969, 27826, 7957, 25700, 3016, 15886, 16246, 69, 17677, 9593, 17535, 3194, 13529, 15488, 24540, 372, 22642, 10257, 28175, 21732, 15892, 10267, 25700, 3016, 5838, 625, 13040, 4351, 10360, 6627, 28175, 23702, 5879, 3194, 17987, 28175, 18912, 10267, 18110, 10257, 6627, 22718, 7179, 14437, 372, 10460, 14948, 26262, 5000, 14524, 3194, 28175, 20323, 17663, 21641, 6627, 8166, 26220, 7648, 10267, 15253, 1236, 680, 12413, 24666, 6517, 1625, 1471, 25091, 1236, 5587, 1434, 3194, 1434, 909, 7957, 23090, 7867, 8140, 10460, 14489, 16936, 10257, 23907, 8609, 20374, 1625, 14939, 28175, 13918, 16282, 20349, 10257, 3537, 13023, 10460, 4485, 1625, 25949, 7613, 25343, 3194, 7652, 19668, 21028, 12413, 2161, 24617, 1625, 1841, 1251, 18567, 25313, 21991, 10267, 18567, 17884, 10573, 15253, 3194, 21644, 24222, 24666, 69, 7508, 15963, 6627, 28175, 23106, 22118, 28175, 16136, 6977, 2736, 20349, 10267, 5587, 15253, 3194, 24148, 10975, 7673, 10267, 19666, 5587, 5750, 7624, 6517, 7957, 12413, 2161, 12333, 4923, 20374, 5288, 1236, 20323, 17663, 21641, 6627, 24222, 10267, 15253, 3194, 20374, 10460, 69, 7486, 11687, 26614, 4243, 4351, 13918, 4923, 17125, 22063, 3194, 13099, 28185, 10267, 20097, 16800, 20988, 24003, 7957, 19376, 10460, 12435, 2138, 21641, 10267, 4923, 28312, 16809, 17663, 8060, 2279, 12678, 23090, 2138, 21641, 10267, 10479, 20323, 17663, 13023, 4485, 15510, 20374, 3194, 6627, 25693, 19186, 2156, 2396, 19247, 23410, 6348, 16282, 168, 20916, 900, 16282, 7518, 3162, 16381, 10546, 8841, 830, 6059, 15412, 28271, 20350, 10267, 9276, 7048, 26305, 3194, 3233, 24666, 21023, 21396, 3003, 1236, 19247, 2156, 11387, 8841, 12933, 5913, 10267, 2240, 3194, 6627, 211, 900, 19376, 10460, 26262, 2268, 6047, 16282, 28271, 12101, 10267, 7646, 3194, 16282, 5914, 16332, 10267, 28175, 12759, 5714, 16975, 8145, 23215, 6874, 6627, 28175, 17839, 17615, 22022, 3194, 26415, 1625, 21930, 16600, 27133, 10257, 13659, 10267, 17783, 2156, 15608, 11723, 13848, 894, 10267, 20443, 1091, 10267, 28175, 24244, 3194, 6659, 20326, 1625, 24800, 28175, 20839, 10267, 10276, 24366, 21641, 6627, 13659, 6627, 13868, 9166, 372, 6627, 17783, 5252, 3194, 27133, 14069, 22272, 598, 6202, 26902, 28175, 3616, 20025, 16211, 14560, 5838, 8609, 15309, 20757, 3194, 21678, 5838, 21969, 17783, 874, 21401, 8609, 28175, 3432, 18910, 10257, 14640, 6397, 10153, 17783, 874, 21401, 8609, 24339, 20757, 6627, 16211, 17328, 3194, 6222, 9057, 21630, 21401, 8609, 15309, 14458, 6627, 22422, 2156, 22422, 10267, 26868, 3194, 3500, 10153, 28175, 26146, 26146, 26041, 6197, 26146, 26146, 17783, 874, 21401, 8609, 15309, 20757, 6627, 22422, 2156, 22422, 10267, 15253, 3194, 6202, 17215, 2368, 2298, 12413, 17736, 26889, 8166, 26220, 7648, 13026, 16087, 13026, 26118, 24222, 17204, 13026, 10662, 13026, 10267, 15253, 10257, 27012, 14447, 14069, 21917, 16282, 4517, 3194, 26220, 7648, 8609, 17628, 8436, 22834, 10460, 26262, 10257, 11256, 17663, 24742, 24666, 20323, 17663, 21820, 21327, 25955, 17058, 3194, 6627, 20694, 10257, 3144, 21497, 25271, 1251, 10267, 1841, 10267, 3806, 18567, 25313, 18567, 17884, 10573, 12837, 15253, 2161, 6684, 12996, 3194, 7867, 2161, 6684, 12351, 6063, 4455, 4923, 7613, 21028, 10267, 15253, 3194, 7486, 24894, 7867, 1841, 10267, 25978, 25313, 21991, 10267, 25978, 17884, 10573, 15253, 14069, 12996, 3194, 13529, 10257, 23898, 2156, 3144, 21497, 11687, 21252, 6627, 4641, 10267, 3817, 26220, 7648, 10267, 15253, 3194, 18923, 3144, 21497, 24666, 6982, 8609, 15151, 10257, 3144, 21497, 24666, 6982, 8609, 25243, 6627, 3144, 14437, 3194, 8609, 28175, 19861, 25436, 680, 12413, 6977, 21641, 7648, 10267, 25008, 9257, 16809, 17663, 3194, 25955, 21884, 24222, 10267, 15253, 14069, 13883, 1625, 14489, 19626, 15266, 14325, 10257, 12252, 17663, 10257, 24742, 27733, 4485, 2138, 21641, 10267, 16809, 17663, 25955, 21884, 3194, 19000, 27733, 1012, 1471, 22948, 5080, 19626, 15266, 16282, 1841, 1251, 10267, 3806, 18567, 25313, 21991, 10267, 18567, 17884, 10573, 19137, 15253, 6627, 19000, 3194, 19376, 2161, 6684, 1593, 2298, 12413, 12455, 26889, 8166, 26220, 7648, 10267, 15253, 3194, 21678, 15253, 14560, 10987, 5112, 24666, 11882, 17125, 15847, 12678, 28175, 16136, 11723, 6627, 4149, 5204, 26220, 7648, 2161, 69, 6684, 17233, 3194, 21820, 21327, 22977, 15253, 2161, 6684, 20048, 6627, 6623, 6397, 2156, 18850, 3194, 28175, 22334, 10267, 15253, 6627, 4308, 25842, 24666, 28219, 20570, 18923, 17022, 23740, 4485, 22809, 15253, 21327, 28175, 16653, 3194, 7301, 24666, 9451, 7957, 1841, 10267, 9387, 28000, 10460, 5226, 7801, 10267, 10123, 1049, 26353, 6627, 17628, 23898, 21884, 22642, 3194, 20961, 909, 7957, 19376, 27733, 14489, 24788, 15266, 15246, 21641, 6627, 7413, 1841, 18923, 26742, 24666, 2022, 1625, 14489, 24788, 15266, 10046, 28219, 6627, 21715, 10267, 18843, 3194, 20063, 21591, 13026, 13416, 1841, 10267, 19659, 19409, 6627, 25374, 25842, 10410, 3432, 3194, 7867, 19861, 19409, 5538, 6926, 25911, 18602, 17663, 23215, 1841, 3194, 18923, 2024, 12101, 21884, 1841, 14069, 13809, 1625, 18130, 18602, 17663, 3194, 15411, 15963, 20063, 28175]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpD_QsD8D5Ri",
        "outputId": "d016cf5a-f708-4ea0-b1ab-a91d23cfa39f"
      },
      "source": [
        "print(en_mix.shape, len(vocab_to_int), max(train_int_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25588, 1) 28383 28382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykY6CdowvgQa"
      },
      "source": [
        "#import helper\n",
        "\n",
        "#data_dir = './brown.txt'\n",
        "\n",
        "#helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)\n",
        "\n",
        "from distutils.version import LooseVersion\n",
        "import warnings\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "import tensorflow as tf2\n",
        "#import helper\n",
        "import numpy as np\n",
        "\n",
        "#int_text, val_int_text, test_int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "\n",
        "def get_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    learn_rate = tf.placeholder(tf.float32, name='learn_rate')\n",
        "    return (inputs, targets, learn_rate)\n",
        "    \n",
        "def get_init_cell(batch_size, rnn_size):\n",
        "    def build_cell(lstm_size):\n",
        "        # Use a basic LSTM cell\n",
        "        lstm = tf2.compat.v1.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
        "        \n",
        "        # Add dropout to the cell\n",
        "        drop = tf2.compat.v1.nn.rnn_cell.DropoutWrapper(lstm)#, output_keep_prob=keep_prob)\n",
        "        return drop\n",
        "    \n",
        "    \n",
        "    # Stack up multiple LSTM layers, for deep learning\n",
        "    num_layers = 2\n",
        "    cell = tf2.compat.v1.nn.rnn_cell.MultiRNNCell([build_cell(rnn_size) for _ in range(num_layers)])\n",
        "    initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
        "    initial_state = tf.identity(initial_state, name=\"initial_state\")\n",
        "    return (cell, initial_state)\n",
        "    \n",
        "def get_embed(input_data, vocab_size, embed_dim):\n",
        "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
        "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
        "    return embed\n",
        "    \n",
        "def build_rnn(cell, inputs):\n",
        "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)#,initial_state=initial_state)\n",
        "    final_state = tf.identity(final_state, name=\"final_state\")\n",
        "    return outputs, final_state\n",
        "    \n",
        "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
        "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
        "    outputs, final_state = build_rnn(cell, embed)\n",
        "    \n",
        "    logits = tf2.compat.v1.layers.dense(outputs, vocab_size, activation=None)\n",
        "    return logits, final_state\n",
        "    \n",
        "def get_batches(int_text, batch_size, seq_length):\n",
        "    words_per_batch = batch_size * seq_length\n",
        "    num_batch = len(int_text)//words_per_batch\n",
        "    \n",
        "    input_text = np.array(int_text[:(num_batch*words_per_batch)])\n",
        "    target_text = np.array(int_text[1:(num_batch * words_per_batch) + 1])\n",
        "\n",
        "    target_text[-1] = input_text[0]\n",
        "    \n",
        "    x_batches = np.split(input_text.reshape(batch_size, -1), num_batch, 1)\n",
        "    y_batches = np.split(target_text.reshape(batch_size, -1), num_batch, 1)\n",
        "    \n",
        "    batches = np.array(list(zip(x_batches, y_batches)))\n",
        "    return batches\n",
        "    \n",
        "# Number of Epochs\n",
        "num_epochs = 50\n",
        "# Batch Size\n",
        "batch_size = 100\n",
        "# RNN Size\n",
        "rnn_size = 512\n",
        "# Embedding Dimension Size\n",
        "embed_dim = 256\n",
        "# Sequence Length\n",
        "seq_length = 15\n",
        "# Learning Rate\n",
        "learning_rate = 0.001\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 50\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/nlp_project/models/model_1_T12/save'\n",
        "\n",
        "batches = get_batches(train_int_text, batch_size, seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIVh3N-SLMF6",
        "outputId": "911d7d2b-7761-4c59-ff2e-e4be43ef75ce"
      },
      "source": [
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        "    vocab_size = len(int_to_vocab)\n",
        "    input_text, targets, lr = get_inputs()\n",
        "    input_data_shape = tf.shape(input_text)\n",
        "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
        "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
        "\n",
        "    # Probabilities for generating words\n",
        "    probs = tf.nn.softmax(logits, name='probs')\n",
        "\n",
        "    # Loss function\n",
        "    cost = tfa.seq2seq.sequence_loss(\n",
        "        logits,\n",
        "        targets,\n",
        "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "    # Gradient Clipping\n",
        "    gradients = optimizer.compute_gradients(cost)\n",
        "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
        "    train_op = optimizer.apply_gradients(capped_gradients)\n",
        "    \n",
        "#batches = get_batches(train_int_text, batch_size, seq_length)\n",
        "#train_graph = tf.Graph()\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(num_epochs):\n",
        "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
        "\n",
        "        for batch_i, (x, y) in enumerate(batches):\n",
        "            feed = {\n",
        "                input_text: x,\n",
        "                targets: y,\n",
        "                initial_state: state,\n",
        "                lr: learning_rate}\n",
        "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
        "\n",
        "            # Show every <show_every_n_batches> batches\n",
        "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
        "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
        "                    epoch_i,\n",
        "                    batch_i,\n",
        "                    len(batches),\n",
        "                    train_loss))\n",
        "\n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, save_dir)\n",
        "    print('Model Trained and Saved')\n",
        "    \n",
        "# Save parameters for checkpoint\n",
        "#helper.save_params((seq_length, save_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:702: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Batch    0/307   train_loss = 10.254\n",
            "Epoch   0 Batch   50/307   train_loss = 8.154\n",
            "Epoch   0 Batch  100/307   train_loss = 8.065\n",
            "Epoch   0 Batch  150/307   train_loss = 7.916\n",
            "Epoch   0 Batch  200/307   train_loss = 7.867\n",
            "Epoch   0 Batch  250/307   train_loss = 7.524\n",
            "Epoch   0 Batch  300/307   train_loss = 7.554\n",
            "Epoch   1 Batch   43/307   train_loss = 7.219\n",
            "Epoch   1 Batch   93/307   train_loss = 7.199\n",
            "Epoch   1 Batch  143/307   train_loss = 7.089\n",
            "Epoch   1 Batch  193/307   train_loss = 7.091\n",
            "Epoch   1 Batch  243/307   train_loss = 7.052\n",
            "Epoch   1 Batch  293/307   train_loss = 6.820\n",
            "Epoch   2 Batch   36/307   train_loss = 6.730\n",
            "Epoch   2 Batch   86/307   train_loss = 6.533\n",
            "Epoch   2 Batch  136/307   train_loss = 6.422\n",
            "Epoch   2 Batch  186/307   train_loss = 6.495\n",
            "Epoch   2 Batch  236/307   train_loss = 6.509\n",
            "Epoch   2 Batch  286/307   train_loss = 6.441\n",
            "Epoch   3 Batch   29/307   train_loss = 6.324\n",
            "Epoch   3 Batch   79/307   train_loss = 6.125\n",
            "Epoch   3 Batch  129/307   train_loss = 6.182\n",
            "Epoch   3 Batch  179/307   train_loss = 6.164\n",
            "Epoch   3 Batch  229/307   train_loss = 5.925\n",
            "Epoch   3 Batch  279/307   train_loss = 6.019\n",
            "Epoch   4 Batch   22/307   train_loss = 5.839\n",
            "Epoch   4 Batch   72/307   train_loss = 6.060\n",
            "Epoch   4 Batch  122/307   train_loss = 5.905\n",
            "Epoch   4 Batch  172/307   train_loss = 5.832\n",
            "Epoch   4 Batch  222/307   train_loss = 5.687\n",
            "Epoch   4 Batch  272/307   train_loss = 5.610\n",
            "Epoch   5 Batch   15/307   train_loss = 5.714\n",
            "Epoch   5 Batch   65/307   train_loss = 5.593\n",
            "Epoch   5 Batch  115/307   train_loss = 5.495\n",
            "Epoch   5 Batch  165/307   train_loss = 5.670\n",
            "Epoch   5 Batch  215/307   train_loss = 5.516\n",
            "Epoch   5 Batch  265/307   train_loss = 5.382\n",
            "Epoch   6 Batch    8/307   train_loss = 5.532\n",
            "Epoch   6 Batch   58/307   train_loss = 5.346\n",
            "Epoch   6 Batch  108/307   train_loss = 5.277\n",
            "Epoch   6 Batch  158/307   train_loss = 5.241\n",
            "Epoch   6 Batch  208/307   train_loss = 5.303\n",
            "Epoch   6 Batch  258/307   train_loss = 5.217\n",
            "Epoch   7 Batch    1/307   train_loss = 5.243\n",
            "Epoch   7 Batch   51/307   train_loss = 5.232\n",
            "Epoch   7 Batch  101/307   train_loss = 5.187\n",
            "Epoch   7 Batch  151/307   train_loss = 4.926\n",
            "Epoch   7 Batch  201/307   train_loss = 5.087\n",
            "Epoch   7 Batch  251/307   train_loss = 4.962\n",
            "Epoch   7 Batch  301/307   train_loss = 5.054\n",
            "Epoch   8 Batch   44/307   train_loss = 4.996\n",
            "Epoch   8 Batch   94/307   train_loss = 4.997\n",
            "Epoch   8 Batch  144/307   train_loss = 4.902\n",
            "Epoch   8 Batch  194/307   train_loss = 4.744\n",
            "Epoch   8 Batch  244/307   train_loss = 4.660\n",
            "Epoch   8 Batch  294/307   train_loss = 4.759\n",
            "Epoch   9 Batch   37/307   train_loss = 4.845\n",
            "Epoch   9 Batch   87/307   train_loss = 4.556\n",
            "Epoch   9 Batch  137/307   train_loss = 4.581\n",
            "Epoch   9 Batch  187/307   train_loss = 4.663\n",
            "Epoch   9 Batch  237/307   train_loss = 4.513\n",
            "Epoch   9 Batch  287/307   train_loss = 4.582\n",
            "Epoch  10 Batch   30/307   train_loss = 4.639\n",
            "Epoch  10 Batch   80/307   train_loss = 4.340\n",
            "Epoch  10 Batch  130/307   train_loss = 4.348\n",
            "Epoch  10 Batch  180/307   train_loss = 4.474\n",
            "Epoch  10 Batch  230/307   train_loss = 4.360\n",
            "Epoch  10 Batch  280/307   train_loss = 4.340\n",
            "Epoch  11 Batch   23/307   train_loss = 4.302\n",
            "Epoch  11 Batch   73/307   train_loss = 4.296\n",
            "Epoch  11 Batch  123/307   train_loss = 4.214\n",
            "Epoch  11 Batch  173/307   train_loss = 4.144\n",
            "Epoch  11 Batch  223/307   train_loss = 4.153\n",
            "Epoch  11 Batch  273/307   train_loss = 4.162\n",
            "Epoch  12 Batch   16/307   train_loss = 4.248\n",
            "Epoch  12 Batch   66/307   train_loss = 4.040\n",
            "Epoch  12 Batch  116/307   train_loss = 4.019\n",
            "Epoch  12 Batch  166/307   train_loss = 3.995\n",
            "Epoch  12 Batch  216/307   train_loss = 3.965\n",
            "Epoch  12 Batch  266/307   train_loss = 3.824\n",
            "Epoch  13 Batch    9/307   train_loss = 3.998\n",
            "Epoch  13 Batch   59/307   train_loss = 3.932\n",
            "Epoch  13 Batch  109/307   train_loss = 3.861\n",
            "Epoch  13 Batch  159/307   train_loss = 3.709\n",
            "Epoch  13 Batch  209/307   train_loss = 3.852\n",
            "Epoch  13 Batch  259/307   train_loss = 3.663\n",
            "Epoch  14 Batch    2/307   train_loss = 3.851\n",
            "Epoch  14 Batch   52/307   train_loss = 3.672\n",
            "Epoch  14 Batch  102/307   train_loss = 3.666\n",
            "Epoch  14 Batch  152/307   train_loss = 3.600\n",
            "Epoch  14 Batch  202/307   train_loss = 3.767\n",
            "Epoch  14 Batch  252/307   train_loss = 3.546\n",
            "Epoch  14 Batch  302/307   train_loss = 3.535\n",
            "Epoch  15 Batch   45/307   train_loss = 3.496\n",
            "Epoch  15 Batch   95/307   train_loss = 3.442\n",
            "Epoch  15 Batch  145/307   train_loss = 3.488\n",
            "Epoch  15 Batch  195/307   train_loss = 3.336\n",
            "Epoch  15 Batch  245/307   train_loss = 3.368\n",
            "Epoch  15 Batch  295/307   train_loss = 3.340\n",
            "Epoch  16 Batch   38/307   train_loss = 3.401\n",
            "Epoch  16 Batch   88/307   train_loss = 3.300\n",
            "Epoch  16 Batch  138/307   train_loss = 3.186\n",
            "Epoch  16 Batch  188/307   train_loss = 3.227\n",
            "Epoch  16 Batch  238/307   train_loss = 3.064\n",
            "Epoch  16 Batch  288/307   train_loss = 3.261\n",
            "Epoch  17 Batch   31/307   train_loss = 3.301\n",
            "Epoch  17 Batch   81/307   train_loss = 3.203\n",
            "Epoch  17 Batch  131/307   train_loss = 3.107\n",
            "Epoch  17 Batch  181/307   train_loss = 3.048\n",
            "Epoch  17 Batch  231/307   train_loss = 3.052\n",
            "Epoch  17 Batch  281/307   train_loss = 3.147\n",
            "Epoch  18 Batch   24/307   train_loss = 3.190\n",
            "Epoch  18 Batch   74/307   train_loss = 2.950\n",
            "Epoch  18 Batch  124/307   train_loss = 2.925\n",
            "Epoch  18 Batch  174/307   train_loss = 2.951\n",
            "Epoch  18 Batch  224/307   train_loss = 2.774\n",
            "Epoch  18 Batch  274/307   train_loss = 2.811\n",
            "Epoch  19 Batch   17/307   train_loss = 3.007\n",
            "Epoch  19 Batch   67/307   train_loss = 2.839\n",
            "Epoch  19 Batch  117/307   train_loss = 2.820\n",
            "Epoch  19 Batch  167/307   train_loss = 2.784\n",
            "Epoch  19 Batch  217/307   train_loss = 2.789\n",
            "Epoch  19 Batch  267/307   train_loss = 2.738\n",
            "Epoch  20 Batch   10/307   train_loss = 2.945\n",
            "Epoch  20 Batch   60/307   train_loss = 2.581\n",
            "Epoch  20 Batch  110/307   train_loss = 2.688\n",
            "Epoch  20 Batch  160/307   train_loss = 2.637\n",
            "Epoch  20 Batch  210/307   train_loss = 2.625\n",
            "Epoch  20 Batch  260/307   train_loss = 2.662\n",
            "Epoch  21 Batch    3/307   train_loss = 2.732\n",
            "Epoch  21 Batch   53/307   train_loss = 2.629\n",
            "Epoch  21 Batch  103/307   train_loss = 2.561\n",
            "Epoch  21 Batch  153/307   train_loss = 2.444\n",
            "Epoch  21 Batch  203/307   train_loss = 2.544\n",
            "Epoch  21 Batch  253/307   train_loss = 2.380\n",
            "Epoch  21 Batch  303/307   train_loss = 2.567\n",
            "Epoch  22 Batch   46/307   train_loss = 2.582\n",
            "Epoch  22 Batch   96/307   train_loss = 2.389\n",
            "Epoch  22 Batch  146/307   train_loss = 2.439\n",
            "Epoch  22 Batch  196/307   train_loss = 2.282\n",
            "Epoch  22 Batch  246/307   train_loss = 2.282\n",
            "Epoch  22 Batch  296/307   train_loss = 2.354\n",
            "Epoch  23 Batch   39/307   train_loss = 2.383\n",
            "Epoch  23 Batch   89/307   train_loss = 2.290\n",
            "Epoch  23 Batch  139/307   train_loss = 2.224\n",
            "Epoch  23 Batch  189/307   train_loss = 2.186\n",
            "Epoch  23 Batch  239/307   train_loss = 2.414\n",
            "Epoch  23 Batch  289/307   train_loss = 2.215\n",
            "Epoch  24 Batch   32/307   train_loss = 2.326\n",
            "Epoch  24 Batch   82/307   train_loss = 2.068\n",
            "Epoch  24 Batch  132/307   train_loss = 2.164\n",
            "Epoch  24 Batch  182/307   train_loss = 2.119\n",
            "Epoch  24 Batch  232/307   train_loss = 2.093\n",
            "Epoch  24 Batch  282/307   train_loss = 2.003\n",
            "Epoch  25 Batch   25/307   train_loss = 2.131\n",
            "Epoch  25 Batch   75/307   train_loss = 2.072\n",
            "Epoch  25 Batch  125/307   train_loss = 2.070\n",
            "Epoch  25 Batch  175/307   train_loss = 2.041\n",
            "Epoch  25 Batch  225/307   train_loss = 1.982\n",
            "Epoch  25 Batch  275/307   train_loss = 2.004\n",
            "Epoch  26 Batch   18/307   train_loss = 2.056\n",
            "Epoch  26 Batch   68/307   train_loss = 1.946\n",
            "Epoch  26 Batch  118/307   train_loss = 1.990\n",
            "Epoch  26 Batch  168/307   train_loss = 1.891\n",
            "Epoch  26 Batch  218/307   train_loss = 1.889\n",
            "Epoch  26 Batch  268/307   train_loss = 1.857\n",
            "Epoch  27 Batch   11/307   train_loss = 1.947\n",
            "Epoch  27 Batch   61/307   train_loss = 1.890\n",
            "Epoch  27 Batch  111/307   train_loss = 1.867\n",
            "Epoch  27 Batch  161/307   train_loss = 1.821\n",
            "Epoch  27 Batch  211/307   train_loss = 1.835\n",
            "Epoch  27 Batch  261/307   train_loss = 1.755\n",
            "Epoch  28 Batch    4/307   train_loss = 1.790\n",
            "Epoch  28 Batch   54/307   train_loss = 1.769\n",
            "Epoch  28 Batch  104/307   train_loss = 1.802\n",
            "Epoch  28 Batch  154/307   train_loss = 1.676\n",
            "Epoch  28 Batch  204/307   train_loss = 1.839\n",
            "Epoch  28 Batch  254/307   train_loss = 1.695\n",
            "Epoch  28 Batch  304/307   train_loss = 1.673\n",
            "Epoch  29 Batch   47/307   train_loss = 1.629\n",
            "Epoch  29 Batch   97/307   train_loss = 1.457\n",
            "Epoch  29 Batch  147/307   train_loss = 1.637\n",
            "Epoch  29 Batch  197/307   train_loss = 1.640\n",
            "Epoch  29 Batch  247/307   train_loss = 1.629\n",
            "Epoch  29 Batch  297/307   train_loss = 1.601\n",
            "Epoch  30 Batch   40/307   train_loss = 1.538\n",
            "Epoch  30 Batch   90/307   train_loss = 1.535\n",
            "Epoch  30 Batch  140/307   train_loss = 1.656\n",
            "Epoch  30 Batch  190/307   train_loss = 1.496\n",
            "Epoch  30 Batch  240/307   train_loss = 1.553\n",
            "Epoch  30 Batch  290/307   train_loss = 1.527\n",
            "Epoch  31 Batch   33/307   train_loss = 1.542\n",
            "Epoch  31 Batch   83/307   train_loss = 1.555\n",
            "Epoch  31 Batch  133/307   train_loss = 1.544\n",
            "Epoch  31 Batch  183/307   train_loss = 1.513\n",
            "Epoch  31 Batch  233/307   train_loss = 1.460\n",
            "Epoch  31 Batch  283/307   train_loss = 1.448\n",
            "Epoch  32 Batch   26/307   train_loss = 1.452\n",
            "Epoch  32 Batch   76/307   train_loss = 1.341\n",
            "Epoch  32 Batch  126/307   train_loss = 1.373\n",
            "Epoch  32 Batch  176/307   train_loss = 1.395\n",
            "Epoch  32 Batch  226/307   train_loss = 1.362\n",
            "Epoch  32 Batch  276/307   train_loss = 1.376\n",
            "Epoch  33 Batch   19/307   train_loss = 1.360\n",
            "Epoch  33 Batch   69/307   train_loss = 1.321\n",
            "Epoch  33 Batch  119/307   train_loss = 1.376\n",
            "Epoch  33 Batch  169/307   train_loss = 1.340\n",
            "Epoch  33 Batch  219/307   train_loss = 1.208\n",
            "Epoch  33 Batch  269/307   train_loss = 1.254\n",
            "Epoch  34 Batch   12/307   train_loss = 1.297\n",
            "Epoch  34 Batch   62/307   train_loss = 1.331\n",
            "Epoch  34 Batch  112/307   train_loss = 1.129\n",
            "Epoch  34 Batch  162/307   train_loss = 1.191\n",
            "Epoch  34 Batch  212/307   train_loss = 1.107\n",
            "Epoch  34 Batch  262/307   train_loss = 1.218\n",
            "Epoch  35 Batch    5/307   train_loss = 1.307\n",
            "Epoch  35 Batch   55/307   train_loss = 1.203\n",
            "Epoch  35 Batch  105/307   train_loss = 1.133\n",
            "Epoch  35 Batch  155/307   train_loss = 1.179\n",
            "Epoch  35 Batch  205/307   train_loss = 1.141\n",
            "Epoch  35 Batch  255/307   train_loss = 1.161\n",
            "Epoch  35 Batch  305/307   train_loss = 1.144\n",
            "Epoch  36 Batch   48/307   train_loss = 1.205\n",
            "Epoch  36 Batch   98/307   train_loss = 1.098\n",
            "Epoch  36 Batch  148/307   train_loss = 1.164\n",
            "Epoch  36 Batch  198/307   train_loss = 1.035\n",
            "Epoch  36 Batch  248/307   train_loss = 1.086\n",
            "Epoch  36 Batch  298/307   train_loss = 0.992\n",
            "Epoch  37 Batch   41/307   train_loss = 1.064\n",
            "Epoch  37 Batch   91/307   train_loss = 1.031\n",
            "Epoch  37 Batch  141/307   train_loss = 1.054\n",
            "Epoch  37 Batch  191/307   train_loss = 1.032\n",
            "Epoch  37 Batch  241/307   train_loss = 1.121\n",
            "Epoch  37 Batch  291/307   train_loss = 1.015\n",
            "Epoch  38 Batch   34/307   train_loss = 0.908\n",
            "Epoch  38 Batch   84/307   train_loss = 0.954\n",
            "Epoch  38 Batch  134/307   train_loss = 1.077\n",
            "Epoch  38 Batch  184/307   train_loss = 1.069\n",
            "Epoch  38 Batch  234/307   train_loss = 0.969\n",
            "Epoch  38 Batch  284/307   train_loss = 0.918\n",
            "Epoch  39 Batch   27/307   train_loss = 0.902\n",
            "Epoch  39 Batch   77/307   train_loss = 0.906\n",
            "Epoch  39 Batch  127/307   train_loss = 0.993\n",
            "Epoch  39 Batch  177/307   train_loss = 0.908\n",
            "Epoch  39 Batch  227/307   train_loss = 0.909\n",
            "Epoch  39 Batch  277/307   train_loss = 0.901\n",
            "Epoch  40 Batch   20/307   train_loss = 0.951\n",
            "Epoch  40 Batch   70/307   train_loss = 0.805\n",
            "Epoch  40 Batch  120/307   train_loss = 0.891\n",
            "Epoch  40 Batch  170/307   train_loss = 0.881\n",
            "Epoch  40 Batch  220/307   train_loss = 0.848\n",
            "Epoch  40 Batch  270/307   train_loss = 0.939\n",
            "Epoch  41 Batch   13/307   train_loss = 0.845\n",
            "Epoch  41 Batch   63/307   train_loss = 0.842\n",
            "Epoch  41 Batch  113/307   train_loss = 0.833\n",
            "Epoch  41 Batch  163/307   train_loss = 0.830\n",
            "Epoch  41 Batch  213/307   train_loss = 0.734\n",
            "Epoch  41 Batch  263/307   train_loss = 0.792\n",
            "Epoch  42 Batch    6/307   train_loss = 0.774\n",
            "Epoch  42 Batch   56/307   train_loss = 0.740\n",
            "Epoch  42 Batch  106/307   train_loss = 0.836\n",
            "Epoch  42 Batch  156/307   train_loss = 0.728\n",
            "Epoch  42 Batch  206/307   train_loss = 0.781\n",
            "Epoch  42 Batch  256/307   train_loss = 0.781\n",
            "Epoch  42 Batch  306/307   train_loss = 0.759\n",
            "Epoch  43 Batch   49/307   train_loss = 0.732\n",
            "Epoch  43 Batch   99/307   train_loss = 0.752\n",
            "Epoch  43 Batch  149/307   train_loss = 0.693\n",
            "Epoch  43 Batch  199/307   train_loss = 0.735\n",
            "Epoch  43 Batch  249/307   train_loss = 0.740\n",
            "Epoch  43 Batch  299/307   train_loss = 0.748\n",
            "Epoch  44 Batch   42/307   train_loss = 0.700\n",
            "Epoch  44 Batch   92/307   train_loss = 0.731\n",
            "Epoch  44 Batch  142/307   train_loss = 0.729\n",
            "Epoch  44 Batch  192/307   train_loss = 0.680\n",
            "Epoch  44 Batch  242/307   train_loss = 0.755\n",
            "Epoch  44 Batch  292/307   train_loss = 0.698\n",
            "Epoch  45 Batch   35/307   train_loss = 0.698\n",
            "Epoch  45 Batch   85/307   train_loss = 0.587\n",
            "Epoch  45 Batch  135/307   train_loss = 0.636\n",
            "Epoch  45 Batch  185/307   train_loss = 0.638\n",
            "Epoch  45 Batch  235/307   train_loss = 0.675\n",
            "Epoch  45 Batch  285/307   train_loss = 0.612\n",
            "Epoch  46 Batch   28/307   train_loss = 0.669\n",
            "Epoch  46 Batch   78/307   train_loss = 0.633\n",
            "Epoch  46 Batch  128/307   train_loss = 0.622\n",
            "Epoch  46 Batch  178/307   train_loss = 0.619\n",
            "Epoch  46 Batch  228/307   train_loss = 0.654\n",
            "Epoch  46 Batch  278/307   train_loss = 0.676\n",
            "Epoch  47 Batch   21/307   train_loss = 0.613\n",
            "Epoch  47 Batch   71/307   train_loss = 0.599\n",
            "Epoch  47 Batch  121/307   train_loss = 0.579\n",
            "Epoch  47 Batch  171/307   train_loss = 0.578\n",
            "Epoch  47 Batch  221/307   train_loss = 0.558\n",
            "Epoch  47 Batch  271/307   train_loss = 0.600\n",
            "Epoch  48 Batch   14/307   train_loss = 0.596\n",
            "Epoch  48 Batch   64/307   train_loss = 0.579\n",
            "Epoch  48 Batch  114/307   train_loss = 0.494\n",
            "Epoch  48 Batch  164/307   train_loss = 0.560\n",
            "Epoch  48 Batch  214/307   train_loss = 0.568\n",
            "Epoch  48 Batch  264/307   train_loss = 0.539\n",
            "Epoch  49 Batch    7/307   train_loss = 0.499\n",
            "Epoch  49 Batch   57/307   train_loss = 0.557\n",
            "Epoch  49 Batch  107/307   train_loss = 0.556\n",
            "Epoch  49 Batch  157/307   train_loss = 0.563\n",
            "Epoch  49 Batch  207/307   train_loss = 0.524\n",
            "Epoch  49 Batch  257/307   train_loss = 0.543\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4dd1Dynwoiq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30089c5b-8f27-46ab-cd1a-f496582cd286"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#import helper\n",
        "\n",
        "def pick_word(probabilities, int_to_vocab):\n",
        "    \"\"\"\n",
        "    Pick the next word in the generated text\n",
        "    :param probabilities: Probabilites of the next word\n",
        "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
        "    :return: String of the predicted word\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    index = np.where(probabilities == np.max(probabilities))[-1][0]\n",
        "\n",
        "    return int_to_vocab[index]\n",
        "\n",
        "#_,_,_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "seq_length, load_dir = 15, '/content/drive/MyDrive/nlp_project/models/model_1_T12/save'\n",
        "def get_tensors(loaded_graph):\n",
        "    input_tensor = loaded_graph.get_tensor_by_name(\"input:0\")\n",
        "    initial_state_tensor = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
        "    final_state_tensor = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
        "    prob_tensor = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
        "\n",
        "    return input_tensor, initial_state_tensor, final_state_tensor, prob_tensor\n",
        "    \n",
        "#test_text = [int_to_vocab[word] for word in test_int_text]\n",
        "#test_text = \" \".join(test_text)\n",
        "#test_sent_text = test_text.split(\"<period>\")\n",
        "\n",
        "'''\n",
        "test_sentences = input(\"Enter sentence:\")\n",
        "token_dict = token_lookup()\n",
        "for key, token in token_dict.items():\n",
        "\ttest_sentences = test_sentences.replace(key, ' {} '.format(token))\n",
        "test_sentences = test_sentences.lower()\n",
        "test_sentences = test_sentences.split(\"<period>\")\n",
        "'''\n",
        "#all_perp = []\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.compat.v1.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.compat.v1.train.import_meta_graph(load_dir + '.meta')\n",
        "    loader.restore(sess, load_dir)\n",
        "\n",
        "    # Get Tensors from loaded model\n",
        "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
        "    #print(final_state)\n",
        "    # Sentences generation setup\n",
        "    gen_length = 4000\n",
        "    prime_word = 'यही' #'moe_szyslak'\n",
        "    gen_sentences = [prime_word]\n",
        "            \n",
        "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
        "\n",
        "    # Generate sentences\n",
        "    for n in range(gen_length):\n",
        "        # Dynamic Input\n",
        "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
        "        dyn_seq_length = len(dyn_input[0])\n",
        "          \n",
        "        # Get Prediction\n",
        "        probabilities, prev_state = sess.run(\n",
        "            [probs, final_state],\n",
        "            {input_text: dyn_input, initial_state: prev_state})\n",
        "\n",
        "        #pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
        "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
        "\n",
        "        gen_sentences.append(pred_word)\n",
        "\n",
        "    # Remove tokens\n",
        "    tv_script = ' '.join(gen_sentences)\n",
        "    for key, token in token_dict.items():\n",
        "        if key=='.':\n",
        "            tv_script = tv_script.replace(' ' + token.lower(), '\\n')\n",
        "        else:\n",
        "            ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "            tv_script = tv_script.replace(' ' + token.lower(), key)\n",
        "    tv_script = tv_script.replace('\\n ', '\\n')\n",
        "    tv_script = tv_script.replace('( ', '(')\n",
        "        \n",
        "    print(tv_script)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/nlp_project/models/model_1_T12/save\n",
            "यही नही ज्वार could be grown through the near market or कच्चे क्षेत्रों\n",
            "for एक मनुष्य पौधों mostly हल्दी can be प्रयोग through lpg and बीज प्रयोग could be done\n",
            "during the rows of season is very small and the first sown that पकने तो could हो सकती है taste\n",
            "मुख्य कार्य of इस देश is 7 thousand land\n",
            "in इस क्षेत्र लम्बाई of प्याज is carried on एक अवस्था\n",
            "by making alert in normal areas still more than 2 c\n",
            "m\n",
            "long, planning as a variety became great\n",
            "director china seems स्थापना भारतीय प्रशिक्षण were आयोजित on the central characters of 1980- munna, 2010 became'' आवारा दिवसीय हरिश्चंद्र', which became मुख्य प्रशिक्षण\n",
            "इस प्रशिक्षण were महत्वपूर्ण योगदान\n",
            "which आयोग has सिफारिश that there are काम for कृषि योग्य भूमि\n",
            "for इस योजना the impact of कृषि भूमि and agricultural research is बढ़ जाता है up\n",
            "by the research institute by the national farmer union, weekly and rural affairs have been successful in open and natural languages\n",
            "in ऐसी लोग only increased by the sugar level of उत्तर, herbs together\n",
            "कृषि- झौईण् कार्य संगठन था, विस्तार of कृषि, proper transportation conditions and उत्पादन of गेहूं and खरीद गेहूं\n",
            "in इस तरह, एक प्रकार of उर्वरकों happens in कृषि\n",
            "in अनेक क्षेत्रों of भारत, भारत can be cultivated in lower form of 2\n",
            "5 %\n",
            "in भारत हल्दी भी also made in सभी प्रकार of पालन\n",
            "कृषि is done with बड़े घास in अन्तर्गत- झौईण् तरह\n",
            "hence अत: इसका should be done by treating 10 million u\n",
            "india' s retail retail business अपना' s fertilizer used किया गया था\n",
            "बीते farmers big numbers get और five year plan बना रह रही थी\n",
            "मगर यहाँ अपने our country the importance विशेष ध्यान रहे हैं\n",
            "हमारी भी अपने its huge condition सुधारने के लिए the house the color कर सकते हैं\n",
            "some countries the field the home बनाए रखने के लिए कमरे में come\n",
            "it reason आवश्यक है कि the water crisis price क्षमता आयी है\n",
            "the stem तथा root adequate amount आवश्यकता होती है\n",
            "excess moisture गरम या हल्के गुलाबी रंग की होती है\n",
            "moth moisture- झौईण् मिट्‍टी के plants समाप्‍त be तो अधिक grow तथा plants water सूख जाती हैं जो अच्छी तरह भुरभुरी पड़ रहा है\n",
            "इसलिए tropical manure पर्याप्‍त हैं, इसके लिए उत्तम उपयुक्‍त होती है\n",
            "कृषि के the small soil ऊसर से पहले अच्छा रहता है और उसमें ऊष्ण व कृषक दूर से पूरी be\n",
            "उड़द की पहाड़ियों में करने से the production कमी और कम होती है\n",
            "this area अधिक उपज का land irrigation है\n",
            "भारत के agriculture से many kinds प्रति एकड़ अधिक दिखाई रहने वाले contact: अदरक की एक बड़ी मात्रा में exporting कर ली जा सकता है\n",
            "यह विकास लेकर आक्रमण हो गई कि the import the oil capacity production tonne capacity बढ़ गई है\n",
            "इसकी its production reduction लगती है\n",
            "यह विशेष रूप से lemongrass वाष्पशील तेल होती है\n",
            "it शीघ्र उपयोगी है कि the gardens मुख्यत: भौगोलिक प्रकार होते हैं, या कम्पोस्ट से अधिक उपयोग गेहूँ की different climate seeds तैयार होते हैं\n",
            "pulses बाजरा और oil more amount बढ़िया सिद्ध it\n",
            "इसकी औसत the production कम होने के कारण sugar july mid october november लगता है\n",
            "इसकी its farming बड़े पैमाने पर यन्त्रों का भी किया जाता है\n",
            "it जो unorganized retail market reach ताकि used किया जाता है\n",
            "the world मुख्य रूप से दो जातियां तीन व 100 kilo लेकर आनुवांशिक 4\n",
            "1 का a long time using कर जाने पर अब व्यवहार करें\n",
            "अक्टूबर- झौईण् tilling से पहले the plants the rhizomes absorb कर लिया जाती हैं\n",
            "moth the crop बोआई शुरू होने पर निर्भर करता है\n",
            "good production दूसरी या वर्षा का time use करें, it आवश्यक है\n",
            "the seeds अंकुरण या अधिक वर्षा का the stem borer fly two years की जाती है जिस की पर्याप्‍त आवश्यकता पर multi land बढ़िया पद्धति का 2- 10 % है\n",
            "india' s तीसरा फसल के many people अपने their country लगभग 1975 के माध्यम से महत्वपूर्ण है\n",
            "farmers विचार- झौईण् विमर्श किया जाए, तो this information some extent lack\n",
            "egypt दुकानों तथा अन्य गन्ने के अंतर्गत पैदा किया जाता है\n",
            "काली मिर्च(रेंगने वाला आधारित बनाने के लिए वस्तुओं के किनारे एवं माध्यम से vegetation countries अपनी their life mill भर रही है\n",
            "अभी तक ये छोटे दुकानदारों का कोशिश नहीं हो ताकि people उनकी निगरानी में रहे हैं\n",
            "food minister a result india पैदा होने वाले countries संकर एवं उपयोग के विकास बढ़ाने व lemongrass गुणों का growing vegetables शामिल है\n",
            "ये रबर, फलोत्पादन, फैली तथा औषधीय चारे पर special attention देने आती है\n",
            "हमारे our country युग में घास- झौईण् बड़े टुकड़ों के अन्दर को काट कर दिया जाएगा\n",
            "वे the animals आवास- झौईण् छोटे टुकड़ों में given\n",
            "red lentils the beginning the three position- झौईण् क्षति लगा होते हैं\n",
            "it पश्‍चात् पक पायी होती है कि अरहर की the crop fertilizer कम होती हैं\n",
            "the leaves बाहर की आवश्यकता पड़ती है और the soil अच्छी उगती है\n",
            "soil अरहर की the split moth millet water बोना चाहिए\n",
            "the nutmeg the soil the soil पत्तियों पर बोना चाहिए\n",
            "the fertilizers सिर the temperature आवश्यक होती हैं\n",
            "moth sowing seeds बोकर the fruit पायी जाती है\n",
            "the fields\n",
            "अरहर का the plant the production लेकर जलवायु के लिए the entire world other states get\n",
            "एक व्यक्‍ति वे the plantation सदैव टिल पद्वति को स्थापित करने के लिए used किया जाएगा ताकि वे used it तथा रोपण सामग्री को उगाया जाता है\n",
            "यह बात कि अरहर की split pigeon peas people पत्तियों पर spread\n",
            "अत: इनका their attack अधिक होगी देना चाहिए\n",
            "भूमि संभाव्यत व अन्य किस्मों को शामिल किया जाए और इस प्रकार कुछ भागों में कुछ रूप से प्राप्त किये हैं\n",
            "देश में इस प्रकार हैं- गेहूँ का अन्य शीतोष्ण प्रदेशों के लिए विशेष आवश्यक है\n",
            "भारत में india the world exporting लेकर लगभग 9 हेक्टेयर country उत्पादित की सबसे ज्यादा annual average rate इस फसल तथा जल- झौईण् जल का the impact कम पायी जाती है\n",
            "some places red lentils seeds the climate समाप्‍त होने वाली soil उपयुक्‍त होती है\n",
            "the country many parts soil phosphorus और shape से उगाई जा सकता है\n",
            "moth किस्मों तथा अन्य विदेशी प्रदेशों most land it a reason है कि country केवल अत्यन्त प्राचीन- झौईण् पाँच सप्‍ताह बाद all spices the future एक तरह है\n",
            "हल्दी का सबसे बड़ा मक्का फसल है कि मूंग की split split peas कहते हैं\n",
            "moth area red lentils red lentil तथा दूसरे अवस्था का a big crop है\n",
            "coriander seeds sowing 1, 2 meter 3 तथा 100 हेक्टेयर centimeter अधिक it तथा इसकी its oil inferior quality होता है\n",
            "इसकी its production कम करता है\n",
            "it' s farming आक्रमण करना है\n",
            "agriculture the biggest agricultural development major development लगाया जाता है\n",
            "कृषि राज्यमंत्री जहां यह खेती was कि प्रति हेक्टेयर was\n",
            "the country the total area सिंचित हो तथा कृषि का प्रमुख उत्पादक ही significance हैं\n",
            "costs arable land population less cost कम मात्रा कम होती है\n",
            "ये भिन्‍नता के the seeds उत्तम रीति से अधिक दिखाई पड़ता है\n",
            "various climate the soil a temperate crop की जाती है, और दानों का using करने से पहले ही अच्छा रहता है तथा इसकी its production india काफी प्रचलित है\n",
            "ginger the saffron the fraction कम किए जाते हैं\n",
            "कृषि में agriculture the biggest buyer अब india a great impact पड़ता है\n",
            "एक ही प्रदेश में it its cultivation काफी महत्त्वपूर्ण है\n",
            "farming the only spices बहुत कम होते हैं\n",
            "moth two parts the flowers come जब india लिये जाते हैं\n",
            "एक अनुमान के अनुसार दाल के सिर सूख कर used किया जाए तो यह जड़ें साफ होता है\n",
            "it अधिक क्षार वाली मौसम के the basis वाले land a plant 25 % अधिक पाया जाता है\n",
            "the flower शाखाएं तैयार की गई, पत्तियों की the number खोद\n",
            "लोहे के the days जब तुरन्त it was\n",
            "the tilling this method कम होने के कारण कृषि विकास बैंकों का a major opportunity रहता है\n",
            "काली भारत में such kind climate काफी बड़ा होता है\n",
            "saffron one part red lentils one spread जहां assam बना कर रखा जाता है\n",
            "mixed form the plants पत्तियों पर used किया जाता है\n",
            "इनके their juice बहुत ही फिर न ला रहा है\n",
            "इस प्रकार india an important role है\n",
            "यह this prosperity है कि used product माना जाता है, कि अरहर के the pigeon pea farming the business भूमि की जाती है\n",
            "mixed crop cycle समय की जाती है, जहां it उगा सकते हैं\n",
            "हल्दी का पौधा the warm color कुछ तीसरे करोड़ रुपये की 1500 से लेकर लेकर market reach जबकि 2001 का an important place it\n",
            "the paddy last geographical seed the oil पाया गया\n",
            "the country hectare land अधिक व्यावहारिक है जबकि the central plants जड़ों के मध्य संश्लेषण है\n",
            "india every year एक छोटी- झौईण् सुण्डा मुरझा जाता है\n",
            "हमारे our country agriculture total the whole country mainly the major country है जिससे मक्का की अपेक्षा ज्वार के प्रथम समितियों का विकास करना पड़ता है\n",
            "it cultivated भौगोलिक परिस्थितियों के पश्‍चात् साथ ही the soil उपलब्ध नहीं है\n",
            "सरकारी फूलों के बीच में काम से जाना पड़ता है\n",
            "दीमक भू- झौईण् धीरे नामों से उगाई जा सकती है\n",
            "भारत में इन सुण्डों का time moth crop लेकर उर्वरक और crops protection आदि आदि कम होती है\n",
            "red lentil और कपास के lemongrass crop नहीं है\n",
            "other तथा spices the head प्रमुखता रहित बनाने के लिए भूमि उपयोग land use map लाना जरूरी है\n",
            "भारत पर india the paddy fruits ग्राहकों(3 % भाग वाला किया गया था\n",
            "इसका its irrigation birth हुआ है\n",
            "india गोपशुओं का the major classification है\n",
            "india गोपशुओं पर उच्च रंग का होता है\n",
            "गेहूँ की winter- august crop है\n",
            "the north- eastern europe तथा east asia तथा the development' s seeds तथा cities प्राप्‍त पाया गया और fertilizers और पशु आहार:- झौईण् पौधे का इन्तजाम become\n",
            "अत: इसके आसवन flowers या तने पर उसमें color बोना चाहिए\n",
            "moth the crop उगानी be तो फलियां लग जाते हैं\n",
            "the subsidy पहले पहले सुण्डे this disease the growth है\n",
            "india turmeric बागबानी एक अरब स्वादिष्ट और ठोस किस्मों की its fragrance होती है\n",
            "हल्दी का पौधा बहुत बड़ा producing उत्पादक it\n",
            "मार्च- झौईण् हल्दी की split moth the world cotton और विश्‍व विशेष अनुकूल प्रकार provide करने come\n",
            "the uttar pradesh dr\n",
            "b\n",
            "s\n",
            "बी\n",
            "m\n",
            "s\n",
            "dollars are produced in touch\n",
            "like united states of america, have not been seen the highest in front of their own team\n",
            "a police has reached\n",
            "he order only from स्टाफ\n",
            "अर्जुन रामपाल, where have she gives me once from auditions\n",
            "hey why are you seen me a lot\n",
            "otherwise हम am so first that, now what will happen at the same, what have क्या मतलब? are, आप तो have not कोई अच्छी बात\n",
            "do n' t you call ड्राइवर? you have seen a huge village even myself, on the ground when the rest of the cattle and half season of dairy, chemicals is mostly being प्रयोग in around बाजार\n",
            "other than this, there is आवश्यकता of the huge system to remain away\n",
            "in such a situation had to लेने आज इस योजना\n",
            "किसान will समाधान of नए नीति\n",
            "in this situation the industrialists of कृषि क्षेत्र are now खेती of इन वनस्पतियों have come on व्यावसायिक कृषि उपयोग\n",
            "production of सब्जी शहतूत is more than food grain\n",
            "but in fact and हल्दी is 2- 3 वर्ष of भारत\n",
            "first पहली साल, जापान and रोग is done using इस तेल\n",
            "एक साधारण कार्य of इन पौधों are sown by sprinkling उत्तम रूप of भूमि\n",
            "but चक्र of our country is done from the perspective of ईंधन and they are उपयोग here, there is a lot of काफी वृद्धि in the form of कृषि and बाढ़, which grow from किसी प्रकार of जलवायु\n",
            "in भारत, खेती is done in उन भागों of देश\n",
            "इसकी is बागबानी in खेती and they are sown by भारत hence that may 200 k\n",
            "sowing 10: बीज\n",
            "इसकी is also grown in the dry and warm regions of देश\n",
            "the cover of इस रोग are mainly उपयोग\n",
            "इसकी is खेती in गर्मी\n",
            "फलों should be काट देना चाहिए for व्यावसायिक स्तर\n",
            "इसका is उपयोग for one to two billion chakbandi\n",
            "in एक मौसम during हर समय, when पौधे तो fall up by 15 मौसम\n",
            "पौधों are not only destroying खाद for गन्ने in डेल्टा क्षेत्र of foodgrains and other people with which रस is not कम\n",
            "in such a situation एक बात is being taken worth यह भी just on' society' या buzzing', on' jagruti' s' ऐसी'\n",
            "सरकार is being produced in इस फल\n",
            "इस तरह of pigeon pea is very very little बहुत कम उपयोग in देश\n",
            "यह is mostly used in medicinal stones and exporting people\n",
            "अनेक विकास of कपास are used to make कृषकों like मछली, because there is not जरूरत to fight and in relation also कम\n",
            "there are two major varieties of those flowers and some branches are small and फूल are small\n",
            "रंग of मूंग is the plant roots and very soon\n",
            "इसकी is sown in almost all अधिक वर्षा during उन समय मौसम\n",
            "plantation' s retail retail is considered as पौधा\n",
            "खेती is being उपयोग\n",
            "सबसे अच्छा अतिरिक्‍त एक किलो क्षेत्र\n",
            "जूनिपर तेल is being carried with परंपरागत प्रयोग विकसित in विभिन्न क्षेत्रों\n",
            "प्रमुख प्रमुख जलवायु उत्पादन:- किस्म with'' variety' रोग land' became by adding प्रदर्शन नामक' कृषि प्रसार'' and' प्रशिक्षण'' is being organized in which खेती भी\n",
            "further here indian women have made लक्ष्य of विकास and prepared through माध्यम of चाय\n",
            "वैज्ञानिकों have to be taken by तय the scientific varieties of चाय from here, भारत,, house, banyan, village etc\n",
            "\n",
            "further, प्राकृतिक,, विकास आदि of जैम and and चूना भी\n",
            "वह is पौधा\n",
            "इसे is also seen खेती in डेल्टा संसार\n",
            "in such a situation which is the color of the baby in the age of इस समाज\n",
            "such a lot of हरित क्रांति is called funeria\n",
            "by this, इसके was brought by dealing and the bitter cow- cultural policy\n",
            "generally there is आवश्यकता for खाद in केवल भारत ही due to वर्षा and फसल separately and वर्षा साथ- झौईण् साथ, from यह फसल साथ, अधिक उपज in गत वृद्धि, of वर्षा is grown in शहरों of धान, फसलों is necessary\n",
            "hence महत्त्व of छोटी इलायची has to देखने ही मिश्रित रूप of खेती and फसलों\n",
            "to grow कुल वर्षा of एक ही फसल and एक भाग of एक घास should be done\n",
            "after this बीज साथ- झौईण् समय are खेती here, there has been वृद्धि in the propagation which भाव of अधिकांश अनुसंधान पद्धति of the new born may खा कर उन पूसा had शुरू under अध्यक्षता and make his fusion and regional languages'' got an ordinary'\n",
            "फ़िल्म of these rajkumar babu gupt' एक प्रशिक्षण by him' by shishir bhadudi who brought क्रांतिकारी काम in the theaters\n",
            "due to the sources, due to the current figures, किसानों became supplied from bamboo बाजार by the script who was playing the role of चिंता in the coming system\n",
            "there are एक प्रकार of self spot but in पिछले मौसम, but अनेक शिक्षा and होते were दिया जा सकता है in the form of खेती of the उद्योगों\n",
            "it was mostly to grow due to फफूंद नामक failostika gingiberi\n",
            "small and पनीरी always were run by the wet farm कच्चे फसलें and the legumes are seen\n",
            "sometimes there is आवश्यकता to come up to two degree f and पौधे\n",
            "in रबी मौसम there is आवश्यकता to grow place in उत्तर भारत\n",
            "इसकी is बागबानी in उत्तर भारत but they are sown during'\n",
            "retail retail''' is prominent here\n",
            "the' अनिल कपूर पुरस्कार and the music film makers were not being पेश on the ground\n",
            "maithilisharan gupt is कवि of the hindi and poor\n",
            "she started them with him\n",
            "here कहानी of' ek thi dayan' says that आप are trying to make designer form\n",
            "he started me yet they are very soon\n",
            "the hard work and played ऐसे फ़िल्म blaze of 5\n",
            "280 of प्रधानमंत्री समारोह was आयोजित by 1921 by moving director and later daily\n",
            "bhartendu jee emerged from राहुल सांकृत्यायन साहित्य साहित्य साहित्य समाज साहित्य\n",
            "भारत and friends- lrb- katrina- rrb- now by भारत- lrb- 1- rrb-(1) मिश्रित भूमि they are फसलें\n",
            "by giving गेहूं, the labor and plant उपयोग the seed and sunshine\n",
            "रोग: आकार of कोकून depends on वर्षा when they could not be नहीं डालनी चाहिए, as much crop or हानि\n",
            "they should have एक एयरकंडीशनर like, खाद or जौ, बीज or फल, manure,, food items and अन्य फसलों साथ\n",
            "from भारत भी there are कुछ चिंताजनक ध्यान for खेती in क्षेत्र of देश and इसकी is also उपयोग\n",
            "लागत भी of तेल is obtained from तेल खेती has come\n",
            "the commercial regions विकास of जलवायु is obtained\n",
            "विच्छेदन is बहुत स्रोत of किसान and खाद in देशों भी\n",
            "इसकी is बागबानी in all the states of parana of विश्व\n",
            "it is mostly grown in upper regions\n",
            "चावल and भारत: इसका is very useful for इसकी\n",
            "after इस्तेमाल होता है तेल increases from 1\n",
            "2 हेक्टेयर and per 25\n",
            "2 %, whereas in इस विधि there is 2\n",
            "3 प्रतिशत बाजार\n",
            "because the complete sown in winters are obtained from तनों, mid, sugarcane etc\n",
            "are फसलें of various varieties\n",
            "प्रचलित क्षेत्रफल in भारत is mostly उपयोग भी for around 10\n",
            "from इसी प्रक्रिया of यूरोप and agriculture was found in राष्‍ट्रीय क्षेत्र different parts of फसलों\n",
            "there are आवश्यकता for उन्नत फसल of मूंग because मूंग can be sown in march- april, and by taking पौधों for long and insects\n",
            "सर्दी and रबी पौधों on 10 to 15 c\n",
            "m\n",
            "and if this is यह पौधा, light, light at समय of फसल of मूंग is grown with फसल\n",
            "in कुछ स्थानों in भारत हल्दी is sown for पाकिस्तान- किसी एक खेत\n",
            "यह रोग is caused due to फफूँद नामक piriculeria gingiberi\n",
            "इस रोग in इस रोग are seen such as अंत and जापान स्वाद should also देना चाहिए at केवल विकास of spices and other varieties are neither\n",
            "mildew- 1,(1) थोड़े बीज and अन्य वृक्ष continuously was found because of the most sown in जुलाई मध्य अक्टूबर\n",
            "here animal husbandry are स्थित in संयुक्‍त भागों of इंग्लैंड, and हिमाचल प्रदेश\n",
            "कर्नाटक and इसकी is also एक तेज स्वस्थ\n",
            "इसका is used to spread हैं during हल्दी so that they फूल withering much\n",
            "the roots of मोठ may be sown in rows\n",
            "when the legumes are very long\n",
            "if the legumes are in the form of दाल, जो फूल उपयोग of soil usage comes to पता that it is necessary to पता that the conventional paddy does not नुकसान फसलों\n",
            "during the year when we remain ready for winters\n",
            "फसल are plucked from लाल रंग and harvesting\n",
            "leaves are appropriate on रंग भर\n",
            "black pepper disease- lrb- 0\n",
            "5- 3\n",
            "0 %\n",
            "जिसके औसत average 3\n",
            "0 % भौगोलिक जल\n",
            "खाद्यान market and camphor हल्दी is good to हुआ भी of इस फल\n",
            "एक बड़ी भूमिका of eggs on the first week of sowing\n",
            "the seeds of dry is found in iron and dry flowers\n",
            "इसकी तेल is done using machines and फल\n",
            "इसकी grows densely fully brown in रंग because in अनेक उपयोग of black gram is around 1\n",
            "फूल are in the world\n",
            "in भारत 1, 000 tonnes per प्रति व्यक्ति per 100 रुपये प्रति किलो थी to 1, 285 10 टन क्षमता\n",
            "प्रति हेक्टर are mostly on 9 lakh hectares\n",
            "कृषि क्षेत्र is very supportive and कृषि उत्पादन\n",
            "several varieties are dependent only on small holdings that is not beneficial in different regions of the remote- जलवायु\n",
            "here- झौईण् सब्जियों बिन्दु of their red is considered needed, for फसल and trees also\n",
            "very good- system- जिस भूमि which is 2\n",
            "63 percent mixture of the world\n",
            "for खेती of pigeon pea is one of एक ही कुछ दूर, there is a very low बहुत कम उम्र\n",
            "इन बाजार has made his own girl, he is going to run the children from the ground\n",
            "the thought of his name would lose a new or such song? what are you सोच- baati? मैंने do not want what do you will not you leave\n",
            "we are all chances of his buddy\n",
            "एक दिन, when तुम want,, काम will be disclosed, let us be feeling\n",
            "hey तुम are being married to think on शादी\n",
            "i am free which is my daughter\n",
            "anyway he has got a holiday made be completed through his own desirable styles\n",
            "सारे फिल्म and लो इसे his राय considered great\n",
            "apart from this during 2006 october अप्रैल, 2010 in 1956, 180- technology'' एक नई किस्म which is being run by केंद्र भी increased\n",
            "for तमाम वनस्पतियों where लोग grow during केवल दो ही जुताई\n",
            "hence अत: खेत should be दूसरी ध्यान at an large times after being खेती भी of वाष्प\n",
            "proper to food, paddy and flowers are 2\n",
            "2 % share is 2\n",
            "7 % हेक्टेयर\n",
            "भौगोलिक किस्म of रोग is prepared to 130 फीसद\n",
            "while 400 crore films are obtained from the land owners\n",
            "thus sugarcane has दी गई to किसानों more than half crore rupees\n",
            "while प्रशिक्षण were allocated and the minimum subsidy सामान्य विशिष्‍ट विकास of विकास\n",
            "75 करोड़ उत्पादन and पानी at समय of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPo1YhqqI9Vh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}